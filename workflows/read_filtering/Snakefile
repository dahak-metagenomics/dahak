'''
Author: Phillip Brooks, Charles Reid
Affiliation: UC Davis Lab for Data Intensive Biology
Objective: A Snakemake workflow to process reads to produce quality trimmed data 
Date: 2018-06-08
Documentation: docs/workflow_readfilt.md
'''

from utils import container_image_is_external, container_image_name
from os.path import join, isfile, dirname
import os, re
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
HTTP = HTTPRemoteProvider()



###################################
# Read Filtering: default config

include: "read_filtering.settings"

data_dir = config['data_dir']
biocontainers = config['biocontainers']
readfilt = config['read_filtering']


###################################
# Read Filtering: high level rules

# Skip to the very end of the file 
# to see the high-level rules that
# trigger cascades of workflow tasks.


###################################
# Read Filtering: biocontainers

# Assemble container image names for containers
quayurls = []
for app in biocontainers.keys():
    if container_image_is_external(biocontainers,app):
        name = container_image_name(biocontainers,app)
        quayurls.append(name)

pulled_containers_touchfile = ".pulled_containers"
pulled_containers_touchfile = join(data_dir,pulled_containers_touchfile)

rule pull_biocontainers:
    """
    Pull the required versions of containers from quay.io.
    Note that this is not necessary with the singularity: directive.
    """
    output:
        touch(pulled_containers_touchfile)
    run:
        for quayurl in quayurls:
            shell('''
                singularity pull --force {quayurl}
            ''')


###################################
# Read Filtering: fetch reads

# Match wildcards with the pre-trimming pattern
pre_trimming_pattern = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'])

def download_reads_read_data_url(wildcards):
    """Returns url where data can be downloaded"""

    # Get the filename only from the relative path, and do wildcard substitution
    _, stripped_output = os.path.split(pre_trimming_pattern)
    stripped_output = stripped_output.format(**wildcards)

    # Get the URL where this file is available
    read_data_url = readfilt['read_files'][stripped_output]

    return read_data_url

def download_reads_read_data_file(wildcards):
    return pre_trimming_pattern.format(**wildcards)

rule download_reads:
    """
    Fetch user-requested files from OSF containing reads that will be used in
    the read filtering process.

    Note that this defines wildcard-based download rules, rather than
    downloading all files all at once, to keep things flexible and fast.
    """
    output:
        pre_trimming_pattern
    message: 
        '''--- Downloading reads.'''
    params:
        read_data_url = download_reads_read_data_url,
        read_data_file = download_reads_read_data_file
    shell:
        'wget -O {params.read_data_file} {params.read_data_url}'


adapter_output = join(data_dir, readfilt['adapter_file']['name'])
adapter_url = readfilt['adapter_file']['url']

rule download_read_adapters:
    """
    Download FASTA read adapaters (TruSeq2-PE sequencer by default).
    """
    output:
        adapter_output
    message:
        '''--- Downloading adapter file.'''
    shell:
        'wget -O {adapter_output} {adapter_url}'


###################################
# Read Filtering: pre and post trimming

pre_trimming_input = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'])

target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)
pre_trimming_output = re.sub('\.fq\.gz',target_ext,pre_trimming_input)

fastqc_image = container_image_name(biocontainers, 'fastqc')

def pre_trimming_qa_input(wildcards):
    # Wildcard substitution to obtain trimming input
    return pre_trimming_input.format(**wildcards) 

def pre_trimming_qa_output(wildcards):
    # Wildcard substitution to obtain trimming output
    return pre_trimming_output.format(**wildcards) 


rule pre_trimming_quality_assessment:
    """
    Perform a pre-trimming quality check of the reads from the sequencer.
    """
    input:
        pre_trimming_input
    output: 
        pre_trimming_output
    message: 
        '''--- Pre-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 2
    params:
        pre_trimming_output_wc = pre_trimming_qa_output,
        pre_trimming_input_wc = pre_trimming_qa_input
    shell:
        'fastqc -t {threads} '
        '/{params.pre_trimming_input_wc} '
        '-o /{data_dir} '

post_trimming_pattern = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
post_trimming_input  = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])

target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)

post_trimming_output = re.sub('\.fq\.gz',target_ext,post_trimming_input)

def post_trimming_qa_input(wildcards):
    return post_trimming_pattern.format(**wildcards)

def post_trimming_qa_output(wildcards):
    return post_trimming_output.format(**wildcards)

rule post_trimming_quality_assessment:
    """
    Perform a post-trimming quality check 
    of the reads from the sequencer.
    """
    input:
        post_trimming_input
    output:
        post_trimming_output
    message: 
        '''--- Post-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 2 
    params:
        post_trimming_output_wc = post_trimming_qa_output,
        post_trimming_input_wc = post_trimming_qa_input
    shell:
        'fastqc -t {threads} '
        '/{params.post_trimming_input_wc} '
        '-o /{data_dir} '


###################################
# Read Filtering: interleave

interleave_ext = readfilt['interleaving']['interleave_suffix']

try:
    # these are strings containing templates of .fq.gz file names
    fq_fwd_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=1,sample='{sample}',qual='{qual}'))
    fq_rev_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=2,sample='{sample}',qual='{qual}'))
    fq_interleave  = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=interleave_ext,sample='{sample}',qual='{qual}'))
except KeyError:
    fq_fwd_trimmed = ""
    fq_rev_trimmed = ""
    fq_interleave  = ""

interleave_input = [fq_fwd_trimmed, fq_rev_trimmed]
interleave_output = fq_interleave
khmer_image = container_image_name(biocontainers,'khmer')
quality_log = join(data_dir,'interleave_{sample}_trim{qual}.log')

def interleave_reads_inputs(wildcards):
    # inputs should include the data directory prefix
    # do wildcard substitution
    interleave_input_wc = [x.format(**wildcards) for x in interleave_input]

    # for docker container, turn prefix 'data/' into '/data/'
    interleave_input_wc = ["/%s"%(x) for x in interleave_input_wc]
    interleave_input_wcs = " ".join(interleave_input_wc)
    return interleave_input_wcs

def interleave_reads_output(wildcards):
    # wildcard substitution
    return interleave_output.format(**wildcards)

def interleave_reads_logfile(wildcards):
    # wildcard substitution
    return quality_log.format(**wildcards)


rule interleave_reads:
    """
    Interleave paired-end reads using khmer.
    The trim quality comes from the filename.
    """
    input:
        interleave_input
    output:
        interleave_output
    message:
        """--- Interleaving read data."""
    singularity:
        khmer_image
    log:
        quality_log
    params:
        interleave_input_wc = interleave_reads_inputs,
        interleave_output_wc = interleave_reads_output,
        interleave_logfile = interleave_reads_logfile
    shell:
        'interleave-reads.py {params.interleave_input_wc} '
        '--no-reformat '
        '-o {params.interleave_output_wc} '
        '--gzip'


###################################
# Read Filtering: quality trimming

# If you only want to substitute a subset of wildcards,
# you can leave a wildcard untouched by substituting
# the string {variable} for {variable}.
# 
# We use this trick several times in these rules.
# 
fq_fwd = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(direction=1,sample='{sample}'))
fq_rev = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(direction=2,sample='{sample}'))

quality_input = [fq_fwd, fq_rev]

fq_fwd_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=1,sample='{sample}',qual='{qual}'))
fq_rev_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=2,sample='{sample}',qual='{qual}'))

trim_target_ext = readfilt['quality_trimming']['trim_suffix']

fq_fwd_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_fwd_trimmed)
fq_rev_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_rev_trimmed)

quality_output = [fq_fwd_trimmed, fq_fwd_se,
                  fq_rev_trimmed, fq_rev_se]

adapter_file = join(data_dir, readfilt['adapter_file']['name'])

quality_log = join(data_dir,'trimmomatic_pe_{sample}_trim{qual}.log')
trimmo_image = container_image_name(biocontainers,'trimmomatic')

def quality_trimming_quality_log(wildcards):
    """Get the log file for this quality trimming param set"""
    return quality_log.format(**wildcards)

def quality_trimming_qual(wildcards):
    """Get quality threshold for trimming"""
    return "{qual}".format(**wildcards)

def quality_trimming_quality_input(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """

    # input includes data/ prefix
    quality_input_wc = [x.format(**wildcards) for x in quality_input]

    # for containers, turn prefix data/ into /data/
    quality_input_wc = ["/%s"%(x) for x in quality_input_wc]
    quality_input_wcs = " ".join(quality_input_wc)
    return quality_input_wcs

def quality_trimming_quality_output(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """
    # input includes data/ prefix
    quality_output_wc = [x.format(**wildcards) for x in quality_output]

    # for containers, turn prefix data/ into /data/
    quality_output_wc = ["/%s"%(x) for x in quality_output_wc]
    quality_output_wcs = " ".join(quality_output_wc)
    return quality_output_wcs


rule quality_trimming:
    """
    Trim reads from the sequencer by dropping low-quality reads.
    """
    input:
        quality_input, adapter_file
    output:
        quality_output
    message: 
        """--- Quality trimming read data."""
    singularity: 
        trimmo_image
    params:
        qual = quality_trimming_qual,
        quality_input_wc = quality_trimming_quality_input,
        quality_output_wc = quality_trimming_quality_output,
        quality_log_wc = quality_trimming_quality_log
    log: 
        quality_log
    shell:
        'trimmomatic PE '
        '{params.quality_input_wc} '
        '{params.quality_output_wc} '
        'ILLUMINACLIP:/{adapter_file}:2:40:15 '
        'LEADING:{params.qual} '
        'TRAILING:{params.qual} '
        'SLIDINGWINDOW:4:{params.qual} '
        'MINLEN:25 '


###################################
# Read Filtering: high level rules

# These "ghost" rules don't do anything 
# except trigger other rules.

rule fetch:
    input:
        expand(pre_trimming_pattern, sample="SRR606249", direction=["1","2"])

rule pre_trim:
    input:
        expand(pre_trimming_pattern, sample="SRR606249", direction=["1","2"], qual=["2","30"])

rule post_trim:
    input:
        expand(post_trimming_pattern, sample="SRR606249", direction=["1","2"], qual=["2","30"])


