'''
Author: Phillip Brooks, Charles Reid
Affiliation: UC Davis Lab for Data Intensive Biology
Objective: A Snakemake workflow to process reads to produce quality trimmed data 
Date: 2018-06-08
Documentation: docs/workflow_readfilt.md
'''

from utils import container_image_is_external, container_image_name
from os.path import join, isfile, dirname
import os, re
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
HTTP = HTTPRemoteProvider()



###################################
# Read Filtering: default config

data_dir = config['data_dir']
biocontainers = config['biocontainers']
taxclass = config['taxonomic_classification']
assembly = config['assembly']
readfilt = config['read_filtering']


###################################
# Read Filtering: build rules

# Skip to the very end of the file 
# to see the high-level build rules
# that trigger cascades of workflow
# tasks.



###################################
# Read Filtering: fetch reads

pre_trimming_pattern = readfilt['read_patterns']['pre_trimming_pattern']
pre_trimming_relative_path = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'])

def download_reads_read_data_url(wildcards):
    """
    Given a set of wildcards, return the URL where the 
    pre-trimmed reads can be downloaded.
    """
    # Get the filename only from the relative path, and do wildcard substitution
    pre_trimming_name = pre_trimming_pattern.format(**wildcards)

    # Get the URL where this file is available
    read_data_url = config['files'][pre_trimming_name]

    return read_data_url

def download_reads_read_data_file(wildcards):
    """
    Return the pre-trimming file that matches the given wildcards
    """
    # Get the relative path and do wildcard substitution
    return pre_trimming_relative_path.format(**wildcards)


rule download_reads:
    """
    Fetch user-requested files from OSF containing reads that will be used in
    the read filtering process.

    Note that this defines wildcard-based download rules, rather than
    downloading all files all at once, to keep things flexible and fast.
    """
    output:
        pre_trimming_relative_path
    message: 
        '''--- Downloading reads.'''
    params:
        read_data_url = download_reads_read_data_url,
        read_data_file = download_reads_read_data_file
    shell:
        'wget -O {params.read_data_file} {params.read_data_url}'


adapter_output = join(data_dir, readfilt['adapter_file']['name'])
adapter_url = readfilt['adapter_file']['url']

rule download_read_adapters:
    """
    Download FASTA read adapaters (TruSeq2-PE sequencer by default).
    """
    output:
        adapter_output
    message:
        '''--- Downloading adapter file.'''
    shell:
        'wget -O {adapter_output} {adapter_url}'


###################################
# Read Filtering: pre trimming

fq_fwd_pre_trimmed = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}'))
fq_rev_pre_trimmed = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}'))

pre_trimming_inputs = [ fq_fwd_pre_trimmed, 
                        fq_rev_pre_trimmed]

target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)

pre_trimming_output_fwd = re.sub('\.fq\.gz', target_ext, fq_fwd_pre_trimmed)
pre_trimming_output_rev = re.sub('\.fq\.gz', target_ext, fq_rev_pre_trimmed) 

pre_trimming_outputs = [pre_trimming_output_fwd,
                        pre_trimming_output_rev]

fastqc_image = container_image_name(biocontainers, 'fastqc')

def pre_trimming_qa_inputs(wildcards):

    # input already includes data/ prefix
    pre_inputs_wc = [x.format(**wildcards) for x in pre_trimming_inputs]

    # for containers, turn prefix data/ into /data/
    pre_inputs_wc = ["/%s"%(x) for x in pre_inputs_wc]
    pre_inputs_wcs = " ".join(pre_inputs_wc)
    return pre_inputs_wc

def pre_trimming_qa_outputs(wildcards):

    # output already includes data/ prefix
    pre_outputs_wc = [x.format(**wildcards) for x in pre_trimming_outputs]

    # for containers, turn prefix data/ into /data/
    pre_outputs_wc = ["/%s"%(x) for x in pre_outputs_wc]
    pre_outputs_wcs = " ".join(pre_outputs_wc)
    return pre_outputs_wcs


rule pre_trimming_quality_assessment:
    """
    Perform a pre-trimming quality check of the reads from the sequencer.
    """
    input:
        pre_trimming_inputs
    output: 
        pre_trimming_outputs
    message: 
        '''--- Pre-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 2
    params:
        pre_trimming_output_wc = pre_trimming_qa_outputs,
        pre_trimming_input_wc = pre_trimming_qa_inputs
    shell:
        'fastqc -t {threads} '
        '/{params.pre_trimming_input_wc} '
        '-o /{data_dir} '



###################################
# Read Filtering: post trimming

fq_fwd_post_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(
                            direction=readfilt['direction_labels']['forward'],
                            sample='{sample}',
                            qual='{qual}'))
fq_rev_post_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(
                            direction=readfilt['direction_labels']['reverse'],
                            sample='{sample}',
                            qual='{qual}'))

post_trimming_inputs = [fq_fwd_post_trimmed, fq_rev_post_trimmed]

target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)

post_trimming_output_fwd = re.sub('\.fq\.gz', target_ext, fq_fwd_post_trimmed)
post_trimming_output_rev = re.sub('\.fq\.gz', target_ext, fq_rev_post_trimmed)

post_trimming_outputs = [post_trimming_output_fwd,
                         post_trimming_output_rev]

fastqc_image = container_image_name(biocontainers, 'fastqc')

def post_trimming_qa_inputs(wildcards):

    # input already includes data/ prefix
    post_inputs_wc = [x.format(**wildcards) for x in post_trimming_inputs]

    # for containers, turn prefix data/ into /data/
    post_inputs_wc = ["/%s"%(x) for x in post_inputs_wc]
    post_inputs_wcs = " ".join(post_inputs_wc)
    return post_inputs_wc

def post_trimming_qa_outputs(wildcards):

    # output already includes data/ prefix
    post_outputs_wc = [x.format(**wildcards) for x in post_trimming_outputs]

    # for containers, turn prefix data/ into /data/
    post_outputs_wc = ["/%s"%(x) for x in post_outputs_wc]
    post_outputs_wcs = " ".join(post_outputs_wc)
    return post_outputs_wcs


rule post_trimming_quality_assessment:
    """
    Perform a post-trimming quality check 
    of the reads from the sequencer.
    """
    input:
        post_trimming_inputs
    output:
        post_trimming_outputs
    message: 
        '''--- Post-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 2 
    params:
        post_trimming_outputs_wc = post_trimming_qa_outputs,
        post_trimming_inputs_wc = post_trimming_qa_inputs
    shell:
        'fastqc -t {threads} '
        '{params.post_trimming_inputs_wc} '
        '-o /{data_dir} '


###################################
# Read Filtering: interleave

# These are strings containing templates of .fq.gz file names.
# R
fq_fwd_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))
fq_rev_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}',
                        qual='{qual}'))
fq_interleave  = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(
                        direction=readfilt['interleaving']['interleave_suffix'],
                        sample='{sample}',
                        qual='{qual}'))

interleave_input = [fq_fwd_trimmed, fq_rev_trimmed]
interleave_output = fq_interleave
khmer_image = container_image_name(biocontainers,'khmer')
quality_log = join(data_dir,'interleave_{sample}_trim{qual}.log')

def interleave_reads_inputs(wildcards):
    # inputs should include the data directory prefix
    # do wildcard substitution
    interleave_input_wc = [x.format(**wildcards) for x in interleave_input]

    # for docker container, turn prefix 'data/' into '/data/'
    interleave_input_wc = ["/%s"%(x) for x in interleave_input_wc]
    interleave_input_wcs = " ".join(interleave_input_wc)
    return interleave_input_wcs

def interleave_reads_output(wildcards):
    # wildcard substitution
    return interleave_output.format(**wildcards)

def interleave_reads_logfile(wildcards):
    # wildcard substitution
    return quality_log.format(**wildcards)


rule interleave_reads:
    """
    Interleave paired-end reads using khmer.
    The trim quality comes from the filename.
    """
    input:
        interleave_input
    output:
        interleave_output
    message:
        """--- Interleaving read data."""
    singularity:
        khmer_image
    log:
        quality_log
    params:
        interleave_input_wc = interleave_reads_inputs,
        interleave_output_wc = interleave_reads_output
    shell:
        'interleave-reads.py {params.interleave_input_wc} '
        '--no-reformat '
        '-o {params.interleave_output_wc} '
        '--gzip'


###################################
# Read Filtering: quality trimming

# If you only want to substitute a subset of wildcards,
# you can leave a wildcard untouched by substituting
# the string {variable} for {variable}.
# 
# We use this trick several times in these rules.
# 
fq_fwd = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(
                    direction=readfilt['direction_labels']['forward'],
                    sample='{sample}'))
fq_rev = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(
                    direction=readfilt['direction_labels']['reverse'],
                    sample='{sample}'))

quality_input = [fq_fwd, fq_rev]

fq_fwd_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))
fq_rev_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}',
                        qual='{qual}'))

trim_target_ext = readfilt['quality_trimming']['trim_suffix']

fq_fwd_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_fwd_trimmed)
fq_rev_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_rev_trimmed)

quality_output = [fq_fwd_trimmed, fq_fwd_se,
                  fq_rev_trimmed, fq_rev_se]

adapter_file = join(data_dir, readfilt['adapter_file']['name'])

quality_log = join(data_dir,'trimmomatic_pe_{sample}_trim{qual}.log')
trimmo_image = container_image_name(biocontainers,'trimmomatic')

def quality_trimming_quality_log(wildcards):
    """Get the log file for this quality trimming param set"""
    return quality_log.format(**wildcards)

def quality_trimming_qual(wildcards):
    """Get quality threshold for trimming"""
    return "{qual}".format(**wildcards)

def quality_trimming_quality_input(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """

    # input already includes data/ prefix
    quality_input_wc = [x.format(**wildcards) for x in quality_input]

    # for containers, turn prefix data/ into /data/
    quality_input_wc = ["/%s"%(x) for x in quality_input_wc]
    quality_input_wcs = " ".join(quality_input_wc)
    return quality_input_wcs

def quality_trimming_quality_output(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """
    # input includes data/ prefix
    quality_output_wc = [x.format(**wildcards) for x in quality_output]

    # for containers, turn prefix data/ into /data/
    quality_output_wc = ["/%s"%(x) for x in quality_output_wc]
    quality_output_wcs = " ".join(quality_output_wc)
    return quality_output_wcs


rule quality_trimming:
    """
    Trim reads from the sequencer by dropping low-quality reads.
    """
    input:
        quality_input, adapter_file
    output:
        quality_output
    message: 
        """--- Quality trimming read data."""
    singularity: 
        trimmo_image
    params:
        qual = quality_trimming_qual,
        quality_input_wc = quality_trimming_quality_input,
        quality_output_wc = quality_trimming_quality_output,
        quality_log_wc = quality_trimming_quality_log
    log: 
        quality_log
    shell:
        'trimmomatic PE '
        '{params.quality_input_wc} '
        '{params.quality_output_wc} '
        'ILLUMINACLIP:/{adapter_file}:2:40:15 '
        'LEADING:{params.qual} '
        'TRAILING:{params.qual} '
        'SLIDINGWINDOW:4:{params.qual} '
        'MINLEN:25 '



### ############################################
### # Read Filtering: bypass trimming, download trimmed data directly
### #
### # THIS RULE SHOULD PROBABLY BE COMMENTED OUT
### #
### # Note: either quality_trimming or download_trimmed_data
### # must be enabled, but not both. Otherwise you get conflicts
### # due to two rules producing the same output file.
### # 
### # download_trimmed_data is for testing, folks will not normally
### # have already-trimmed data to download.
### 
### post_trimmed_pattern = readfilt['read_patterns']['post_trimming_pattern']
### post_trimmed_relative_path = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
### 
### def download_reads_trimmed_data_url(wildcards):
###     """
###     Given a set of wildcards, return the URL where the 
###     post-trimmed reads can be downloaded (if available).
###     """
###     # Get the filename only from the relative path, and do wildcard substitution
###     post_trimmed_name = post_trimmed_pattern.format(**wildcards)
### 
###     # Get the URL where this file is available
###     read_data_url = config['files'][post_trimmed_name]
### 
###     return read_data_url
### 
### def download_reads_trimmed_data_file(wildcards):
###     """
###     Return the post-trimming file that matches the given wildcards
###     """
###     # Get the relative path and do wildcard substitution
###     post_trimming_file = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
###     return post_trimming_file.format(**wildcards)
### 
### 
### rule download_trimmed_data:
###     """
###     Fetch user-requested files from OSF containing trimmed reads that will be
###     used in various workflows.
### 
###     Note that this defines wildcard-based download rules, rather than
###     downloading all files all at once, to keep things flexible and fast.
###     """
###     output:
###         post_trimmed_relative_path
###     message:
###         """--- Skipping read trimming step, downloading trimmed reads directly."""
###     params:
###         trimmed_data_url = download_reads_trimmed_data_url,
###         trimmed_data_file = download_reads_trimmed_data_file
###     shell:
###         'wget -O {params.read_data_file} {params.read_data_url}'



###################################
# Read Filtering: build rules

workflows = config['workflows']

directions = [readfilt['direction_labels']['forward'],
              readfilt['direction_labels']['reverse']]

rule read_filtering_pretrim_workflow:
    """
    Build rule: trigger the read filtering workflow
    """
    input:
        expand( pre_trimming_outputs,
                sample    = workflows['read_filtering_workflow']['sample'],
                direction = directions,
        )

rule read_filtering_posttrim_workflow:
    """
    Build rule: trigger the read filtering workflow
    """
    input:
        expand( post_trimming_outputs,
                sample    = workflows['read_filtering_workflow']['sample'],
                qual      = workflows['read_filtering_workflow']['qual'],
                direction = directions, 
        )

