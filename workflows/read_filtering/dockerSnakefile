'''
To run this workflow,
use conda and singularity:
snakemake --use-conda --use-singularity

Note: the reason we try/except with param values
is in case the user only wants to run a portion
of the workflow. 

If the config values are all set, the workflow will
actually run and do work the user did not want.

If the config values are not set, the user will see
key errors for workflow steps they don't care about.
'''

from utils import container_image_is_external, container_image_name


###################################
# Read Filtering: default config

include: "read_filtering.settings"

data_dir = config['data_dir']
biocontainers = config['biocontainers']
readfilt = config['read_filtering']


###################################
# Read Filtering: high level rules

# Skip to the very end of the file 
# to see the high-level rules that
# trigger cascades of workflow tasks.


###################################
# Read Filtering: biocontainers

from os.path import join

# assemble container image names for containers.
# these are usually quay.io urls, but in special cases
# they can be the name of a local docker image.
quayurls = []
for app in biocontainers.keys():
    if container_image_is_external(biocontainers,app):
        name = container_image_name(biocontainers,app)
        quayurls.append(name)

#pulled_containers_touchfile_docker = ".pulled_containers_docker"
#pulled_containers_touchfile_docker = join(data_dir,pulled_containers_touchfile_docker)

pulled_containers_touchfile_singularity = ".pulled_containers_singularity"
pulled_containers_touchfile_singularity = join(data_dir,pulled_containers_touchfile_singularity)

rule pull_biocontainers:
    """
    Pull the required versions of containers from quay.io.
    """
    output:
        touch(pulled_containers_touchfile_docker)
    run:
        for quayurl in quayurls:
            shell('''
                docker pull {quayurl}
            ''')


###################################
# Read Filtering: fetch reads

import re
from os.path import join, isfile
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
HTTP = HTTPRemoteProvider()

try:
    pre_trimming_pattern = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'])
except KeyError:
    pre_trimming_pattern = ""

def download_reads_read_data_url(wildcards):
    """Returns url where data can be downloaded"""
    # Get the filename only from the relative path, and do wildcard substitution
    _, stripped_output = os.path.split(pre_trimming_pattern)
    stripped_output = stripped_output.format(**wildcards)

    # Get the URL where this file is available
    read_data_url = readfilt['read_files'][stripped_output]

    return read_data_url

def download_reads_read_data_file(wildcards):
    return pre_trimming_pattern.format(**wildcards)

rule download_reads:
    """
    Fetch user-requested files from OSF 
    containing reads that will be used 
    in the read filtering process.

    Note that this defines wildcard-based download
    rules, rather than downloading all files all 
    at once, to keep things flexible and fast.
    """
    output:
        pre_trimming_pattern
    message: 
        '''--- Downloading reads.'''
    params:
        read_data_url = download_reads_read_data_url,
        read_data_file = download_reads_read_data_file
    shell:
        'wget -O {params.read_data_file} {params.read_data_url}'


try:
    adapter_output = join(data_dir, readfilt['adapter_file']['name'])
except KeyError:
    adapter_output = ""

try:
    adapter_url = readfilt['adapter_file']['url']
except:
    adapter_url = ""

rule download_read_adapters:
    """
    Download FASTA read adapaters.
    This downloads adpaters for 
    the TruSeq2-PE sequencer by default.
    """
    output:
        adapter_output
    message:
        '''--- Downloading adapter file.'''
    shell:
        'wget -O {adapter_output} {adapter_url}'


###################################
# Read Filtering: pre and post trimming

import os, re
from os.path import join, dirname

PWD = os.getcwd()

try:
    pre_trimming_input = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'])
except KeyError:
    pre_trimming_input = ""

try:
    target_suffix = readfilt['quality_assessment']['fastqc_suffix']
except KeyError:
    target_suffix = ""

target_ext = "_%s.zip"%(target_suffix)
pre_trimming_output = re.sub('\.fq\.gz',target_ext,pre_trimming_input)

# Get the quay URL
# 
# To allow the user to use local Dockerfiles,
# in case something in bioconda is borked,
# we require the user to specify the name 
# of the docker image to run, and set 
# use_local to true
# 
# The 'use_local' key should indicate
# whether to use a local Docker image.
# If it is true, the 'local' key
# should indicate the local image to use.
# Those are checked in utilities.py

fastqc_image = container_image_name(biocontainers, 'fastqc')

def pre_trimming_qa_input(wildcards):
    return pre_trimming_input.format(**wildcards) 

def pre_trimming_qa_output(wildcards):
    return pre_trimming_output.format(**wildcards) 

rule pre_trimming_quality_assessment:
    """
    Perform a pre-trimming quality check 
    of the reads from the sequencer.

    This calls docker in the command that is run.
    """
    output: 
        pre_trimming_output
    message: 
        '''--- Pre-trim quality check of trimmed data with fastqc.'''
    threads: 2
    run:
        # get output file name with wildcards replaced
        pre_trimming_output_wc = pre_trimming_output.format(**wildcards)

        # get input file name with wildcards replaced
        pre_trimming_input_wc = pre_trimming_input.format(**wildcards)

        # get directory for final output
        output_dir = dirname(pre_trimming_output_wc)

        shell('''
            docker run \
                -u "$(id -u):$(id -g)" \
                -v {PWD}/{data_dir}:/{data_dir} \
                -it \
                {fastqc_image} \
                fastqc \
                -t {threads} \
                /{pre_trimming_input_wc} \
                -o /{data_dir}
        ''')


try:
    post_trimming_pattern = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
except KeyError:
    post_trimming_pattern = ""

try:
    post_trimming_input  = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
except KeyError:
    post_trimming_input = ""

try:
    target_suffix = readfilt['quality_assessment']['fastqc_suffix']
except KeyError:
    target_suffix = ""

target_ext = "_%s.zip"%(target_suffix)
post_trimming_output = re.sub('\.fq\.gz',target_ext,post_trimming_input)

def post_trimming_qa_input(wildcards):
    return post_trimming_pattern.format(**wildcards)

def post_trimming_qa_output(wildcards):
    return post_trimming_output.format(**wildcards)

rule post_trimming_quality_assessment:
    """
    Perform a post-trimming quality check 
    of the reads from the sequencer.
    This calls docker in the command that is run.
    """
    input:
        post_trimming_input
    output:
        post_trimming_output
    message: 
        '''--- Post-trim quality check of trimmed data with fastqc.'''
    threads: 2 
    run:
        # get output file name with wildcards replaced
        post_trimming_output_wc = post_trimming_output.format(**wildcards)

        # get input file name with wildcards replaced
        post_trimming_input_wc = post_trimming_input.format(**wildcards)

        # get directory for final output
        output_dir = dirname(post_trimming_output_wc)

        shell('''
            docker run \
                -u "$(id -u):$(id -g)" \
                -v {PWD}/{data_dir}:/{data_dir} \
                -it \
                {fastqc_image} \
                fastqc \
                -t {threads} \
                /{post_trimming_input_wc} \
                -o /{data_dir}
        ''')


###################################
# Read Filtering: interleave

from os.path import join

# essentially
# feeding trimmed 1 and 2 in
# pointing to directory out

try:
    interleave_ext = readfilt['interleaving']['interleave_suffix']
except KeyError:
    interleave_ext = ""

try:
    # these are strings containing templates of .fq.gz file names
    fq_fwd_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=1,sample='{sample}',qual='{qual}'))
    fq_rev_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=2,sample='{sample}',qual='{qual}'))
    fq_interleave  = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=interleave_ext,sample='{sample}',qual='{qual}'))
except KeyError:
    fq_fwd_trimmed = ""
    fq_rev_trimmed = ""
    fq_interleave  = ""

interleave_input = [fq_fwd_trimmed, fq_rev_trimmed]
interleave_output = fq_interleave
khmer_image = container_image_name(biocontainers,'khmer')
quality_log = join(data_dir,'interleave_{sample}.log')

def interleave_reads_inputs(wildcards):
    # inputs should include the data directory prefix
    # do wildcard substitution
    interleave_input_wc = [x.format(**wildcards) for x in interleave_input]

    # for docker container, turn prefix 'data/' into '/data/'
    interleave_input_wc = ["/%s"%(x) for x in interleave_input_wc]
    interleave_input_wcs = " ".join(interleave_input_wc)
    return interleave_input_wcs

def interleave_reads_output(wildcards):
    # wildcard substitution
    return interleave_output.format(**wildcards)

def interleave_reads_logfile(wildcards):
    # wildcard substitution
    return quality_log.format(**wildcards)

rule interleave_reads:
    """
    Interleave paired-end reads using khmer.
    The trim quality comes from the filename.

    This run directive calls docker directly.
    """
    input:
        interleave_input
    output:
        interleave_output
    message:
        """--- Interleaving read data."""
    #log:
    #    '{quality_log_wc}'
    run:
        quality_log_wc = quality_log.format(**wildcards)

        # input includes data/ prefix
        interleave_input_wc = [x.format(**wildcards) for x in interleave_input]
        # for docker container, turn prefix data/ into /data/
        interleave_input_wc = ["/%s"%(x) for x in interleave_input_wc]
        interleave_input_wcs = " ".join(interleave_input_wc)

        interleave_output_wc = interleave_output.format(**wildcards)

        shell('''
            docker run \
                -u $(id -u):$(id -g) \
                -v {PWD}/{data_dir}:/{data_dir} \
                -it \
                {khmer_image} \
                interleave-reads.py \
                {interleave_input_wcs} \
                --no-reformat \
                -o {interleave_output_wc} \
                --gzip
        ''')


###################################
# Read Filtering: quality trimming

from os.path import join

data_dir = config['data_dir']

try:
    # If you only want to substitute a subset of wildcards,
    # you can leave a wildcard untouched by substituting
    # the string {variable} for {variable}.
    # 
    # We use this trick several times in these rules.
    # 
    fq_fwd = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(direction=1,sample='{sample}'))
    fq_rev = join(data_dir, readfilt['read_patterns']['pre_trimming_pattern'].format(direction=2,sample='{sample}'))
except KeyError:
    fq_fwd = ""
    fq_rev = ""

quality_input = [fq_fwd, fq_rev]

try:
    fq_fwd_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=1,sample='{sample}',qual='{qual}'))
    fq_rev_trimmed = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'].format(direction=2,sample='{sample}',qual='{qual}'))
except KeyError:
    fq_fwd_trimmed = "" # if we leave these blank,
    fq_rev_trimmed = "" # snakemake complains about duplicate output targets

try:
    trim_target_ext = readfilt['quality_trimming']['trim_suffix']
except KeyError:
    trim_target_ext = ""

fq_fwd_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_fwd_trimmed)
fq_rev_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_rev_trimmed)

quality_output = [fq_fwd_trimmed, fq_fwd_se,
                  fq_rev_trimmed, fq_rev_se]
if(quality_output==['','','','']):
    quality_output = ''

try:
    adapter_file = join(data_dir, readfilt['adapter_file']['name'])
except KeyError:
    adapter_file = "_"

quality_log = join(data_dir,'trimmomatic_pe_{sample}.log')
trimmo_image = container_image_name(biocontainers,'trimmomatic')

def quality_trimming_quality_log(wildcards):
    return quality_log.format(**wildcards)

def quality_trimming_qual(wildcards):
    return "{qual}".format(**wildcards)

def quality_trimming_quality_input(wildcards):
    # input includes data/ prefix
    quality_input_wc = [x.format(**wildcards) for x in quality_input]
    # for docker container, turn prefix data/ into /data/
    quality_input_wc = ["/%s"%(x) for x in quality_input_wc]
    quality_input_wcs = " ".join(quality_input_wc)
    return quality_input_wcs

def quality_trimming_quality_output(wildcards):
    quality_output_wc = [x.format(**wildcards) for x in quality_output]
    quality_output_wc = ["/%s"%(x) for x in quality_output_wc]
    quality_output_wcs = " ".join(quality_output_wc)
    return quality_output_wcs

rule quality_trimming:
    """
    Trim reads from the sequencer by dropping low-quality reads.
    This calls docker directly in the command that is run.
    """
    input:
        quality_input, adapter_file, pulled_containers_touchfile_docker
    output:
        quality_output
    message: 
        """--- Quality trimming read data."""
    run:
        qual_wc = "{qual}".format(**wildcards)

        # input includes data/ prefix
        quality_input_wc = [x.format(**wildcards) for x in quality_input]
        # for docker container, turn prefix data/ into /data/
        quality_input_wc = ["/%s"%(x) for x in quality_input_wc]
        quality_input_wcs = " ".join(quality_input_wc)

        quality_output_wc = [x.format(**wildcards) for x in quality_output]
        quality_output_wc = ["/%s"%(x) for x in quality_output_wc]
        quality_output_wcs = " ".join(quality_output_wc)

        quality_log_wc = quality_log.format(**wildcards)

        shell('''
            docker run \
                -u $(id -u):$(id -g) \
                -v {PWD}/{data_dir}:/{data_dir} \
                -it \
                {trimmo_image} \
                trimmomatic PE \
                {quality_input_wcs} \
                {quality_output_wcs} \
                ILLUMINACLIP:/{adapter_file}:2:40:15 \
                LEADING:{qual_wc} \
                TRAILING:{qual_wc} \
                SLIDINGWINDOW:4:{qual_wc} \
                MINLEN:25 \
                2> {quality_log_wc}
        ''')



###################################
# Read Filtering: high level rules

# These "ghost" rules don't do anything 
# except trigger other rules.

rule piggly_wiggly:
    """Pull all the biocontainers."""
    input:
        expand("{data_dir}/{pullfile}", data_dir=data_dir, pullfile=pulled_containers_touchfile_singularity)

rule fetch:
    input:
        expand(pre_trimming_pattern, sample="SRR606249", direction=["1","2"])

rule trim:
    input:
        expand(post_trimming_pattern, sample="SRR606249", direction=["1","2"], qual=["2","30"])



