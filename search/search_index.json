{
    "docs": [
        {
            "location": "/",
            "text": "Dahak\n\u00b6\n\n\nDahak is a software suite that integrates state-of-the-art open source tools\nfor metagenomic analyses. Tools in the dahak software suite will perform\nvarious steps in metagenomic analysis workflows including data pre-processing,\nmetagenome assembly, taxonomic and functional classification, genome binning,\nand gene assignment. We aim to deliver the analytical framework as a robust and\nreliable containerized workflow system, which will be free from dependency,\ninstallation, and execution problems typically associated with other\nopen-source bioinformatics solutions. This will maximize the transparency, data\nprovenance (i.e., the process of tracing the origins of data and its movement\nthrough the workflow), and reproducibility.\n\n\nBenchmarking Data\n\u00b6\n\n\nFor purposes of benchmarking this project will use the following datasets: \n\n\n\n\n\n\n\n\nDataset\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nShakya complete\n\n\nComplete metagenomic dataset from Shakya et al., 2013\n*\n containing bacterial and archaeal genomes\n\n\n\n\n\n\nShakya subset 50\n\n\n50 percent of the reads from Shakya complete\n\n\n\n\n\n\nShakya subset 25\n\n\n25 percent of the reads from Shakya complete\n\n\n\n\n\n\nShakya subset 10\n\n\n10 percent of the reads from Shakya complete\n\n\n\n\n\n\n\n\n*\nShakya, M., C. Quince, J. H. Campbell, Z. K. Yang, C. W. Schadt and M. Podar (2013). \"Comparative metagenomic and rRNA microbial diversity characterization using archaeal and bacterial synthetic communities.\" Environ Microbiol 15(6): 1882-1899.\n\n\nRequirements and Installation\n\u00b6\n\n\nDahak is not a standalone program, but rather a collection of workflows\nthat are defined in Snakemake files. These workflows utilize Bioconda,\nBiocontainers, and Docker/Singularity containerization technologies to\ninstall and run software for different tasks.\n\n\nThe following software is required to run Dahak workflows:\n\n\nREQUIRED:\n\n\n\n\nPython 3\n\n\nSnakemake\n\n\nConda\n\n\nSingularity or Docker\n\n\n\n\nTARGET PLATFORM:\n\n\n\n\n(Required) Singularity >= 2.4 (does not require sudo access) or Docker\n  (requires sudo access)\n\n\n(Optional) Sun Grid Compute Engine\n\n\n(Optional) Ubuntu 16.04 (Xenial)\n\n\n\n\nSee the \nInstalling\n page for detailed instructions\non installing each of the required components listed above,\nincluding Singularity and Docker.\n\n\nSee the \nQuickstart\n page for instructions on getting \nstarted running dahak workflows with Snakemake.\n\n\nWorkflows\n\u00b6\n\n\nDahak provides a set of workflow components that all fit together to perform \nvarious useful tasks. \n\n\nSee the \nRunning Workflows\n page for some background\non how to run workflows using singularity and snakemake.\n\n\nSee the \nQuickstart\n guide if you just want to get\nup and running with workflows.\n\n\nOur \ntarget compute system\n is a generic cluster running Sun Grid Engine\nin an HPC environment; all Snakemake files are written for this target\nsystem.\n\n\nList of workflows:\n\n\n\n\nRead Filtering\n\n\nAssembly\n\n\nMetagenomic Comparison\n\n\nTaxonomic Classification\n\n\nVariant Calling\n\n\nFunctional Inference\n\n\n\n\nTo get started with a particular workflow, select it from the \nnavigation menu on the left side of the page.\n\n\nParameters and Configuration\n\u00b6\n\n\nSee the \nParameters and Configuration\n page for details about\ncontrolling how each workflow operates, and how to use parameter presets.\n\n\nContributing\n\u00b6\n\n\nPlease read\n\nCONTRIBUTING.md\n\nfor details on our code of conduct and the process for submitting pull requests to us.\n\n\nContributors\n\u00b6\n\n\nPhillip Brooks\n1\n, Charles Reid\n1\n, Bruce Budowle\n2\n, Chris Grahlmann\n3\n, Stephanie L. Guertin\n3\n, F. Curtis Hewitt\n3\n, Alexander F. Koeppel\n4\n, Oana I. Lungu\n3\n, Krista L. Ternus\n3\n, Stephen D. Turner\n4,\n5\n, C. Titus Brown\n1\n\n\n1\nSchool of Veterinary Medicine, University of California Davis, Davis, CA, United States of America \n\n\n2\nInstitute of Applied Genetics, Department of Molecular and Medical Genetics, University of North Texas Health Science Center, Fort Worth, Texas, United States of America\n\n\n3\nSignature Science, LLC, Austin, Texas, United States of America\n\n\n4\nDepartment of Public Health Sciences, University of Virginia, Charlottesville, VA, United States of America\n\n\n5\nBioinformatics Core, University of Virginia School of Medicine, Charlottesville, VA, United States of America\n\n\nSee also the list of \ncontributors\n who participated in this project.\n\n\nLicense\n\u00b6\n\n\nThis project is licensed under the BSD 3-Clause License - see the\n\nLICENSE\n file\nfor details.",
            "title": "Index"
        },
        {
            "location": "/#dahak",
            "text": "Dahak is a software suite that integrates state-of-the-art open source tools\nfor metagenomic analyses. Tools in the dahak software suite will perform\nvarious steps in metagenomic analysis workflows including data pre-processing,\nmetagenome assembly, taxonomic and functional classification, genome binning,\nand gene assignment. We aim to deliver the analytical framework as a robust and\nreliable containerized workflow system, which will be free from dependency,\ninstallation, and execution problems typically associated with other\nopen-source bioinformatics solutions. This will maximize the transparency, data\nprovenance (i.e., the process of tracing the origins of data and its movement\nthrough the workflow), and reproducibility.",
            "title": "Dahak"
        },
        {
            "location": "/#benchmarking-data",
            "text": "For purposes of benchmarking this project will use the following datasets:      Dataset  Description      Shakya complete  Complete metagenomic dataset from Shakya et al., 2013 *  containing bacterial and archaeal genomes    Shakya subset 50  50 percent of the reads from Shakya complete    Shakya subset 25  25 percent of the reads from Shakya complete    Shakya subset 10  10 percent of the reads from Shakya complete     * Shakya, M., C. Quince, J. H. Campbell, Z. K. Yang, C. W. Schadt and M. Podar (2013). \"Comparative metagenomic and rRNA microbial diversity characterization using archaeal and bacterial synthetic communities.\" Environ Microbiol 15(6): 1882-1899.",
            "title": "Benchmarking Data"
        },
        {
            "location": "/#requirements-and-installation",
            "text": "Dahak is not a standalone program, but rather a collection of workflows\nthat are defined in Snakemake files. These workflows utilize Bioconda,\nBiocontainers, and Docker/Singularity containerization technologies to\ninstall and run software for different tasks.  The following software is required to run Dahak workflows:  REQUIRED:   Python 3  Snakemake  Conda  Singularity or Docker   TARGET PLATFORM:   (Required) Singularity >= 2.4 (does not require sudo access) or Docker\n  (requires sudo access)  (Optional) Sun Grid Compute Engine  (Optional) Ubuntu 16.04 (Xenial)   See the  Installing  page for detailed instructions\non installing each of the required components listed above,\nincluding Singularity and Docker.  See the  Quickstart  page for instructions on getting \nstarted running dahak workflows with Snakemake.",
            "title": "Requirements and Installation"
        },
        {
            "location": "/#workflows",
            "text": "Dahak provides a set of workflow components that all fit together to perform \nvarious useful tasks.   See the  Running Workflows  page for some background\non how to run workflows using singularity and snakemake.  See the  Quickstart  guide if you just want to get\nup and running with workflows.  Our  target compute system  is a generic cluster running Sun Grid Engine\nin an HPC environment; all Snakemake files are written for this target\nsystem.  List of workflows:   Read Filtering  Assembly  Metagenomic Comparison  Taxonomic Classification  Variant Calling  Functional Inference   To get started with a particular workflow, select it from the \nnavigation menu on the left side of the page.",
            "title": "Workflows"
        },
        {
            "location": "/#parameters-and-configuration",
            "text": "See the  Parameters and Configuration  page for details about\ncontrolling how each workflow operates, and how to use parameter presets.",
            "title": "Parameters and Configuration"
        },
        {
            "location": "/#contributing",
            "text": "Please read CONTRIBUTING.md \nfor details on our code of conduct and the process for submitting pull requests to us.",
            "title": "Contributing"
        },
        {
            "location": "/#contributors",
            "text": "Phillip Brooks 1 , Charles Reid 1 , Bruce Budowle 2 , Chris Grahlmann 3 , Stephanie L. Guertin 3 , F. Curtis Hewitt 3 , Alexander F. Koeppel 4 , Oana I. Lungu 3 , Krista L. Ternus 3 , Stephen D. Turner 4, 5 , C. Titus Brown 1  1 School of Veterinary Medicine, University of California Davis, Davis, CA, United States of America   2 Institute of Applied Genetics, Department of Molecular and Medical Genetics, University of North Texas Health Science Center, Fort Worth, Texas, United States of America  3 Signature Science, LLC, Austin, Texas, United States of America  4 Department of Public Health Sciences, University of Virginia, Charlottesville, VA, United States of America  5 Bioinformatics Core, University of Virginia School of Medicine, Charlottesville, VA, United States of America  See also the list of  contributors  who participated in this project.",
            "title": "Contributors"
        },
        {
            "location": "/#license",
            "text": "This project is licensed under the BSD 3-Clause License - see the LICENSE  file\nfor details.",
            "title": "License"
        },
        {
            "location": "/installing/",
            "text": "Installation Instructions\n\u00b6\n\n\nBefore getting started with Dahak, you will need the following software installed:\n\n\nREQUIRED:\n\n\n\n\nDocker or Singularity\n\n\nPython 3\n\n\nConda\n\n\nSnakemake\n\n\n\n\nThe instructions below will help you get started running the software above.\n\n\n\n\n\n\nInstalling Docker or Singularity\n\u00b6\n\n\n(Recommended) Singularity\n\u00b6\n\n\nSingularity\n is a tool for running Docker\ncontainers in higher security environments (e.g., high performance computing\nclusters) where permissions are restricted. If you wish to use Docker directly\nand have root access (e.g., with AWS/cloud machines), see the \"Getting Started\nwith Docker\" section below.\n\n\nInstalling a stable version of Singularity is recommended. Stable versions can\nbe obtained from \nSingularity's Releases on\nGithub\n.\n\n\n(In an HPC environment, these commands are run by the system administrator.)\n\n\n# Latest\nVERSION=2.5.1\n\n# More widely available\nVERSION=2.4.6\n\nwget https://github.com/singularityware/singularity/releases/download/$VERSION/singularity-$VERSION.tar.gz\ntar xvf singularity-$VERSION.tar.gz\ncd singularity-$VERSION\n./configure --prefix=/usr/local\nmake\nsudo make install\n\n\n\n\n\nTo check whether Singularity is installed (in an HPC environment, this command\nis run by the user), check the version:\n\n\nsingularity --version\n\n\n\n\n\n(Optional) Docker\n\u00b6\n\n\nIf you wish to follow along with the walkthroughs, which cover the use of\nDocker containers to run the workflows interactively, you will need to install\nDocker, which requires root access.\n\n\nAlternatively, Dahak provides Snakefiles for automating these workflows without\nrequiring root access by using Singularity (see instructions above). \n\n\nFirst, update your machine:\n\n\n# Update aptitude and install dependencies\nsudo apt-get -y update && sudo apt-get install zlib1g-dev ncurses-dev\n\n\n\n\n\nNext, install Docker:\n\n\n# Install Docker\nwget -qO- https://get.docker.com/ | sudo sh\nsudo usermod -aG docker ubuntu\n\n\n\n\n\n\n\n\n\nInstalling Python\n\u00b6\n\n\nTo run Dahak workflows, we utilize \nSnakemake\n,\na Python build tool. To use Snakemake, we must first install Python.\n\n\nTo ensure a universal installation process, we will use \npyenv\n\nto install the correct versions of Python and Conda.\n\n\nThere are many versions of Python, so your setup may vary.\nWe provide installation instructions using two methods:\n\n\n\n\nAptitude-installed Python (requires root)\n\n\nPyenv-managed Python (non-root)\n\n\n\n\nPyenv is a command-line tool for managing multiple\nPython versions, including Conda.\n\n\n(Recommended) Installing Aptitude Python\n\u00b6\n\n\nTo install Python with Aptitude:\n\n\nsudo apt-get -y update\nsudo apt-get -y install python-pip python-dev\n\n\n\n\n\n(Optional) Installing Pyenv\n\u00b6\n\n\nStart by running the pyenv installer:\n\n\n# Install pyenv \ncurl -L https://raw.githubusercontent.com/pyenv/pyenv-installer/master/bin/pyenv-installer | bash\n\n\n\n\n\nAdd pyenv's bin directory to \n$PATH\n (should already be in \n~/.bash_profile\n but just in case):\n\n\necho 'export PATH=\"~/.pyenv/bin:$PATH\"' >> ~/.bash_profile\nsource ~/.bash_profile\n\n\n\n\n\nYou should see a path with a \n.pyenv\n directory in it when you type \n\nwhich pyenv\n.\n\n\nYou can now install various Python versions (we will\ninstall a version of Conda below):\n\n\nPYVERSION=\"3.6.5\"\nPYVERSION=\"anaconda3-5.1.0\"\nPYVERSION=\"miniconda3-4.3.30\"\n\n\n\n\n\nTo install it:\n\n\npyenv install $PYVERSION\n\n\n\n\n\nTo make it available on the \n$PATH\n (to make it the\nversion of Python that \npython\n on the command line\npoints to):\n\n\npyenv global $PYVERSION\neval \"$(pyenv init -)\"\n\n\n\n\n\nTo make this change permanent, you can add the\npyenv init statement to your \n~/.bash_profile\n:\n\n\necho 'eval \"$(pyenv init -)\"' >> ~/.bash_profile\nsource ~/.bash_profile\n\n\n\n\n\nYou should also have a version of \npip\n associated\nwith \npython\n:\n\n\npip install --upgrade pip\n\n\n\n\n\n\n\n\n\nInstalling Conda\n\u00b6\n\n\nNow we set the version of conda we wish to install. We will install\nMiniconda 4.3.30 with Python 3:\n\n\nCONDA=\"miniconda3-4.3.30\"\npyenv install $CONDA\npyenv global $CONDA\neval \"$(pyenv init -)\"\n\n\n\n\n\nYou can also add this to your bash profile to ensure that the \nglobal pyenv Python version is always the first version of Python\non your path:\n\n\necho 'eval \"$(pyenv init -)\"' >> ~/.bash_profile\nsource ~/.bash_profile\n\n\n\n\n\nNow check to make sure you have the pyenv-installed version of conda: \n\n\nwhich conda\nconda --version\npython --version\n\n\n\n\n\nAdd the required conda channels:\n\n\nconda update\nconda config --add channels r\nconda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda\n\n\n\n\n\n\n\n\n\nInstalling Snakemake\n\u00b6\n\n\nNow install snakemake from the bioconda channel,\nor install it using pip:\n\n\nconda install -c bioconda snakemake\n\n\n\n\n\nor\n\n\npip install snakemake\n\n\n\n\n\nNote that this pip will correspond to the version of\nPython and Conda that are on the path.\n\n\nFinally, install the Open Science Framework CLI client\nusing pip:\n\n\npip install --user osfclient",
            "title": "Installing"
        },
        {
            "location": "/installing/#installation-instructions",
            "text": "Before getting started with Dahak, you will need the following software installed:  REQUIRED:   Docker or Singularity  Python 3  Conda  Snakemake   The instructions below will help you get started running the software above.",
            "title": "Installation Instructions"
        },
        {
            "location": "/installing/#installing-docker-or-singularity",
            "text": "",
            "title": "Installing Docker or Singularity"
        },
        {
            "location": "/installing/#recommended-singularity",
            "text": "Singularity  is a tool for running Docker\ncontainers in higher security environments (e.g., high performance computing\nclusters) where permissions are restricted. If you wish to use Docker directly\nand have root access (e.g., with AWS/cloud machines), see the \"Getting Started\nwith Docker\" section below.  Installing a stable version of Singularity is recommended. Stable versions can\nbe obtained from  Singularity's Releases on\nGithub .  (In an HPC environment, these commands are run by the system administrator.)  # Latest\nVERSION=2.5.1\n\n# More widely available\nVERSION=2.4.6\n\nwget https://github.com/singularityware/singularity/releases/download/$VERSION/singularity-$VERSION.tar.gz\ntar xvf singularity-$VERSION.tar.gz\ncd singularity-$VERSION\n./configure --prefix=/usr/local\nmake\nsudo make install  To check whether Singularity is installed (in an HPC environment, this command\nis run by the user), check the version:  singularity --version",
            "title": "(Recommended) Singularity"
        },
        {
            "location": "/installing/#optional-docker",
            "text": "If you wish to follow along with the walkthroughs, which cover the use of\nDocker containers to run the workflows interactively, you will need to install\nDocker, which requires root access.  Alternatively, Dahak provides Snakefiles for automating these workflows without\nrequiring root access by using Singularity (see instructions above).   First, update your machine:  # Update aptitude and install dependencies\nsudo apt-get -y update && sudo apt-get install zlib1g-dev ncurses-dev  Next, install Docker:  # Install Docker\nwget -qO- https://get.docker.com/ | sudo sh\nsudo usermod -aG docker ubuntu",
            "title": "(Optional) Docker"
        },
        {
            "location": "/installing/#installing-python",
            "text": "To run Dahak workflows, we utilize  Snakemake ,\na Python build tool. To use Snakemake, we must first install Python.  To ensure a universal installation process, we will use  pyenv \nto install the correct versions of Python and Conda.  There are many versions of Python, so your setup may vary.\nWe provide installation instructions using two methods:   Aptitude-installed Python (requires root)  Pyenv-managed Python (non-root)   Pyenv is a command-line tool for managing multiple\nPython versions, including Conda.",
            "title": "Installing Python"
        },
        {
            "location": "/installing/#recommended-installing-aptitude-python",
            "text": "To install Python with Aptitude:  sudo apt-get -y update\nsudo apt-get -y install python-pip python-dev",
            "title": "(Recommended) Installing Aptitude Python"
        },
        {
            "location": "/installing/#optional-installing-pyenv",
            "text": "Start by running the pyenv installer:  # Install pyenv \ncurl -L https://raw.githubusercontent.com/pyenv/pyenv-installer/master/bin/pyenv-installer | bash  Add pyenv's bin directory to  $PATH  (should already be in  ~/.bash_profile  but just in case):  echo 'export PATH=\"~/.pyenv/bin:$PATH\"' >> ~/.bash_profile\nsource ~/.bash_profile  You should see a path with a  .pyenv  directory in it when you type  which pyenv .  You can now install various Python versions (we will\ninstall a version of Conda below):  PYVERSION=\"3.6.5\"\nPYVERSION=\"anaconda3-5.1.0\"\nPYVERSION=\"miniconda3-4.3.30\"  To install it:  pyenv install $PYVERSION  To make it available on the  $PATH  (to make it the\nversion of Python that  python  on the command line\npoints to):  pyenv global $PYVERSION\neval \"$(pyenv init -)\"  To make this change permanent, you can add the\npyenv init statement to your  ~/.bash_profile :  echo 'eval \"$(pyenv init -)\"' >> ~/.bash_profile\nsource ~/.bash_profile  You should also have a version of  pip  associated\nwith  python :  pip install --upgrade pip",
            "title": "(Optional) Installing Pyenv"
        },
        {
            "location": "/installing/#installing-conda",
            "text": "Now we set the version of conda we wish to install. We will install\nMiniconda 4.3.30 with Python 3:  CONDA=\"miniconda3-4.3.30\"\npyenv install $CONDA\npyenv global $CONDA\neval \"$(pyenv init -)\"  You can also add this to your bash profile to ensure that the \nglobal pyenv Python version is always the first version of Python\non your path:  echo 'eval \"$(pyenv init -)\"' >> ~/.bash_profile\nsource ~/.bash_profile  Now check to make sure you have the pyenv-installed version of conda:   which conda\nconda --version\npython --version  Add the required conda channels:  conda update\nconda config --add channels r\nconda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda",
            "title": "Installing Conda"
        },
        {
            "location": "/installing/#installing-snakemake",
            "text": "Now install snakemake from the bioconda channel,\nor install it using pip:  conda install -c bioconda snakemake  or  pip install snakemake  Note that this pip will correspond to the version of\nPython and Conda that are on the path.  Finally, install the Open Science Framework CLI client\nusing pip:  pip install --user osfclient",
            "title": "Installing Snakemake"
        },
        {
            "location": "/running_workflows/",
            "text": "Running Workflows\n\u00b6\n\n\nA flowchart illustrating how each workflow component fits \ntogether with tools into the overall process is included below:\n\n\n\n\nDahak workflows are run from the command line using Snakemake, a Python package\nthat provides similar capabilities to GNU make. Each workflow consists of a set\nof Snakemake rules.\n\n\nSnakemake\n is a Python program that assembles and\nruns tasks using a task graph approach. See \nInstalling\n for\ninstructions on how to install it. \n\n\nDahak workflows benefit directly from Snakemake's rich feature set and capabilities.\nThere is an extensive documentation page on \nexecuting Snakemake\n,\nand its command line options. There are other projects demonstrating ways of creating \n\nsnakemake-profiles\n, or platform-specific\nconfiguration profiles.\n\n\n\n\n\n\nHow To Run Workflows\n\u00b6\n\n\nGenerally, Snakemake is called by passing command line flags and the name of a\ntarget file or rule name:\n\n\n$ snakemake [FLAGS] <target>\n\n\n\n\n\nWhat targets are available?\n\u00b6\n\n\nTargets for each workflow are listed on the respective \n\"Snakemake Rules\" page for that workflow (see left side navigation menu).\n\n\nThere are two types of targets defined:\n\n\nTarget Files:\n The user can ask Snakemake to generate a particular file, and\nSnakemake will dynamically determine the rules that are required to generate the\nrequested file. Snakemake uses a dependency graph to determine what rules to run\nto generate the file.\n\n\nBuild Rules:\n There are rules that do not themselves do anything but that\ntrigger all of the rules in a given workflow. (The build rules work by\nassembling filenames and passing target filenames to Snakemake.)\n\n\nWhat targets should I use?\n\u00b6\n\n\nUsers should use the build rules to trigger workflows.\n\n\nThe build rules require workflow configuration details to be\nset using Snakemake's configuration dictionary. See \n\n\nEach workflow has a set of \"build rules\" that will trigger\nrules for a given workflow or portion of a workflow. Available\nbuild rules for each workflow are listed on the respective \n\"Snakemake Rules\" page for that workflow (see left side navigation menu).\n\n\nThe build rules require some information about which read files\nto run the workflow on; the information required is covered on\neach \"Snakemake Rules\" page.\n\n\nThe \nQuick Start\n covers some examples.\n\n\nHow do I specify workflow parameters?\n\u00b6\n\n\nWorkflow parameters are specified by passing a JSON configuration file to\nSnakemake.\n\n\nThe default workflow parameter values are set in \ndefault_workflowparams.settings\n.\nAny of these values can be overridden using a custom JSON file, as described\nabove and on the \nWorkflow Configuration\n page. \n\n\nFor example, to override the default version of trimmomatic (0.36) and use 0.38\ninstead, the following JSON would override the version to \n0.38--5\n:\n\n\n{\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.38--5\"\n        }\n    }\n}\n\n\n\n\n\nThis can be placed in a JSON file like \nconfig/custom_trimmomatic.json\n (in\nthe \nworkflows/\n directory) and passed to Snakemake using the \n--config\n flag like:\n\n\n$ snakemake --config=config/custom_trimmomatic.json \\\n        [FLAGS] <target>\n\n\n\n\n\nHow do I use Snakemake with Singularity?\n\u00b6\n\n\nSingularity is a containerization technology similar to Docker but without the\nneed for root access. Snakefiles in Dahak contain \nsingularity:\n directives,\nwhich specify a Singularity image to pull and use to run the given commands.\nThese directives are ignored by default, Snakemake must be run with the\n\n--use-singularity\n flag to run each command through a singularity container:\n\n\nsnakemake --use-singularity <target>\n\n\n\n\n\nWhen Singularity containers are run, a host directory can be bind-mounted\ninside the container to provide a shared-access folder on the host filesystem.\n\n\nTo specify a directory for Singularity to bind-mount, use the\n\nSINGULARITY_BINDPATH\n environment variable:\n\n\n$ SINGULARITY_BINDPATH=\"my_data:/data\" snakemake --use-singularity <target>\n\n\n\n\n\nThis bind-mounts the directory \nmy_data/\n into the Singularity container at \n/data/\n.\n\n\nWhere will data files live?\n\u00b6\n\n\nTo set the scratch/working directory for the Snakemake workflows, \nin which all intermediate and final data files will be placed, \nset the \ndata_dir\n key in the Snakemake configuration dictionary. \nIf this option is not set, it will be \ndata\n by default.\n\n\n(No trailing slash is needed when specifying the directory name.)\n\n\n\n\nSingularity Bind Path\n\n\nIf you use a custom directory by setting the \ndata_dir\n key,\nyou must also adjust the \nSINGULARITY_BINDPATH\n variable\naccordingly.\n\n\n\n\nFor example, to put all intermediate files into the \nwork/\n directory\ninstead of the \ndata/\n directory, the following very short JSON file\ncould be used for the Snakemake configuration dictionary (this would \nuse default values for everything except \ndata_dir\n):\n\n\n{\n    \"data_dir\" : \"work\"\n}\n\n\n\n\n\nThis JSON file can be used as the Snakemake configuration dictionary\nby passing the JSON file name to the \n--configfile\n flag to Snakemake\nand updating the Singularity environment variable:\n\n\n$ SINGULARITY_BINDPATH=\"work:/work\" \\\n        snakemake --configfile=config/custom_scratch.settings \\\n        [FLAGS] <target>\n\n\n\n\n\nHow do I customize my workflow with custom configuration files?\n\u00b6\n\n\nSee the \nSnakemake Configuration\n page.\n\n\nSummary\n\u00b6\n\n\nAll together, the command to run a Dahak workflow will \nlook like this:\n\n\n$ SINGULARITY_BINDPATH=\"data:/data\" snakemake \\\n    --configfile my_workflow_params.json \\\n    --use-singularity \\\n    <target>",
            "title": "Running Workflows"
        },
        {
            "location": "/running_workflows/#running-workflows",
            "text": "A flowchart illustrating how each workflow component fits \ntogether with tools into the overall process is included below:   Dahak workflows are run from the command line using Snakemake, a Python package\nthat provides similar capabilities to GNU make. Each workflow consists of a set\nof Snakemake rules.  Snakemake  is a Python program that assembles and\nruns tasks using a task graph approach. See  Installing  for\ninstructions on how to install it.   Dahak workflows benefit directly from Snakemake's rich feature set and capabilities.\nThere is an extensive documentation page on  executing Snakemake ,\nand its command line options. There are other projects demonstrating ways of creating  snakemake-profiles , or platform-specific\nconfiguration profiles.",
            "title": "Running Workflows"
        },
        {
            "location": "/running_workflows/#how-to-run-workflows",
            "text": "Generally, Snakemake is called by passing command line flags and the name of a\ntarget file or rule name:  $ snakemake [FLAGS] <target>",
            "title": "How To Run Workflows"
        },
        {
            "location": "/running_workflows/#what-targets-are-available",
            "text": "Targets for each workflow are listed on the respective \n\"Snakemake Rules\" page for that workflow (see left side navigation menu).  There are two types of targets defined:  Target Files:  The user can ask Snakemake to generate a particular file, and\nSnakemake will dynamically determine the rules that are required to generate the\nrequested file. Snakemake uses a dependency graph to determine what rules to run\nto generate the file.  Build Rules:  There are rules that do not themselves do anything but that\ntrigger all of the rules in a given workflow. (The build rules work by\nassembling filenames and passing target filenames to Snakemake.)",
            "title": "What targets are available?"
        },
        {
            "location": "/running_workflows/#what-targets-should-i-use",
            "text": "Users should use the build rules to trigger workflows.  The build rules require workflow configuration details to be\nset using Snakemake's configuration dictionary. See   Each workflow has a set of \"build rules\" that will trigger\nrules for a given workflow or portion of a workflow. Available\nbuild rules for each workflow are listed on the respective \n\"Snakemake Rules\" page for that workflow (see left side navigation menu).  The build rules require some information about which read files\nto run the workflow on; the information required is covered on\neach \"Snakemake Rules\" page.  The  Quick Start  covers some examples.",
            "title": "What targets should I use?"
        },
        {
            "location": "/running_workflows/#how-do-i-specify-workflow-parameters",
            "text": "Workflow parameters are specified by passing a JSON configuration file to\nSnakemake.  The default workflow parameter values are set in  default_workflowparams.settings .\nAny of these values can be overridden using a custom JSON file, as described\nabove and on the  Workflow Configuration  page.   For example, to override the default version of trimmomatic (0.36) and use 0.38\ninstead, the following JSON would override the version to  0.38--5 :  {\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.38--5\"\n        }\n    }\n}  This can be placed in a JSON file like  config/custom_trimmomatic.json  (in\nthe  workflows/  directory) and passed to Snakemake using the  --config  flag like:  $ snakemake --config=config/custom_trimmomatic.json \\\n        [FLAGS] <target>",
            "title": "How do I specify workflow parameters?"
        },
        {
            "location": "/running_workflows/#how-do-i-use-snakemake-with-singularity",
            "text": "Singularity is a containerization technology similar to Docker but without the\nneed for root access. Snakefiles in Dahak contain  singularity:  directives,\nwhich specify a Singularity image to pull and use to run the given commands.\nThese directives are ignored by default, Snakemake must be run with the --use-singularity  flag to run each command through a singularity container:  snakemake --use-singularity <target>  When Singularity containers are run, a host directory can be bind-mounted\ninside the container to provide a shared-access folder on the host filesystem.  To specify a directory for Singularity to bind-mount, use the SINGULARITY_BINDPATH  environment variable:  $ SINGULARITY_BINDPATH=\"my_data:/data\" snakemake --use-singularity <target>  This bind-mounts the directory  my_data/  into the Singularity container at  /data/ .",
            "title": "How do I use Snakemake with Singularity?"
        },
        {
            "location": "/running_workflows/#where-will-data-files-live",
            "text": "To set the scratch/working directory for the Snakemake workflows, \nin which all intermediate and final data files will be placed, \nset the  data_dir  key in the Snakemake configuration dictionary. \nIf this option is not set, it will be  data  by default.  (No trailing slash is needed when specifying the directory name.)   Singularity Bind Path  If you use a custom directory by setting the  data_dir  key,\nyou must also adjust the  SINGULARITY_BINDPATH  variable\naccordingly.   For example, to put all intermediate files into the  work/  directory\ninstead of the  data/  directory, the following very short JSON file\ncould be used for the Snakemake configuration dictionary (this would \nuse default values for everything except  data_dir ):  {\n    \"data_dir\" : \"work\"\n}  This JSON file can be used as the Snakemake configuration dictionary\nby passing the JSON file name to the  --configfile  flag to Snakemake\nand updating the Singularity environment variable:  $ SINGULARITY_BINDPATH=\"work:/work\" \\\n        snakemake --configfile=config/custom_scratch.settings \\\n        [FLAGS] <target>",
            "title": "Where will data files live?"
        },
        {
            "location": "/running_workflows/#how-do-i-customize-my-workflow-with-custom-configuration-files",
            "text": "See the  Snakemake Configuration  page.",
            "title": "How do I customize my workflow with custom configuration files?"
        },
        {
            "location": "/running_workflows/#summary",
            "text": "All together, the command to run a Dahak workflow will \nlook like this:  $ SINGULARITY_BINDPATH=\"data:/data\" snakemake \\\n    --configfile my_workflow_params.json \\\n    --use-singularity \\\n    <target>",
            "title": "Summary"
        },
        {
            "location": "/config/",
            "text": "Snakemake Configuration\n\u00b6\n\n\nThe \nworkflows/config/\n directory contains files used to set \nkey-value pairs in the configuration dictionary used by Snakemake.\n\n\nTo use a custom configuration file (JSON or YAML format), use the\n\n--configfile\n flag:\n\n\n$ snakemake --configfile my_workflow_params.json ...\n\n\n\n\n\nData Files, Workflow Config, and Parameters\n\u00b6\n\n\nThe \ndahak/workflows/config/\n directory contains configuration files for\nDahak workflows. There are three types of configuration details that the user\nmay wish to customize:\n\n\n\n\n\n\nData files\n (a list of names and URLs for read files; these read files do not\n  all necessarily need to be used in the workflow)\n\n\n\n\nSee \ndahak/workflows/config/example_datafiles.json\n\n  for an example\n\n\n\n\n\n\n\n\nWorkflow configuration\n (specifying which read files to run each workflow on)\n\n\n\n\nSee \ndahak/workflows/config/example_workflowconfig.json\n\n  for an example\n\n\n\n\n\n\n\n\nWorkflow parameters\n (parameters specifying how the workflow will run)\n\n\n\n\nSee \ndahak/workflows/config/example_workflowparams.json\n\n  for an example\n\n\n\n\n\n\n\n\nHow do I specify my data files?\n\u00b6\n\n\nTo set the names and URLs of read files, set the \nfiles\n key in the Snakemake\nconfiguration dictionary to a list of key-value pairs, where the key is the \nname of the read file and the value is the URL of the file (do not include \n\nhttp://\n or \nhttps://\n in the URL).\n\n\nFor example, the following JSON block will provide a list of reads and their\ncorresponding URLs:\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}\n\n\n\n\n\nThis can be placed in a JSON file like \ndahak/workflows/config/custom_datafiles.json\n \nand passed to Snakemake using the \n--config\n flag like:\n\n\n$ snakemake --config=config/custom_datafiles.json \\\n        [FLAGS] <target>\n\n\n\n\n\nNOTE:\n Dahak assumes that read files are not present on the local machine and\nuses a rule to download the reads from the given URL if they are not present.\nIf your read files \nare\n present locally, they must be put into the temporary\ndirectory (set by \ndata_dir\n key in the Snakemake configuration dictionary, \n\ndata/\n by default) so that Snakemake will find them. If Snakemake finds the \nread files, the download command will not be run. You can use an empty string\nfor the URLs in this case.\n\n\nHow do I specify my workflow configuration?\n\u00b6\n\n\nHow do I specify my workflow parameters?\n\u00b6\n\n\n.settings (Defaults) vs .json (Overriding Defaults)\n\u00b6\n\n\nThe \n*.settings\n files are Python scripts that set the default\nSnakemake configuration dictionary. Each \n*.settings\n file is\nprefixed by \ndefault_\n because it is setting default values.\n\n\nIf the user does not specify a particular configuration dictionary\nkey-value pair, then the default key-value pair will be used.\nHowever, if the user sets a key-value pair, it will override\nthe default value.\n\n\nThis allows the user to customize any configuration key-value pair\nused by a workflow without having to explicitly specify every \nconfiguration key-value pair.\n\n\nFor example, by default the version of each container image for each\nprogram obtained from the biocontainers project is specified in the\ntop-level \nbiocontainers\n key in \ndefault_parameters.settings\n:\n\n\nconfig_default = {\n\n    ...\n\n    \"biocontainers\" : {\n        \"sourmash\" : {\n            \"use_local\" : False,\n            \"quayurl\" : \"quay.io/biocontainers/sourmash\",\n            \"version\" : \"2.0.0a3--py36_0\"\n        },\n        \"trimmomatic\" : {\n            \"use_local\" : False,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        },\n        \"khmer\" : {\n            \"use_local\" : False,\n            \"quayurl\" : \"quay.io/biocontainers/khmer\",\n            \"version\" : \"2.1.2--py35_0\"\n        },\n\n        ...\n\n\n\n\n\nIf the user wishes to bump the version of trimmomatic to (e.g.) 0.38,\nbut not change the version of khmer or sourmash, the user can specify\na JSON file with the following configuration block:\n\n\n{\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.38--5\"\n        }\n    }\n}\n\n\n\n\n\nThis can be placed in a JSON file like \ndahak/workflows/config/custom_workflowparams.json\n \nand passed to Snakemake using the \n--config\n flag like:\n\n\n$ snakemake --config=config/custom_workflowparams.json \\\n        [FLAGS] <target>\n\n\n\n\n\nThis will only override the container version used for trimmomatic, and will\nuse the defaults for all other container images.",
            "title": "Workflow Configuration"
        },
        {
            "location": "/config/#snakemake-configuration",
            "text": "The  workflows/config/  directory contains files used to set \nkey-value pairs in the configuration dictionary used by Snakemake.  To use a custom configuration file (JSON or YAML format), use the --configfile  flag:  $ snakemake --configfile my_workflow_params.json ...",
            "title": "Snakemake Configuration"
        },
        {
            "location": "/config/#data-files-workflow-config-and-parameters",
            "text": "The  dahak/workflows/config/  directory contains configuration files for\nDahak workflows. There are three types of configuration details that the user\nmay wish to customize:    Data files  (a list of names and URLs for read files; these read files do not\n  all necessarily need to be used in the workflow)   See  dahak/workflows/config/example_datafiles.json \n  for an example     Workflow configuration  (specifying which read files to run each workflow on)   See  dahak/workflows/config/example_workflowconfig.json \n  for an example     Workflow parameters  (parameters specifying how the workflow will run)   See  dahak/workflows/config/example_workflowparams.json \n  for an example",
            "title": "Data Files, Workflow Config, and Parameters"
        },
        {
            "location": "/config/#how-do-i-specify-my-data-files",
            "text": "To set the names and URLs of read files, set the  files  key in the Snakemake\nconfiguration dictionary to a list of key-value pairs, where the key is the \nname of the read file and the value is the URL of the file (do not include  http://  or  https://  in the URL).  For example, the following JSON block will provide a list of reads and their\ncorresponding URLs:  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}  This can be placed in a JSON file like  dahak/workflows/config/custom_datafiles.json  \nand passed to Snakemake using the  --config  flag like:  $ snakemake --config=config/custom_datafiles.json \\\n        [FLAGS] <target>  NOTE:  Dahak assumes that read files are not present on the local machine and\nuses a rule to download the reads from the given URL if they are not present.\nIf your read files  are  present locally, they must be put into the temporary\ndirectory (set by  data_dir  key in the Snakemake configuration dictionary,  data/  by default) so that Snakemake will find them. If Snakemake finds the \nread files, the download command will not be run. You can use an empty string\nfor the URLs in this case.",
            "title": "How do I specify my data files?"
        },
        {
            "location": "/config/#how-do-i-specify-my-workflow-configuration",
            "text": "",
            "title": "How do I specify my workflow configuration?"
        },
        {
            "location": "/config/#how-do-i-specify-my-workflow-parameters",
            "text": "",
            "title": "How do I specify my workflow parameters?"
        },
        {
            "location": "/config/#settings-defaults-vs-json-overriding-defaults",
            "text": "The  *.settings  files are Python scripts that set the default\nSnakemake configuration dictionary. Each  *.settings  file is\nprefixed by  default_  because it is setting default values.  If the user does not specify a particular configuration dictionary\nkey-value pair, then the default key-value pair will be used.\nHowever, if the user sets a key-value pair, it will override\nthe default value.  This allows the user to customize any configuration key-value pair\nused by a workflow without having to explicitly specify every \nconfiguration key-value pair.  For example, by default the version of each container image for each\nprogram obtained from the biocontainers project is specified in the\ntop-level  biocontainers  key in  default_parameters.settings :  config_default = {\n\n    ...\n\n    \"biocontainers\" : {\n        \"sourmash\" : {\n            \"use_local\" : False,\n            \"quayurl\" : \"quay.io/biocontainers/sourmash\",\n            \"version\" : \"2.0.0a3--py36_0\"\n        },\n        \"trimmomatic\" : {\n            \"use_local\" : False,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        },\n        \"khmer\" : {\n            \"use_local\" : False,\n            \"quayurl\" : \"quay.io/biocontainers/khmer\",\n            \"version\" : \"2.1.2--py35_0\"\n        },\n\n        ...  If the user wishes to bump the version of trimmomatic to (e.g.) 0.38,\nbut not change the version of khmer or sourmash, the user can specify\na JSON file with the following configuration block:  {\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.38--5\"\n        }\n    }\n}  This can be placed in a JSON file like  dahak/workflows/config/custom_workflowparams.json  \nand passed to Snakemake using the  --config  flag like:  $ snakemake --config=config/custom_workflowparams.json \\\n        [FLAGS] <target>  This will only override the container version used for trimmomatic, and will\nuse the defaults for all other container images.",
            "title": ".settings (Defaults) vs .json (Overriding Defaults)"
        },
        {
            "location": "/quickstart/",
            "text": "Quick Start\n\u00b6\n\n\nNote for the \nvery\n impatient: skip straight to \nThe Really Quick Copy And Paste\nQuick Start\n section.\n\n\nGetting Set Up to Run Workflows\n\u00b6\n\n\nNOTE:\n This guide assumes familiarity with Dahak workflows and how they work.\nFor a more clear explanation of what's going on and how things work, \nstart with the \nRunning Workflows\n page.\n\n\nStart by cloning a copy of the repository:\n\n\n$ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak\n\n\n\n\n\nthen move into the \nworkflows/\n directory in the Dahak repository:\n\n\n$ cd dahak/workflows/\n\n\n\n\n\nThe quick start assumes that all commands are run from the \nworkflows/\n directory\nunless stated otherwise.\n\n\nAs covered on the \nRunning Workflows\n page, workflows are\nrun by using Snakemake from the command line. The basic syntax is:\n\n\n$ snakemake \n[\nFLAGS\n]\n <target>\n\n\n\n\n\n(See the \nexecuting\nSnakemake\n\npage of the Snakemake documentation for a full list of options that can be used\nwith Snakemake.)\n\n\nThe most important flag to pass is the \n--config\n flag, which allows users to\nspecify a Snakemake configuration dictionary to customize their Dahak workflows.\n\n\nThe user should also pass the \n--use-singularity\n flag to tell Snakemake to use\nSingularity containers. This also requires the \nSINGULARITY_BINDPATH\n variable\nto be set, to bind-mount a local directory into the container. \n\n\nThe user should use build targets. Each workflow's build targets and details\nare listed on the respective workflow's Snakemake Rules\" page.\n\n\nAll together, commands to run Dahak workflows will look like this:\n\n\n$ \nSINGULARITY_BINDPATH\n=\n\"data:/data\"\n \n\\\n\n        snakemake --use-singularity \n\\\n\n        \n[\nFLAGS\n]\n <target>\n\n\n\n\n\nConfiguring Snakemake\n\u00b6\n\n\nSee \nConfiguring Snakemake\n for more details about Snakemake\nconfiguration files. We cover the basics below.\n\n\nHow do I specify my data files?\n\u00b6\n\n\nSpecify the locations of your data files by assigning a key-value map\n(keys are filenames, values are URLs) to the \nfiles\n key:\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\"\n    }\n}\n\n\n\n\n\nThis JSON can be put into a JSON file like \nconfig/custom_readfilt.json\n and\npassed to Snakemake via the \n--configfile\n flag:\n\n\n$ SINGULARITY_BINDPATH=\"data:/data\" \\\n        snakemake --configfile=config/custom_datafiles.json \\\n        [FLAGS] <target>\n\n\n\n\n\nIf your read files are present locally, they must be in the same\ndirectory as the Snakemake working directory, which is specified by\nthe \ndata_dir\n key (\ndata/\n by default).\n\n\n\n\nExamples in the Repository\n\n\nAlso see \nworkflows/config/example_datafiles.json\n\nin the Dahak repository, as well as the \"Snakemake\" page \nfor each respective workflow.\n\n\n\n\nHow do I specify my workflow configuration?\n\u00b6\n\n\nThe \nworkflow configuration\n section of the Snakemake configuration dictionary\ncontains a subset of parameters that will end up in the final file name of the\noutput file generated by that workflow.  This is generally a small number of\nparameters.\n\n\nEach workflow has different build rules requiring different information.\nSee the \"Snakemake Rules\" page for a list of available build rules.\nSee the \"Snakemake Configuration\" page for a list of key-value pairs the\nbuild rule extracts from the Snakemake configuration dictionary.\n\n\nFor example, to evaluate the quality of reads from a sequencer after quality\ntrimming, the \nread_filtering_posttrim_workflow\n Snakemake rule is used, and the\nworkflow configuration JSON looks like this:\n\n\n{\n    \"workflows\" : {\n        \"read_filtering_posttrim_workflow\" : {\n            \"sample\"    : [\"SRR606249\",\"SRR606249_subset10\"],\n            \"qual\"   : [\"2\",\"30\"]\n        },\n    }\n}\n\n\n\n\n\nThe \nsample\n list specifies the prefixes of the sample reads to \nrun the read filtering workflow on. The \nqual\n list specifies the \nvalues to use for quality trimming.\n\n\nThis file can be put into a JSON file like \nconfig/custom_workflowconfig.json\n and\npassed to Snakemake via the \n--configfile\n flag:\n\n\n$ SINGULARITY_BINDPATH=\"data:/data\" \\\n        snakemake --configfile=config/custom_readfilt.json \\\n        [FLAGS] read_filtering_posttrim_workflow\n\n\n\n\n\n\n\nExamples in the Repository\n\n\nAlso see \nworkflows/config/example_workflowconfig.json\n\nin the Dahak repository, as well as the \"Snakemake\" page \nfor each respective workflow.\n\n\n\n\nHow do I specify my workflow parameters?\n\u00b6\n\n\nThe workflow parameters section of the Snakemake configuration file\nis where the details of each step of the workflow can be controlled.\n\n\nThe \nworkflow parameters\n section of the Snakemake configuration dictionary\ncontains all other parameters for all intermediate steps, and is therefore\nlonger and contains more options than the workflow configuration section.\n\n\nThe workflow parameters are organized by workflow, with one additional\nparameters section for setting the version and URL of the biocontainers \nused in each workflow.\n\n\nFor example, the read filtering workflow parameters section \nis under the top-level \nread_filtering\n key:\n\n\n{\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}.trim{qual}.fq.gz\",\n        },\n        \"direction_labels\" : {\n            \"forward\" : \"1\",\n            \"reverse\" : \"2\"\n        },\n        \"quality_assessment\" : {\n            \"fastqc_suffix\": \"fastqc\",\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        }\n    }\n}\n\n\n\n\n\nThis file can be put into a JSON file like \nconfig/custom_readfilt.json\n and\npassed to Snakemake via the \n--configfile\n flag:\n\n\n$ SINGULARITY_BINDPATH=\"data:/data\" \\\n        snakemake --configfile=config/custom_readfilt.json \\\n        [FLAGS] read_filtering_posttrim_workflow\n\n\n\n\n\n(See the \nRead Filtering Snakemake\n page\nfor an explanation of each of the above options.)\n\n\n\n\nExamples in the Repository\n\n\nThe \nworkflows/config/example_workflowparams.json\n\nfile in the repository contains an example, but JSON files\ndo not contain any comments. For a well-commented version,\ncheck the `workflows/config/default_workflowparams.settings](https://github.com/dahak-metagenomics/dahak/blob/snakemake/comparison/workflows/config/default_workflowparams.settings)\nfile.\n\n\n\n\nWhere will data files live?\n\u00b6\n\n\nTo set the scratch/working directory for the Snakemake workflows, \nin which all intermediate and final data files will be placed, \nset the \ndata_dir\n key in the Snakemake configuration dictionary. \nIf this option is not set, it will be \ndata\n by default.  No \ntrailing slash is needed when setting this option.\n\n\n\n\nImportant Note About \ndata_dir\n\n\nIf you use a custom directory, you must also \nadjust the \nSINGULARITY_BINDPATH\n variable accordingly.\n\n\n\n\nFor example, to put all intermediate files into the \nwork/\n directory\ninstead of the \ndata/\n directory, you can use the following JSON\nfile:\n\n\n{\n    \"data_dir\" : \"work\"\n}\n\n\n\n\n\nThis file can be put into a JSON file like \nconfig/custom_datadir.json\n and\npassed to Snakemake via the \n--configfile\n flag:\n\n\n$ SINGULARITY_BINDPATH=\"work:/work\" \\\n        snakemake --configfile=config/custom_datadir.settings \\\n        [FLAGS] <target>\n\n\n\n\n\n\n\n\n\nThe Really Quick Copy-And-Paste Quick Start\n\u00b6\n\n\nNow that we've provided some examples that you can use, let's run through\nthe entire process start to finish to illustrate how this works.\n\n\nRead Filtering\n\u00b6\n\n\nWe will run two variations of the read filtering workflow, and perform a quality\nassessment of our reads both before and after quality trimming.\n\n\nBefore you begin, make sure you have everything listed on the\n\nInstalling\n page available on your command line.\n\n\nStart by cloning a copy of the repository:\n\n\n$ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak\n\n\n\n\n\nthen move into the \nworkflows/\n directory of the Dahak repository:\n\n\n$ cd dahak/workflows/\n\n\n\n\n\nNow create a JSON file that defines a Snakemake configuration dictionary.\nThis file should:\n\n\n\n\nProvides URLs at which each read filtering file can be accessed\n\n\nProvides a set of quality trimming values to use (2 and 30)\n\n\nSets all read filtering parameters (for simplicity, we will set each\n    parameter to its default value)\n\n\n\n\n(See the \nRead Filtering Snakemake\n page for details on\nthese options.)\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\"\n    },\n\n    \"workflows\" : {\n        \"read_filtering_pretrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset25\",\"SRR606249_subset10\"],\n        },\n        \"read_filtering_posttrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset25\",\"SRR606249_subset10\"],\n            \"qual\"   : [\"2\",\"30\"]\n        },\n    },\n\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    },\n\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}.trim{qual}.fq.gz\",\n        },\n        \"direction_labels\" : {\n            \"forward\" : \"1\",\n            \"reverse\" : \"2\"\n        },\n        \"quality_assessment\" : {\n            \"fastqc_suffix\": \"fastqc\",\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        }\n    }\n}\n\n\n\n\n\nPut this file into \nconfig/custom_readfilt_workflow.json\n (in the \nworkflows\n\ndirectory of the repository). We want to run two workflows: one pre-trimming\nquality assessment, and one post-trimming quality assessment, so we call\nSnakemake and pass it two build targets: \nread_filtering_pretrim_workflow\n\nand \nread_filtering_posttrim_workflow\n.\n\n\nWe add two flags to the Snakemake command: \n-n\n and \n-p\n.\n\n\n-n\n does a \ndry-run\n, meaning Snakemake will print out what tasks it is\ngoing to run, but will not actually run them.\n\n\n-p\n tells Snakemake to print the shell commands that it will run for each\nworkflow step. This is useful for understanding what Snakemake is doing\nand what commands and options are being run.\n\n\n$ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p -n \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_pretrim_workflow\n\n$ snakemake -p -n \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_posttrim_workflow\n\n\n\n\n\nOnce we have reviewed the output from Snakemake and are satisfied\nit is running the correct workflow and commands, we can actually\nrun the workflow by removing the \n-n\n flag: \n\n\n$ export SINGULARITY_BINDPATH=\"data:/data\" \n\n$ snakemake -p \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_pretrim_workflow\n\n$ snakemake -p \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_posttrim_workflow\n\n\n\n\n\nWe can also run both workflows at once by specifying two targets:\n\n\n$ snakemake -p \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_pretrim_workflow read_filtering_posttrim_workflow\n\n\n\n\n\n\n\n\n\nAssembly\n\u00b6\n\n\nWe will run two assembler workflows using the two assemblers\nimplemented in Dahak: SPAdes and Megahit.\n\n\nBefore you begin, make sure you have everything listed on the\n\nInstalling\n page available on your command line.\n\n\nIf you have not already, clone a copy of the repository and move\nto the \nworkflows/\n directory:\n\n\n$ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak\n$ cd dahak/workflows/\n\n\n\n\n\nNow create a JSON file that defines a Snakemake configuration dictionary.\nThis file should:\n\n\n\n\nProvides URLs at which each read filtering file can be accessed\n\n\nProvides a set of quality trimming values to use (2 and 30)\n\n\nSets all read filtering parameters (for simplicity, we will set each\n    parameter to its default value)\n\n\n\n\n(See the \nAssembly Snakemake\n page for details on\nthese options.)\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\"\n    },\n\n    \"workflows\" : {\n        \"assembly_workflow_all\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        }\n    },\n\n    \"biocontainers\" : {\n        \"metaspades\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/spades\",\n            \"version\" : \"3.11.1--py27_zlib1.2.8_0\"\n        },\n        \"megahit\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/megahit\",\n            \"version\" : \"1.1.2--py35_0\"\n        }\n    },\n\n    \"assembly\" : {\n        \"assembly_patterns\" : {\n            \"metaspades_pattern\" : \"{sample}.trim{qual}_metaspades.contigs.fa\",\n            \"megahit_pattern\" : \"{sample}.trim{qual}_megahit.contigs.fa\",\n            \"assembly_pattern\" : \"{sample}.trim{qual}_{assembler}.contigs.fa\",\n            \"quast_pattern\" : \"{sample}.trim{qual}_{assembler}_quast/report.html\",\n            \"multiqc_pattern\" : \"{sample}.trim{qual}_{assembler}_multiqc/report.html\",\n        }\n    },\n\n}\n\n\n\n\n\nPut this file into \nconfig/custom_assembly_workflow.json\n (in the \nworkflows\n\ndirectory of the repository). We want to run the assembly workflow with\ntwo assemblers, so we call Snakemake and the \nassembly_workflow_all\n target.\n\n\n$ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p -n \\\n        --configfile=config/custom_assembly_workflow.json \\\n        assembly_workflow_all\n\n\n\n\n\nOnce we have reviewed the output from Snakemake and are satisfied\nit is running the correct workflow and commands, we can actually\nrun the workflow by removing the \n-n\n flag: \n\n\n$ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p \\\n        --configfile=config/custom_assembly_workflow.json \\\n        assembly_workflow_all\n\n\n\n\n\n\n\n\n\nComparison\n\u00b6\n\n\nIn this section we will run a comparison workflow to compute signatures for \nboth filtered reads and assemblies, and compare the computed signatures to \na reference database.\n\n\nBefore you begin, make sure you have everything listed on the\n\nInstalling\n page available on your command line.\n\n\nStart by cloning the repository and moving to the \nworkflows/\n directory:\n\n\n$ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak\n$ cd dahak/workflows/\n\n\n\n\n\nNow create a JSON file that defines a Snakemake configuration dictionary.\nThis file should:\n\n\n\n\nProvides URLs at which each read filtering file can be accessed\n\n\nProvides a set of quality trimming values to use (2 and 30)\n\n\nSets all read filtering parameters (for simplicity, we will set each\n    parameter to its default value)\n\n\n\n\n(See the \nComparison Snakemake\n page for details on\nthese options.)\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\"\n    },\n\n    \"workflows\" : {\n        \"comparison_workflow_reads_assembly\" : {\n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        }\n    },\n\n    \"biocontainers\" : {\n        \"sourmash_compare\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/sourmash\",\n            \"version\" : \"2.0.0a3--py36_0\"\n        }\n    },\n\n    \"comparison\" : {\n        \"compute_read_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\"    : \"_scaled10k.k21_31_51.sig\", \n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compute_assembly_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\" : \"_scaled10k.k21_31_51.sig\",\n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compare_read_assembly_signatures\" : {\n            \"samples\"   : [\"SRR606249_subset10\"],\n            \"assembler\" : [\"megahit\",\"metaspades\"],\n            \"kvalues\"   : [21, 31, 51],\n            \"csv_out\"   : \"SRR606249_trim2and30_ra_comparison.k{kvalue}.csv\"\n        }\n    }\n}\n\n\n\n\n\nNote that there are two additional keys within the \ncomparison\n configuration \nsub-dictionary, \ncompare_read_signatures\n and \ncompare_assembly_signatures\n,\nbut these sections are only used when comparing \njust\n reads (when passing the \n\ncomparison_workflow_reads\n target to Snakemake) or when comparing \njust\n\nassemblies (when passing the \ncomparison_workflow_assembly\n target to Snakemake).\n\n\nThe JSON above can be put into the file \nconfig/custom_comparison_workflow.json\n \n(in the \nworkflows\n directory of the repository), and the workflow can be run by\npassing the config file to Snakemake. It is important you run with the \n-n\n flag\nto do a dry-run first!\n\n\n$ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p -n \\\n        --configfile=config/custom_comparison_workflow.json \\\n        comparison_workflow_reads_assembly\n\n$ snakemake -p \\\n        --configfile=config/custom_comparison_workflow.json \\\n        comparison_workflow_reads_assembly",
            "title": "Quick Start"
        },
        {
            "location": "/quickstart/#quick-start",
            "text": "Note for the  very  impatient: skip straight to  The Really Quick Copy And Paste\nQuick Start  section.",
            "title": "Quick Start"
        },
        {
            "location": "/quickstart/#getting-set-up-to-run-workflows",
            "text": "NOTE:  This guide assumes familiarity with Dahak workflows and how they work.\nFor a more clear explanation of what's going on and how things work, \nstart with the  Running Workflows  page.  Start by cloning a copy of the repository:  $ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak  then move into the  workflows/  directory in the Dahak repository:  $ cd dahak/workflows/  The quick start assumes that all commands are run from the  workflows/  directory\nunless stated otherwise.  As covered on the  Running Workflows  page, workflows are\nrun by using Snakemake from the command line. The basic syntax is:  $ snakemake  [ FLAGS ]  <target>  (See the  executing\nSnakemake \npage of the Snakemake documentation for a full list of options that can be used\nwith Snakemake.)  The most important flag to pass is the  --config  flag, which allows users to\nspecify a Snakemake configuration dictionary to customize their Dahak workflows.  The user should also pass the  --use-singularity  flag to tell Snakemake to use\nSingularity containers. This also requires the  SINGULARITY_BINDPATH  variable\nto be set, to bind-mount a local directory into the container.   The user should use build targets. Each workflow's build targets and details\nare listed on the respective workflow's Snakemake Rules\" page.  All together, commands to run Dahak workflows will look like this:  $  SINGULARITY_BINDPATH = \"data:/data\"   \\ \n        snakemake --use-singularity  \\ \n         [ FLAGS ]  <target>",
            "title": "Getting Set Up to Run Workflows"
        },
        {
            "location": "/quickstart/#configuring-snakemake",
            "text": "See  Configuring Snakemake  for more details about Snakemake\nconfiguration files. We cover the basics below.",
            "title": "Configuring Snakemake"
        },
        {
            "location": "/quickstart/#how-do-i-specify-my-data-files",
            "text": "Specify the locations of your data files by assigning a key-value map\n(keys are filenames, values are URLs) to the  files  key:  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\"\n    }\n}  This JSON can be put into a JSON file like  config/custom_readfilt.json  and\npassed to Snakemake via the  --configfile  flag:  $ SINGULARITY_BINDPATH=\"data:/data\" \\\n        snakemake --configfile=config/custom_datafiles.json \\\n        [FLAGS] <target>  If your read files are present locally, they must be in the same\ndirectory as the Snakemake working directory, which is specified by\nthe  data_dir  key ( data/  by default).   Examples in the Repository  Also see  workflows/config/example_datafiles.json \nin the Dahak repository, as well as the \"Snakemake\" page \nfor each respective workflow.",
            "title": "How do I specify my data files?"
        },
        {
            "location": "/quickstart/#how-do-i-specify-my-workflow-configuration",
            "text": "The  workflow configuration  section of the Snakemake configuration dictionary\ncontains a subset of parameters that will end up in the final file name of the\noutput file generated by that workflow.  This is generally a small number of\nparameters.  Each workflow has different build rules requiring different information.\nSee the \"Snakemake Rules\" page for a list of available build rules.\nSee the \"Snakemake Configuration\" page for a list of key-value pairs the\nbuild rule extracts from the Snakemake configuration dictionary.  For example, to evaluate the quality of reads from a sequencer after quality\ntrimming, the  read_filtering_posttrim_workflow  Snakemake rule is used, and the\nworkflow configuration JSON looks like this:  {\n    \"workflows\" : {\n        \"read_filtering_posttrim_workflow\" : {\n            \"sample\"    : [\"SRR606249\",\"SRR606249_subset10\"],\n            \"qual\"   : [\"2\",\"30\"]\n        },\n    }\n}  The  sample  list specifies the prefixes of the sample reads to \nrun the read filtering workflow on. The  qual  list specifies the \nvalues to use for quality trimming.  This file can be put into a JSON file like  config/custom_workflowconfig.json  and\npassed to Snakemake via the  --configfile  flag:  $ SINGULARITY_BINDPATH=\"data:/data\" \\\n        snakemake --configfile=config/custom_readfilt.json \\\n        [FLAGS] read_filtering_posttrim_workflow   Examples in the Repository  Also see  workflows/config/example_workflowconfig.json \nin the Dahak repository, as well as the \"Snakemake\" page \nfor each respective workflow.",
            "title": "How do I specify my workflow configuration?"
        },
        {
            "location": "/quickstart/#how-do-i-specify-my-workflow-parameters",
            "text": "The workflow parameters section of the Snakemake configuration file\nis where the details of each step of the workflow can be controlled.  The  workflow parameters  section of the Snakemake configuration dictionary\ncontains all other parameters for all intermediate steps, and is therefore\nlonger and contains more options than the workflow configuration section.  The workflow parameters are organized by workflow, with one additional\nparameters section for setting the version and URL of the biocontainers \nused in each workflow.  For example, the read filtering workflow parameters section \nis under the top-level  read_filtering  key:  {\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}.trim{qual}.fq.gz\",\n        },\n        \"direction_labels\" : {\n            \"forward\" : \"1\",\n            \"reverse\" : \"2\"\n        },\n        \"quality_assessment\" : {\n            \"fastqc_suffix\": \"fastqc\",\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        }\n    }\n}  This file can be put into a JSON file like  config/custom_readfilt.json  and\npassed to Snakemake via the  --configfile  flag:  $ SINGULARITY_BINDPATH=\"data:/data\" \\\n        snakemake --configfile=config/custom_readfilt.json \\\n        [FLAGS] read_filtering_posttrim_workflow  (See the  Read Filtering Snakemake  page\nfor an explanation of each of the above options.)   Examples in the Repository  The  workflows/config/example_workflowparams.json \nfile in the repository contains an example, but JSON files\ndo not contain any comments. For a well-commented version,\ncheck the `workflows/config/default_workflowparams.settings](https://github.com/dahak-metagenomics/dahak/blob/snakemake/comparison/workflows/config/default_workflowparams.settings)\nfile.",
            "title": "How do I specify my workflow parameters?"
        },
        {
            "location": "/quickstart/#where-will-data-files-live",
            "text": "To set the scratch/working directory for the Snakemake workflows, \nin which all intermediate and final data files will be placed, \nset the  data_dir  key in the Snakemake configuration dictionary. \nIf this option is not set, it will be  data  by default.  No \ntrailing slash is needed when setting this option.   Important Note About  data_dir  If you use a custom directory, you must also \nadjust the  SINGULARITY_BINDPATH  variable accordingly.   For example, to put all intermediate files into the  work/  directory\ninstead of the  data/  directory, you can use the following JSON\nfile:  {\n    \"data_dir\" : \"work\"\n}  This file can be put into a JSON file like  config/custom_datadir.json  and\npassed to Snakemake via the  --configfile  flag:  $ SINGULARITY_BINDPATH=\"work:/work\" \\\n        snakemake --configfile=config/custom_datadir.settings \\\n        [FLAGS] <target>",
            "title": "Where will data files live?"
        },
        {
            "location": "/quickstart/#the-really-quick-copy-and-paste-quick-start",
            "text": "Now that we've provided some examples that you can use, let's run through\nthe entire process start to finish to illustrate how this works.",
            "title": "The Really Quick Copy-And-Paste Quick Start"
        },
        {
            "location": "/quickstart/#read-filtering",
            "text": "We will run two variations of the read filtering workflow, and perform a quality\nassessment of our reads both before and after quality trimming.  Before you begin, make sure you have everything listed on the Installing  page available on your command line.  Start by cloning a copy of the repository:  $ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak  then move into the  workflows/  directory of the Dahak repository:  $ cd dahak/workflows/  Now create a JSON file that defines a Snakemake configuration dictionary.\nThis file should:   Provides URLs at which each read filtering file can be accessed  Provides a set of quality trimming values to use (2 and 30)  Sets all read filtering parameters (for simplicity, we will set each\n    parameter to its default value)   (See the  Read Filtering Snakemake  page for details on\nthese options.)  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\"\n    },\n\n    \"workflows\" : {\n        \"read_filtering_pretrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset25\",\"SRR606249_subset10\"],\n        },\n        \"read_filtering_posttrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset25\",\"SRR606249_subset10\"],\n            \"qual\"   : [\"2\",\"30\"]\n        },\n    },\n\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    },\n\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}.trim{qual}.fq.gz\",\n        },\n        \"direction_labels\" : {\n            \"forward\" : \"1\",\n            \"reverse\" : \"2\"\n        },\n        \"quality_assessment\" : {\n            \"fastqc_suffix\": \"fastqc\",\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        }\n    }\n}  Put this file into  config/custom_readfilt_workflow.json  (in the  workflows \ndirectory of the repository). We want to run two workflows: one pre-trimming\nquality assessment, and one post-trimming quality assessment, so we call\nSnakemake and pass it two build targets:  read_filtering_pretrim_workflow \nand  read_filtering_posttrim_workflow .  We add two flags to the Snakemake command:  -n  and  -p .  -n  does a  dry-run , meaning Snakemake will print out what tasks it is\ngoing to run, but will not actually run them.  -p  tells Snakemake to print the shell commands that it will run for each\nworkflow step. This is useful for understanding what Snakemake is doing\nand what commands and options are being run.  $ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p -n \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_pretrim_workflow\n\n$ snakemake -p -n \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_posttrim_workflow  Once we have reviewed the output from Snakemake and are satisfied\nit is running the correct workflow and commands, we can actually\nrun the workflow by removing the  -n  flag:   $ export SINGULARITY_BINDPATH=\"data:/data\" \n\n$ snakemake -p \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_pretrim_workflow\n\n$ snakemake -p \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_posttrim_workflow  We can also run both workflows at once by specifying two targets:  $ snakemake -p \\\n        --configfile=config/custom_readfilt_workflow.json \\\n        read_filtering_pretrim_workflow read_filtering_posttrim_workflow",
            "title": "Read Filtering"
        },
        {
            "location": "/quickstart/#assembly",
            "text": "We will run two assembler workflows using the two assemblers\nimplemented in Dahak: SPAdes and Megahit.  Before you begin, make sure you have everything listed on the Installing  page available on your command line.  If you have not already, clone a copy of the repository and move\nto the  workflows/  directory:  $ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak\n$ cd dahak/workflows/  Now create a JSON file that defines a Snakemake configuration dictionary.\nThis file should:   Provides URLs at which each read filtering file can be accessed  Provides a set of quality trimming values to use (2 and 30)  Sets all read filtering parameters (for simplicity, we will set each\n    parameter to its default value)   (See the  Assembly Snakemake  page for details on\nthese options.)  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\"\n    },\n\n    \"workflows\" : {\n        \"assembly_workflow_all\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        }\n    },\n\n    \"biocontainers\" : {\n        \"metaspades\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/spades\",\n            \"version\" : \"3.11.1--py27_zlib1.2.8_0\"\n        },\n        \"megahit\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/megahit\",\n            \"version\" : \"1.1.2--py35_0\"\n        }\n    },\n\n    \"assembly\" : {\n        \"assembly_patterns\" : {\n            \"metaspades_pattern\" : \"{sample}.trim{qual}_metaspades.contigs.fa\",\n            \"megahit_pattern\" : \"{sample}.trim{qual}_megahit.contigs.fa\",\n            \"assembly_pattern\" : \"{sample}.trim{qual}_{assembler}.contigs.fa\",\n            \"quast_pattern\" : \"{sample}.trim{qual}_{assembler}_quast/report.html\",\n            \"multiqc_pattern\" : \"{sample}.trim{qual}_{assembler}_multiqc/report.html\",\n        }\n    },\n\n}  Put this file into  config/custom_assembly_workflow.json  (in the  workflows \ndirectory of the repository). We want to run the assembly workflow with\ntwo assemblers, so we call Snakemake and the  assembly_workflow_all  target.  $ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p -n \\\n        --configfile=config/custom_assembly_workflow.json \\\n        assembly_workflow_all  Once we have reviewed the output from Snakemake and are satisfied\nit is running the correct workflow and commands, we can actually\nrun the workflow by removing the  -n  flag:   $ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p \\\n        --configfile=config/custom_assembly_workflow.json \\\n        assembly_workflow_all",
            "title": "Assembly"
        },
        {
            "location": "/quickstart/#comparison",
            "text": "In this section we will run a comparison workflow to compute signatures for \nboth filtered reads and assemblies, and compare the computed signatures to \na reference database.  Before you begin, make sure you have everything listed on the Installing  page available on your command line.  Start by cloning the repository and moving to the  workflows/  directory:  $ git clone -b snakemake/comparison https://github.com/dahak-metagenomics/dahak\n$ cd dahak/workflows/  Now create a JSON file that defines a Snakemake configuration dictionary.\nThis file should:   Provides URLs at which each read filtering file can be accessed  Provides a set of quality trimming values to use (2 and 30)  Sets all read filtering parameters (for simplicity, we will set each\n    parameter to its default value)   (See the  Comparison Snakemake  page for details on\nthese options.)  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\"\n    },\n\n    \"workflows\" : {\n        \"comparison_workflow_reads_assembly\" : {\n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        }\n    },\n\n    \"biocontainers\" : {\n        \"sourmash_compare\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/sourmash\",\n            \"version\" : \"2.0.0a3--py36_0\"\n        }\n    },\n\n    \"comparison\" : {\n        \"compute_read_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\"    : \"_scaled10k.k21_31_51.sig\", \n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compute_assembly_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\" : \"_scaled10k.k21_31_51.sig\",\n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compare_read_assembly_signatures\" : {\n            \"samples\"   : [\"SRR606249_subset10\"],\n            \"assembler\" : [\"megahit\",\"metaspades\"],\n            \"kvalues\"   : [21, 31, 51],\n            \"csv_out\"   : \"SRR606249_trim2and30_ra_comparison.k{kvalue}.csv\"\n        }\n    }\n}  Note that there are two additional keys within the  comparison  configuration \nsub-dictionary,  compare_read_signatures  and  compare_assembly_signatures ,\nbut these sections are only used when comparing  just  reads (when passing the  comparison_workflow_reads  target to Snakemake) or when comparing  just \nassemblies (when passing the  comparison_workflow_assembly  target to Snakemake).  The JSON above can be put into the file  config/custom_comparison_workflow.json  \n(in the  workflows  directory of the repository), and the workflow can be run by\npassing the config file to Snakemake. It is important you run with the  -n  flag\nto do a dry-run first!  $ export SINGULARITY_BINDPATH=\"data:/data\"\n\n$ snakemake -p -n \\\n        --configfile=config/custom_comparison_workflow.json \\\n        comparison_workflow_reads_assembly\n\n$ snakemake -p \\\n        --configfile=config/custom_comparison_workflow.json \\\n        comparison_workflow_reads_assembly",
            "title": "Comparison"
        },
        {
            "location": "/readfilt_overview/",
            "text": "Read Filtering Workflow\n\u00b6\n\n\n\n\nThe read filtering step consists of processing raw reads from a \nsequencer, such as discarding reads with a high uncertainty value\nor trimming off adapters.\n\n\nTools like Fastqc and Trimmomatic will perform this filtering \nprocess for the sequencer's reads.\n\n\nMore information:\n\n\n\n\nRead Filtering Walkthrough\n\n\nRead Filtering Snakemake\n\n\nworkflows/readfilt/\n directory in the repository",
            "title": "Overview"
        },
        {
            "location": "/readfilt_overview/#read-filtering-workflow",
            "text": "The read filtering step consists of processing raw reads from a \nsequencer, such as discarding reads with a high uncertainty value\nor trimming off adapters.  Tools like Fastqc and Trimmomatic will perform this filtering \nprocess for the sequencer's reads.  More information:   Read Filtering Walkthrough  Read Filtering Snakemake  workflows/readfilt/  directory in the repository",
            "title": "Read Filtering Workflow"
        },
        {
            "location": "/readfilt_walkthru/",
            "text": "Read Filtering Walkthrough\n\u00b6\n\n\n\n\nImportant Note\n\n\nThe following walkthrough is \nindependent of the Snakemake workflows.\n\nAll commands given below are bash commands. This walkthrough also utilizes\nDocker, while the Snakemake workflows utilize SIngularity.\n\n\n\n\nThe following walkthrough covers the steps in the read filtering and quality\nassessment workflow.  This walkthrough covers the use of Docker to interactively\nrun the workflow on a fresh Ubuntu 16.04 (Xenial) image, and requires sudo\ncommands to be run.\n\n\nThis walkthrough presumes that the steps covered on the\n\nInstalling\n page have been run, and that a version of Python,\nConda, Snakemake, and Docker are available.  See the \nInstalling\n\npage for instructions on installing required software.\n\n\nWalkthrough Steps\n\u00b6\n\n\nStarting with a fresh image, go through the installation instructions\non the \nInstalling\n page.\n\n\nInstall the open science framework \ncommand-line client\n:\n\n\n$ pip install osfclient\n\n\n\n\n\nInstall \ndocker\n with the following shell commands:\n\n\n$ wget -qO- https://get.docker.com/ \n|\n sudo sh\n$ sudo usermod -aG docker ubuntu\n\n\n\n\n\nMake a directory called data and retrieve some data using the osfclient.\nSpecify the path to files.txt or move it to your working directory.  \n\n\nmkdir data\n\ncd\n data\n\n\nfor\n i in \n$(\ncat files.txt\n)\n\n\ndo\n \n    osf -p dm938 fetch osfstorage/data/\n${\ni\n}\n\n\ndone\n\n\n\n\n\n\nLink the data and run \nfastqc\n \n\n\nmkdir -p ~/data/qc/before_trim\n\ndocker run -v \n${\nPWD\n}\n:/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_1.fq.gz -o /data/qc/before_trim\n\ndocker run -v \n${\nPWD\n}\n:/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_2.fq.gz -o /data/qc/before_trim\n\n\n\n\n\nGrab the adapter sequences:\n\n\ncd\n ~/data\ncurl -O -L http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\n\n\n\n\n\nLink the data and run \ntrimmomatic\n\n\nfor\n filename in *_1*.fq.gz\n\ndo\n\n    \n# first, make the base by removing .fq.gz using the unix program basename\n\n    \nbase\n=\n$(\nbasename \n$filename\n .fq.gz\n)\n\n    \necho\n \n$base\n\n\n    \n# now, construct the base2 filename by replacing _1 with _2\n\n    \nbase2\n=\n${\nbase\n/_1/_2\n}\n\n    \necho\n \n$base2\n\n\n    docker run -v \n${\nPWD\n}\n:/data -it quay.io/biocontainers/trimmomatic:0.36--4 trimmomatic PE /data/\n${\nbase\n}\n.fq.gz \n\\\n\n                /data/\n${\nbase2\n}\n.fq.gz \n\\\n\n        /data/\n${\nbase\n}\n.trim.fq.gz /data/\n${\nbase\n}\n_se \n\\\n\n        /data/\n${\nbase2\n}\n.trim.fq.gz /data/\n${\nbase2\n}\n_se \n\\\n\n        ILLUMINACLIP:/data/TruSeq2-PE.fa:2:40:15 \n\\\n\n        LEADING:2 TRAILING:2 \n\\\n\n        SLIDINGWINDOW:4:2 \n\\\n\n        MINLEN:25\n\ndone\n\n\n\n\n\n\nNow run fastqc on the trimmed data:\n\n\nmkdir -p ~/data/qc/after_trim\n\n\ndocker run -v \n${\nPWD\n}\n:/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_1.trim.fq.gz -o /data/qc/after_trim\n\ndocker run -v \n${\nPWD\n}\n:/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_2.trim.fq.gz -o /data/qc/after_trim\n\n\n\n\n\nInterleave paired-end reads using \nkhmer\n. \nThe output file name includes 'trim2' indicating the reads were trimmed at a quality score of 2. \nIf other values were used change the output name accordingly.\n\n\ncd\n ~/data\n\nfor\n filename in *_1.trim.fq.gz\n\ndo\n\n    \n# first, make the base by removing _1.trim.fq.gz with basename\n\n    \nbase\n=\n$(\nbasename \n$filename\n _1.trim.fq.gz\n)\n\n    \necho\n \n$base\n\n\n    \n# construct the output filename\n\n    \noutput\n=\n${\nbase\n}\n.pe.trim2.fq.gz\n\n    docker run -v \n${\nPWD\n}\n:/data -it quay.io/biocontainers/khmer:2.1--py35_0 interleave-reads.py \n\\\n\n        /data/\n${\nbase\n}\n_1.trim.fq.gz /data/\n${\nbase\n}\n_2.trim.fq.gz --no-reformat -o /data/\n$output\n --gzip\n\n\ndone",
            "title": "Walkthrough"
        },
        {
            "location": "/readfilt_walkthru/#read-filtering-walkthrough",
            "text": "Important Note  The following walkthrough is  independent of the Snakemake workflows. \nAll commands given below are bash commands. This walkthrough also utilizes\nDocker, while the Snakemake workflows utilize SIngularity.   The following walkthrough covers the steps in the read filtering and quality\nassessment workflow.  This walkthrough covers the use of Docker to interactively\nrun the workflow on a fresh Ubuntu 16.04 (Xenial) image, and requires sudo\ncommands to be run.  This walkthrough presumes that the steps covered on the Installing  page have been run, and that a version of Python,\nConda, Snakemake, and Docker are available.  See the  Installing \npage for instructions on installing required software.",
            "title": "Read Filtering Walkthrough"
        },
        {
            "location": "/readfilt_walkthru/#walkthrough-steps",
            "text": "Starting with a fresh image, go through the installation instructions\non the  Installing  page.  Install the open science framework  command-line client :  $ pip install osfclient  Install  docker  with the following shell commands:  $ wget -qO- https://get.docker.com/  |  sudo sh\n$ sudo usermod -aG docker ubuntu  Make a directory called data and retrieve some data using the osfclient.\nSpecify the path to files.txt or move it to your working directory.    mkdir data cd  data for  i in  $( cat files.txt )  do  \n    osf -p dm938 fetch osfstorage/data/ ${ i }  done   Link the data and run  fastqc    mkdir -p ~/data/qc/before_trim\n\ndocker run -v  ${ PWD } :/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_1.fq.gz -o /data/qc/before_trim\n\ndocker run -v  ${ PWD } :/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_2.fq.gz -o /data/qc/before_trim  Grab the adapter sequences:  cd  ~/data\ncurl -O -L http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa  Link the data and run  trimmomatic  for  filename in *_1*.fq.gz do \n     # first, make the base by removing .fq.gz using the unix program basename \n     base = $( basename  $filename  .fq.gz ) \n     echo   $base \n\n     # now, construct the base2 filename by replacing _1 with _2 \n     base2 = ${ base /_1/_2 } \n     echo   $base2 \n\n    docker run -v  ${ PWD } :/data -it quay.io/biocontainers/trimmomatic:0.36--4 trimmomatic PE /data/ ${ base } .fq.gz  \\ \n                /data/ ${ base2 } .fq.gz  \\ \n        /data/ ${ base } .trim.fq.gz /data/ ${ base } _se  \\ \n        /data/ ${ base2 } .trim.fq.gz /data/ ${ base2 } _se  \\ \n        ILLUMINACLIP:/data/TruSeq2-PE.fa:2:40:15  \\ \n        LEADING:2 TRAILING:2  \\ \n        SLIDINGWINDOW:4:2  \\ \n        MINLEN:25 done   Now run fastqc on the trimmed data:  mkdir -p ~/data/qc/after_trim\n\n\ndocker run -v  ${ PWD } :/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_1.trim.fq.gz -o /data/qc/after_trim\n\ndocker run -v  ${ PWD } :/data -it biocontainers/fastqc fastqc /data/SRR606249_subset10_2.trim.fq.gz -o /data/qc/after_trim  Interleave paired-end reads using  khmer . \nThe output file name includes 'trim2' indicating the reads were trimmed at a quality score of 2. \nIf other values were used change the output name accordingly.  cd  ~/data for  filename in *_1.trim.fq.gz do \n     # first, make the base by removing _1.trim.fq.gz with basename \n     base = $( basename  $filename  _1.trim.fq.gz ) \n     echo   $base \n\n     # construct the output filename \n     output = ${ base } .pe.trim2.fq.gz\n\n    docker run -v  ${ PWD } :/data -it quay.io/biocontainers/khmer:2.1--py35_0 interleave-reads.py  \\ \n        /data/ ${ base } _1.trim.fq.gz /data/ ${ base } _2.trim.fq.gz --no-reformat -o /data/ $output  --gzip done",
            "title": "Walkthrough Steps"
        },
        {
            "location": "/readfilt_snakemake/",
            "text": "Read Filtering Workflow: Snakemake\n\u00b6\n\n\nAs mentioned on the \nRunning Workflows\n page,\nthe Snakemake workflows define \nbuild rules\n that trigger all of\nthe rules composing a given workflow.\n\n\nAs a reminder, the \nRunning Workflows\n page \nshowed how to call Snakemake and ask for a particular target:\n\n\n$ snakemake [FLAGS] <target>\n\n\n\n\n\nYou can replace \n<target>\n with any of the build rules below.\n\n\nBuild Rules\n\u00b6\n\n\nThe build rules are the rules that the end user should be calling.\nA list of available build rules in the read filtering workflow is\ngiven below.\n\n\nread_filtering_pretrim_workflow\n\n    Build rule: trigger the read filtering workflow\n\nread_filtering_posttrim_workflow\n\n    Build rule: trigger the read filtering workflow\n\n\n\n\n\nPass the name of the build rule directly to Snakemake\non the command line:\n\n\n$ snakemake [FLAGS] read_filtering_pretrim_workflow read_filtering_posttrim_workflow\n\n\n\n\n\nSee the \nQuick Start\n for details on the process of running this workflow.\n\n\nSnakemake Configuration Dictionary\n\u00b6\n\n\nThere are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the \n\nWorkflow Configuration\n page).\n\n\nData Files Configuration\n\u00b6\n\n\nSet the \nfiles\n key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}\n\n\n\n\n\nPut these in a JSON file (e.g., \nconfig/custom_datafiles.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Configuration\n\u00b6\n\n\nSet the \nworkflows\n key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.\n\n\nHere is the structure of the configuration dictionary\nfor read filtering workflows (pre-trimming and post-trimming\nquality assessment):\n\n\n{\n    \"workflows\" : {\n\n        \"read_filtering_pretrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"]\n        },\n\n        \"read_filtering_posttrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"   : [\"2\",\"30\"]\n        }\n    }\n}\n\n\n\n\n\nThe \nsample\n list specifies the prefixes of the sample reads to \nrun the read filtering workflow on. The \nqual\n list specifies the \nvalues to use for quality trimming (for the post-trimming workflow).\n\n\nPut these in a JSON file (e.g., \nconfig/custom_workflowconfig.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Parameters\n\u00b6\n\n\nSet the \nread_filtering\n key of the Snakemake configuration dictionary\nto a dictionary containing various child dictionaries and key-value pairs:\n\n\n{\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}.trim{qual}.fq.gz\",\n        },\n\n        \"direction_labels\" : {\n            \"forward\" : \"1\",\n            \"reverse\" : \"2\"\n        },\n\n        \"quality_assessment\" : {\n            \"fastqc_suffix\": \"fastqc\",\n        },\n\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        }\n    }\n}\n\n\n\n\n\nThe \npre_trimming_pattern\n must match the filename pattern of the reads \nthat are provided in the \nfiles\n key. The \n{sample}\n and \n{direction}\n\nnotation is for Snakemake to match wildcards. For example, the pattern\n\n\n{sample}_{direction}_reads.fq.gz\n\n\n\n\n\nmatch the filename\n\n\nSRR606249_subset10_1_reads.fq.gz\n\n\n\n\n\nsuch that the wildcard values are \nsample=SRR606249_subset10\n\nand \ndirection=1\n.\n\n\nThe \ndirection_labels\n key is used to indicate the suffix used for\nforward and reverse reads; this is typically \n1\n and \n2\n but can be\ncustomized by the user if needed.\n\n\nTo use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g.,\n\nconfig/custom_workflowparams.json\n in the \nworkflows\n directory) and pass the\nname of the config file to Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Snakemake"
        },
        {
            "location": "/readfilt_snakemake/#read-filtering-workflow-snakemake",
            "text": "As mentioned on the  Running Workflows  page,\nthe Snakemake workflows define  build rules  that trigger all of\nthe rules composing a given workflow.  As a reminder, the  Running Workflows  page \nshowed how to call Snakemake and ask for a particular target:  $ snakemake [FLAGS] <target>  You can replace  <target>  with any of the build rules below.",
            "title": "Read Filtering Workflow: Snakemake"
        },
        {
            "location": "/readfilt_snakemake/#build-rules",
            "text": "The build rules are the rules that the end user should be calling.\nA list of available build rules in the read filtering workflow is\ngiven below.  read_filtering_pretrim_workflow\n\n    Build rule: trigger the read filtering workflow\n\nread_filtering_posttrim_workflow\n\n    Build rule: trigger the read filtering workflow  Pass the name of the build rule directly to Snakemake\non the command line:  $ snakemake [FLAGS] read_filtering_pretrim_workflow read_filtering_posttrim_workflow  See the  Quick Start  for details on the process of running this workflow.",
            "title": "Build Rules"
        },
        {
            "location": "/readfilt_snakemake/#snakemake-configuration-dictionary",
            "text": "There are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the  Workflow Configuration  page).",
            "title": "Snakemake Configuration Dictionary"
        },
        {
            "location": "/readfilt_snakemake/#data-files-configuration",
            "text": "Set the  files  key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}  Put these in a JSON file (e.g.,  config/custom_datafiles.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>",
            "title": "Data Files Configuration"
        },
        {
            "location": "/readfilt_snakemake/#workflow-configuration",
            "text": "Set the  workflows  key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.  Here is the structure of the configuration dictionary\nfor read filtering workflows (pre-trimming and post-trimming\nquality assessment):  {\n    \"workflows\" : {\n\n        \"read_filtering_pretrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"]\n        },\n\n        \"read_filtering_posttrim_workflow\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"   : [\"2\",\"30\"]\n        }\n    }\n}  The  sample  list specifies the prefixes of the sample reads to \nrun the read filtering workflow on. The  qual  list specifies the \nvalues to use for quality trimming (for the post-trimming workflow).  Put these in a JSON file (e.g.,  config/custom_workflowconfig.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>",
            "title": "Workflow Configuration"
        },
        {
            "location": "/readfilt_snakemake/#workflow-parameters",
            "text": "Set the  read_filtering  key of the Snakemake configuration dictionary\nto a dictionary containing various child dictionaries and key-value pairs:  {\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}.trim{qual}.fq.gz\",\n        },\n\n        \"direction_labels\" : {\n            \"forward\" : \"1\",\n            \"reverse\" : \"2\"\n        },\n\n        \"quality_assessment\" : {\n            \"fastqc_suffix\": \"fastqc\",\n        },\n\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        }\n    }\n}  The  pre_trimming_pattern  must match the filename pattern of the reads \nthat are provided in the  files  key. The  {sample}  and  {direction} \nnotation is for Snakemake to match wildcards. For example, the pattern  {sample}_{direction}_reads.fq.gz  match the filename  SRR606249_subset10_1_reads.fq.gz  such that the wildcard values are  sample=SRR606249_subset10 \nand  direction=1 .  The  direction_labels  key is used to indicate the suffix used for\nforward and reverse reads; this is typically  1  and  2  but can be\ncustomized by the user if needed.  To use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g., config/custom_workflowparams.json  in the  workflows  directory) and pass the\nname of the config file to Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Workflow Parameters"
        },
        {
            "location": "/assembly_overview/",
            "text": "Assembly Workflow\n\u00b6\n\n\n(ASSEMBLY DIAGRAM)\n\n\nThe assembly workflow uses assembly-based approaches to give higher-confidence\ngene identity assignment than raw read assignment alone.\n\n\nThe assembly step determines the proper\norder of the reads, and assembles the genome. The assembly tool\nmay use short reads (~350 or fewer reads), or it may use \nlong reads (>1000 reads). \n\n\nReads are assembled in order into contigs (chunks of contiguous\nreads). The contigs are themselves assembled into scaffolds \nthat consist of several contigs.\n\n\nThe SPAdes tool can handle short or long reads, while the megahit \ntool works better for short reads. \n\n\nMetaquast gives assembly statistics that can help evaluate the assembly\n(how long, number of fragments, number of contigs, number of scaffolds, \netc.).\n\n\nTypically 30-40% of the reads can be fingerprinted by the assembler.\n\n\nMore information:\n\n\n\n\nAssembly Walkthrough\n\n\nAssembly Snakemake Rules\n\n\nworkflows/assembly/\n directory in the repository",
            "title": "Overview"
        },
        {
            "location": "/assembly_overview/#assembly-workflow",
            "text": "(ASSEMBLY DIAGRAM)  The assembly workflow uses assembly-based approaches to give higher-confidence\ngene identity assignment than raw read assignment alone.  The assembly step determines the proper\norder of the reads, and assembles the genome. The assembly tool\nmay use short reads (~350 or fewer reads), or it may use \nlong reads (>1000 reads).   Reads are assembled in order into contigs (chunks of contiguous\nreads). The contigs are themselves assembled into scaffolds \nthat consist of several contigs.  The SPAdes tool can handle short or long reads, while the megahit \ntool works better for short reads.   Metaquast gives assembly statistics that can help evaluate the assembly\n(how long, number of fragments, number of contigs, number of scaffolds, \netc.).  Typically 30-40% of the reads can be fingerprinted by the assembler.  More information:   Assembly Walkthrough  Assembly Snakemake Rules  workflows/assembly/  directory in the repository",
            "title": "Assembly Workflow"
        },
        {
            "location": "/assembly_walkthru/",
            "text": "Assembly with \nSPAdes\n and \nMEGAHIT\n\u00b6\n\n\nRequirements (from unbuntu 16.04 using filtered reads)\n\n\nDisk space(60 gb)\nRAM(32 gb)\nUpdated packages(see \nread filtering\n)\nDocker(see \nread filtering\n for installation instructions) \n\n\nLink the data and run MEGAHIT\n\u00b6\n\n\nfor filename in *.trim30.fq.gz\ndo \n\n    base=$(basename $filename .trim30.fq.gz)\n    echo $base \n\n    docker run -v ${PWD}:/data -it quay.io/biocontainers/megahit:1.1.1--py36_0 \\ \n    megahit --12 /data/${i} --out-prefix=${base} -o /data/${base}.megahit_output\ndone\n\n\n\n\n\nNow run Quast to generate some assembly statistics\n\u00b6\n\n\nfor file in *.megahit_output/*contigs.fa\ndo\n\n    base=$(basename $file .contigs.fa)\n    echo $base \n\n    docker run -v /${PWD}:/data -it quay.io/biocontainers/quast:4.5--boost1.61_1 \\\n    quast.py /data/${file} -o /data/${base}.megahit_quast_report\ndone\n\n\n\n\n\nLink the data and run SPAdes\n\u00b6\n\n\nfor filename in *pe.trim30.fq.gz\ndo\n    base=$(basename $filename .trim30.fq.gz)\n    echo $base\n\n    docker run -v ${PWD}:/data -it quay.io/biocontainers/spades:3.10.1--py27_0 \\\n    metaspades.py --12 /data/${filename} \\\n    -o /data/${base}.spades_output\ndone \n\n\n\n\n\nNow run Quast to generate some assembly statistics\n\u00b6\n\n\nfor file in *.spades_output\ndo \n\n    base=$(basename $file .spades_output)\n    echo ${base}\n\n    docker run -v ${PWD}:/data -it quay.io/biocontainers/quast:4.5--boost1.61_1 \\\n    quast.py /data/${base}.spades_output/contigs.fasta \\\n    -o data/${base}.spades_quast_report\ndone",
            "title": "Walkthrough"
        },
        {
            "location": "/assembly_walkthru/#assembly-with-spades-and-megahit",
            "text": "Requirements (from unbuntu 16.04 using filtered reads)  Disk space(60 gb)\nRAM(32 gb)\nUpdated packages(see  read filtering )\nDocker(see  read filtering  for installation instructions)",
            "title": "Assembly with SPAdes and MEGAHIT"
        },
        {
            "location": "/assembly_walkthru/#link-the-data-and-run-megahit",
            "text": "for filename in *.trim30.fq.gz\ndo \n\n    base=$(basename $filename .trim30.fq.gz)\n    echo $base \n\n    docker run -v ${PWD}:/data -it quay.io/biocontainers/megahit:1.1.1--py36_0 \\ \n    megahit --12 /data/${i} --out-prefix=${base} -o /data/${base}.megahit_output\ndone",
            "title": "Link the data and run MEGAHIT"
        },
        {
            "location": "/assembly_walkthru/#now-run-quast-to-generate-some-assembly-statistics",
            "text": "for file in *.megahit_output/*contigs.fa\ndo\n\n    base=$(basename $file .contigs.fa)\n    echo $base \n\n    docker run -v /${PWD}:/data -it quay.io/biocontainers/quast:4.5--boost1.61_1 \\\n    quast.py /data/${file} -o /data/${base}.megahit_quast_report\ndone",
            "title": "Now run Quast to generate some assembly statistics"
        },
        {
            "location": "/assembly_walkthru/#link-the-data-and-run-spades",
            "text": "for filename in *pe.trim30.fq.gz\ndo\n    base=$(basename $filename .trim30.fq.gz)\n    echo $base\n\n    docker run -v ${PWD}:/data -it quay.io/biocontainers/spades:3.10.1--py27_0 \\\n    metaspades.py --12 /data/${filename} \\\n    -o /data/${base}.spades_output\ndone",
            "title": "Link the data and run SPAdes"
        },
        {
            "location": "/assembly_walkthru/#now-run-quast-to-generate-some-assembly-statistics_1",
            "text": "for file in *.spades_output\ndo \n\n    base=$(basename $file .spades_output)\n    echo ${base}\n\n    docker run -v ${PWD}:/data -it quay.io/biocontainers/quast:4.5--boost1.61_1 \\\n    quast.py /data/${base}.spades_output/contigs.fasta \\\n    -o data/${base}.spades_quast_report\ndone",
            "title": "Now run Quast to generate some assembly statistics"
        },
        {
            "location": "/assembly_snakemake/",
            "text": "Assembly: Snakemake\n\u00b6\n\n\nAs mentioned on the \nRunning Workflows\n page,\nthe Snakemake workflows define \nbuild rules\n that trigger all of\nthe rules composing a given workflow.\n\n\nAs a reminder, the \nRunning Workflows\n page \nshowed how to call Snakemake and ask for a particular target:\n\n\n$ snakemake [FLAGS] <target>\n\n\n\n\n\nYou can replace \n<target>\n with any of the build rules below.\n\n\nBuild Rules\n\u00b6\n\n\nThe build rules are the rules that the end user should be calling.\nA list of available build rules in the assembly workflow is\ngiven below.\n\n\nassembly_workflow_metaspades\n\n    Build rule: trigger the metaspades assembly step.\n\nassembly_workflow_megahit\n\n    Build rule: trigger the megahit assembly step.\n\nassembly_workflow_all\n\n    Build rule: trigger the assembly step with all assemblers.\n\n\n\n\n\nPass the name of the build rule directly to Snakemake\non the command line:\n\n\n$ snakemake [FLAGS] assembly_workflow_all\n\n\n\n\n\nSee below for how to configure these workflows.  See the \nQuick\nStart\n for details on the process of running this workflow.\n\n\nSnakemake Configuration Dictionary\n\u00b6\n\n\nThere are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the \n\nWorkflow Configuration\n page).\n\n\nData Files Configuration\n\u00b6\n\n\nSet the \nfiles\n key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}\n\n\n\n\n\nPut these in a JSON file (e.g., \nconfig/custom_datafiles.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Configuration\n\u00b6\n\n\nSet the \nworkflows\n key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.\n\n\nHere is the structure of the configuration dictionary\nfor assembly workflows:\n\n\n{\n    \"workflows\" : {\n        \"assembly_workflow_metaspades\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        },\n\n        \"assembly_workflow_megahit\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        },\n\n        \"assembly_workflow_all\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        },\n\n    }\n}\n\n\n\n\n\nThe \nsample\n and \nqual\n keys are used by Snakemake to generate a list of\ninput files required for the rule, and to generate the name of the output\nfile from the workflow. These should be lists of strings. \nsample\n should\nmatch the read files listed in the \nfiles\n section, while \nqual\n should be \nthe quality trimming value to use.\n\n\nPut these in a JSON file (e.g., \nconfig/custom_workflowconfig.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Parameters\n\u00b6\n\n\nSet the \nassembly\n key of the Snakemake configuration dictionary\nas shown below:\n\n\n{\n    \"assembly\" : {\n        \"assembly_patterns\" : {\n            \"metaspades_pattern\" : \"{sample}.trim{qual}_metaspades.contigs.fa\",\n            \"megahit_pattern\" : \"{sample}.trim{qual}_megahit.contigs.fa\",\n            \"assembly_pattern\" : \"{sample}.trim{qual}_{assembler}.contigs.fa\",\n            \"quast_pattern\" : \"{sample}.trim{qual}_{assembler}_quast/report.html\",\n            \"multiqc_pattern\" : \"{sample}.trim{qual}_{assembler}_multiqc/report.html\",\n        }\n    }\n}\n\n\n\n\n\nTo use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g.,\n\nconfig/custom_workflowparams.json\n in the \nworkflows\n directory) and pass the\nname of the config file to Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Snakemake"
        },
        {
            "location": "/assembly_snakemake/#assembly-snakemake",
            "text": "As mentioned on the  Running Workflows  page,\nthe Snakemake workflows define  build rules  that trigger all of\nthe rules composing a given workflow.  As a reminder, the  Running Workflows  page \nshowed how to call Snakemake and ask for a particular target:  $ snakemake [FLAGS] <target>  You can replace  <target>  with any of the build rules below.",
            "title": "Assembly: Snakemake"
        },
        {
            "location": "/assembly_snakemake/#build-rules",
            "text": "The build rules are the rules that the end user should be calling.\nA list of available build rules in the assembly workflow is\ngiven below.  assembly_workflow_metaspades\n\n    Build rule: trigger the metaspades assembly step.\n\nassembly_workflow_megahit\n\n    Build rule: trigger the megahit assembly step.\n\nassembly_workflow_all\n\n    Build rule: trigger the assembly step with all assemblers.  Pass the name of the build rule directly to Snakemake\non the command line:  $ snakemake [FLAGS] assembly_workflow_all  See below for how to configure these workflows.  See the  Quick\nStart  for details on the process of running this workflow.",
            "title": "Build Rules"
        },
        {
            "location": "/assembly_snakemake/#snakemake-configuration-dictionary",
            "text": "There are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the  Workflow Configuration  page).",
            "title": "Snakemake Configuration Dictionary"
        },
        {
            "location": "/assembly_snakemake/#data-files-configuration",
            "text": "Set the  files  key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}  Put these in a JSON file (e.g.,  config/custom_datafiles.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>",
            "title": "Data Files Configuration"
        },
        {
            "location": "/assembly_snakemake/#workflow-configuration",
            "text": "Set the  workflows  key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.  Here is the structure of the configuration dictionary\nfor assembly workflows:  {\n    \"workflows\" : {\n        \"assembly_workflow_metaspades\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        },\n\n        \"assembly_workflow_megahit\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        },\n\n        \"assembly_workflow_all\" : {\n            \"sample\"    : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\"      : [\"2\",\"30\"],\n        },\n\n    }\n}  The  sample  and  qual  keys are used by Snakemake to generate a list of\ninput files required for the rule, and to generate the name of the output\nfile from the workflow. These should be lists of strings.  sample  should\nmatch the read files listed in the  files  section, while  qual  should be \nthe quality trimming value to use.  Put these in a JSON file (e.g.,  config/custom_workflowconfig.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>",
            "title": "Workflow Configuration"
        },
        {
            "location": "/assembly_snakemake/#workflow-parameters",
            "text": "Set the  assembly  key of the Snakemake configuration dictionary\nas shown below:  {\n    \"assembly\" : {\n        \"assembly_patterns\" : {\n            \"metaspades_pattern\" : \"{sample}.trim{qual}_metaspades.contigs.fa\",\n            \"megahit_pattern\" : \"{sample}.trim{qual}_megahit.contigs.fa\",\n            \"assembly_pattern\" : \"{sample}.trim{qual}_{assembler}.contigs.fa\",\n            \"quast_pattern\" : \"{sample}.trim{qual}_{assembler}_quast/report.html\",\n            \"multiqc_pattern\" : \"{sample}.trim{qual}_{assembler}_multiqc/report.html\",\n        }\n    }\n}  To use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g., config/custom_workflowparams.json  in the  workflows  directory) and pass the\nname of the config file to Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Workflow Parameters"
        },
        {
            "location": "/comparison_overview/",
            "text": "Sample Comparison Workflow\n\u00b6\n\n\n \n\n\nThe sample comparison workflow helps perform rapid k-mer-based\nordination analyses of many samples to provide sample\ngroupings and identify potential outliers.\n\n\nOperating at the level of k-mers (representations of the reads),\nthe comparison step is taking the reads that were not fingerprinted\nby the assembler and seeing if they match genomes of other organisms. \n\n\nThe tool used for comparison is sourmash.\n\n\nMore information:\n\n\n\n\nComparison Walkthrough\n\n\nComparison Snakemake Rules\n\n\nworkflows/comparison/\n directory in the repository",
            "title": "Overview"
        },
        {
            "location": "/comparison_overview/#sample-comparison-workflow",
            "text": "The sample comparison workflow helps perform rapid k-mer-based\nordination analyses of many samples to provide sample\ngroupings and identify potential outliers.  Operating at the level of k-mers (representations of the reads),\nthe comparison step is taking the reads that were not fingerprinted\nby the assembler and seeing if they match genomes of other organisms.   The tool used for comparison is sourmash.  More information:   Comparison Walkthrough  Comparison Snakemake Rules  workflows/comparison/  directory in the repository",
            "title": "Sample Comparison Workflow"
        },
        {
            "location": "/comparison_walkthru/",
            "text": "Metagenome comparison with \nsourmash\n\u00b6\n\n\nSourmash is a tool for calculating and comparing MinHash signatures. Sourmash compare \ncalculates the jaccard similarity of MinHash signatures.    \n\n\nIf you don't already have them, retrieve the assembled contigs:\n\n\nfor i in $(cat assembly_names.txt) \ndo \n\n    osf -u philliptbrooks@gmail.com -p dm938 fetch ${i} ${i}\n    echo ${i}\ndone  \n\n\n\n\n\nCalculate signatures\n\u00b6\n\n\nCalculate sourmash signatures for reads:\n\n\nfor filename in *_1.trim2.fq.gz\ndo\n    #Remove _1.trim2.fq from file name to create base\n    base=$(basename $filename _1.trim2.fq.gz)\n    echo $base\n\n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n        sourmash compute \\\n        --merge /data/${base}.trim2.fq.gz \\\n        --scaled 10000 \\\n        -k 21,31,51 \\\n        /data/${base}_1.trim2.fq.gz \\ \n        /data/${base}_2.trim2.fq.gz \\\n        -o /data/${base}.pe.trim2.fq.gz.k21_31_51.sig\ndone\n\nfor filename in *_1.trim30.fq.gz\ndo\n    #Remove _1.trim30.fq from file name to create base\n    base=$(basename $filename _1.trim30.fq.gz)\n    echo $base\n\n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n        sourmash compute \\\n        --merge /data/${base}.trim30.fq.gz \\\n        --scaled 10000 \\\n        -k 21,31,51 \\\n        /data/${base}_1.trim30.fq.gz \\\n        /data/${base}_2.trim30.fq.gz \\\n        -o /data/${base}pe.trim30.fq.gz.k21_31_51.sig\ndone\n\n\n\n\n\nCalculate sourmash signatures for assemblies:\n\n\nfor i in osfstorage/assembly/SRR606249{\"_1\",\"_subset10_1\",\"_subset25_1\",\"_subset50_1\"}.trim{\"2\",\"30\"}.fq.gz_megahit_output/final.contigs.fa\ndo     \n    base=`echo ${i} | awk -F/ '{print $3}'`\n    echo ${base}\n\n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n            sourmash compute \\ \n            -k 21,31,51 \\ \n            --scaled 10000 \\\n            /data/${i} \\\n            -o /data/${base}.k21_31_51.sig\ndone \n\nfor i in osfstorage/assembly/SRR606249{\"_1\",\"_subset10_1\",\"_subset25_1\",\"_subset50_1\"}.trim{\"2\",\"30\"}.fq.gz_spades_output/contigs.fasta\ndo     \n        base=`echo ${i} | awk -F/ '{print $3}'`\n        echo ${base}\n\n        docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n            sourmash compute \\\n            -k 21,31,51 \\\n            --scaled 10000 \\\n            /data/${i} \\\n            -o /data/${base}.k21_31_51.sig\ndone\n\n\n\n\n\nCompare read signatures\n\u00b6\n\n\nRun sourmash compare on all the read signatures:\n\n\nfor i in 21 31 51 \ndo \n\n        docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n            sourmash compare \\\n            /data/SRR606249.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249.pe.trim30.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset10.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset10.pe.trim30.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset25.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset25.pe.trim30.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset50.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset50.pe.trim30.fq.gz.k21_31_51.sig \\\n            -k ${i} \\\n            --csv /data/SRR606249.pe.trim2and30_comparison.k${i}.csv\ndone\n\n\n\n\n\nCompare assembly signatures\n\u00b6\n\n\nfor i in 21 31 51 \ndo \n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n        sourmash compare \\\n        /data/SRR606249_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset10_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset10_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset10_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset25_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset25_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset25_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n        -k ${i} \\\n        --csv /data/SRR606249.pe.trim2and30_megahitandspades_comparison.k${i}.csv\ndone\n\n\n\n\n\nCompare read signatures to assembly signatures\n\u00b6\n\n\nfor i in 21 31 51\ndo\n        docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n                sourmahs compare \\\n                /data/SRR606249_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n                /data/SRR606249_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n                /data/SRR606249.pe.trim2.fq.gz.k21_31_51.sig \\\n                /data/SRR606249.pe.trim30.fq.gz.k21_31_51.sig \\\n                /data/SRR606249_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n                /data/SRR606249_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n                -k ${i} \\\n                --csv /data/SRR606249.pe.trim2and30_readstoassemblies_comparison.k${i}.csv\ndone",
            "title": "Walkthrough"
        },
        {
            "location": "/comparison_walkthru/#metagenome-comparison-with-sourmash",
            "text": "Sourmash is a tool for calculating and comparing MinHash signatures. Sourmash compare \ncalculates the jaccard similarity of MinHash signatures.      If you don't already have them, retrieve the assembled contigs:  for i in $(cat assembly_names.txt) \ndo \n\n    osf -u philliptbrooks@gmail.com -p dm938 fetch ${i} ${i}\n    echo ${i}\ndone",
            "title": "Metagenome comparison with sourmash"
        },
        {
            "location": "/comparison_walkthru/#calculate-signatures",
            "text": "Calculate sourmash signatures for reads:  for filename in *_1.trim2.fq.gz\ndo\n    #Remove _1.trim2.fq from file name to create base\n    base=$(basename $filename _1.trim2.fq.gz)\n    echo $base\n\n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n        sourmash compute \\\n        --merge /data/${base}.trim2.fq.gz \\\n        --scaled 10000 \\\n        -k 21,31,51 \\\n        /data/${base}_1.trim2.fq.gz \\ \n        /data/${base}_2.trim2.fq.gz \\\n        -o /data/${base}.pe.trim2.fq.gz.k21_31_51.sig\ndone\n\nfor filename in *_1.trim30.fq.gz\ndo\n    #Remove _1.trim30.fq from file name to create base\n    base=$(basename $filename _1.trim30.fq.gz)\n    echo $base\n\n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n        sourmash compute \\\n        --merge /data/${base}.trim30.fq.gz \\\n        --scaled 10000 \\\n        -k 21,31,51 \\\n        /data/${base}_1.trim30.fq.gz \\\n        /data/${base}_2.trim30.fq.gz \\\n        -o /data/${base}pe.trim30.fq.gz.k21_31_51.sig\ndone  Calculate sourmash signatures for assemblies:  for i in osfstorage/assembly/SRR606249{\"_1\",\"_subset10_1\",\"_subset25_1\",\"_subset50_1\"}.trim{\"2\",\"30\"}.fq.gz_megahit_output/final.contigs.fa\ndo     \n    base=`echo ${i} | awk -F/ '{print $3}'`\n    echo ${base}\n\n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n            sourmash compute \\ \n            -k 21,31,51 \\ \n            --scaled 10000 \\\n            /data/${i} \\\n            -o /data/${base}.k21_31_51.sig\ndone \n\nfor i in osfstorage/assembly/SRR606249{\"_1\",\"_subset10_1\",\"_subset25_1\",\"_subset50_1\"}.trim{\"2\",\"30\"}.fq.gz_spades_output/contigs.fasta\ndo     \n        base=`echo ${i} | awk -F/ '{print $3}'`\n        echo ${base}\n\n        docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n            sourmash compute \\\n            -k 21,31,51 \\\n            --scaled 10000 \\\n            /data/${i} \\\n            -o /data/${base}.k21_31_51.sig\ndone",
            "title": "Calculate signatures"
        },
        {
            "location": "/comparison_walkthru/#compare-read-signatures",
            "text": "Run sourmash compare on all the read signatures:  for i in 21 31 51 \ndo \n\n        docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n            sourmash compare \\\n            /data/SRR606249.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249.pe.trim30.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset10.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset10.pe.trim30.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset25.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset25.pe.trim30.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset50.pe.trim2.fq.gz.k21_31_51.sig \\\n            /data/SRR606249_subset50.pe.trim30.fq.gz.k21_31_51.sig \\\n            -k ${i} \\\n            --csv /data/SRR606249.pe.trim2and30_comparison.k${i}.csv\ndone",
            "title": "Compare read signatures"
        },
        {
            "location": "/comparison_walkthru/#compare-assembly-signatures",
            "text": "for i in 21 31 51 \ndo \n    docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n        sourmash compare \\\n        /data/SRR606249_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset10_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset10_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset10_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset25_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset25_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset25_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n        /data/SRR606249_subset50_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n        -k ${i} \\\n        --csv /data/SRR606249.pe.trim2and30_megahitandspades_comparison.k${i}.csv\ndone",
            "title": "Compare assembly signatures"
        },
        {
            "location": "/comparison_walkthru/#compare-read-signatures-to-assembly-signatures",
            "text": "for i in 21 31 51\ndo\n        docker run -v ${PWD}:/data quay.io/biocontainers/sourmash:2.0.0a1--py35_2 \\\n                sourmahs compare \\\n                /data/SRR606249_1.trim2.fq.gz_megahit_output.k21_31_51.sig \\\n                /data/SRR606249_1.trim2.fq.gz_spades_output.k21_31_51.sig \\\n                /data/SRR606249.pe.trim2.fq.gz.k21_31_51.sig \\\n                /data/SRR606249.pe.trim30.fq.gz.k21_31_51.sig \\\n                /data/SRR606249_1.trim30.fq.gz_megahit_output.k21_31_51.sig \\\n                /data/SRR606249_1.trim30.fq.gz_spades_output.k21_31_51.sig \\\n                -k ${i} \\\n                --csv /data/SRR606249.pe.trim2and30_readstoassemblies_comparison.k${i}.csv\ndone",
            "title": "Compare read signatures to assembly signatures"
        },
        {
            "location": "/comparison_snakemake/",
            "text": "Comparison: Snakemake\n\u00b6\n\n\nAs mentioned on the \nRunning Workflows\n page,\nthe Snakemake workflows define \nbuild rules\n that trigger all of\nthe rules composing a given workflow.\n\n\nAs a reminder, the \nRunning Workflows\n page \nshowed how to call Snakemake and ask for a particular target:\n\n\n$ snakemake [FLAGS] <target>\n\n\n\n\n\nBuild Rules\n\u00b6\n\n\nThe build rules are the rules that the end user should be calling.\nA list of available build rules in the taxonomic classification \nworkflow is given below.\n\n\nList of available build rules in the comparison workflow:\n\n\ncomparison_workflow_reads\n\n    Build rule: run sourmash compare on all reads\n\ncomparison_workflow_assembly\n\n    Build rule: run sourmash compare on all assemblies\n\ncomparison_workflow_reads_assembly\n\n    Build rule: run sourmash compare on all reads and assemblies together\n\n\n\n\n\nPass the name of the build rule directly to Snakemake\non the command line:\n\n\n$ snakemake [FLAGS] comparison_workflow_reads \\\n        comparison_workflow_assembly \\\n        comparison_workflow_reads_assembly\n\n\n\n\n\nSee below for how to configure these workflows.  See the \nQuick\nStart\n for details on the process of running this workflow.\n\n\nSnakemake Configuration Dictionary\n\u00b6\n\n\nThere are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the \n\nWorkflow Configuration\n page).\n\n\nData Files Configuration\n\u00b6\n\n\nSet the \nfiles\n key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}\n\n\n\n\n\nPut these in a JSON file (e.g., \nconfig/custom_datafiles.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Configuration\n\u00b6\n\n\nSet the \nworkflows\n key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.\n\n\nHere is the structure of the configuration dictionary\nfor taxonomic classification workflows:\n\n\n{\n    \"workflows\" : {\n        \"comparison_workflow_reads\": {\n            #\n            # these parameters determine which reads\n            # the comparison workflow will be run on\n            # \n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        },\n\n        \"comparison_workflow_assembly\" : {\n            #\n            # these parameters determine which assembled reads\n            # the comparison workflow will be run on\n            # \n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        },\n\n        \"comparison_workflow_reads_assembly\" : {\n            #\n            # these parameters determine which reads and assembled \n            # reads the comparison workflow will be run on\n            # \n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        }\n    }\n}\n\n\n\n\n\nPut these in a JSON file (e.g., \nconfig/custom_workflowconfig.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Parameters\n\u00b6\n\n\nSet the \ntaxonomic_classification\n key of the Snakemake configuration dictionary\nas shown below:\n\n\n{\n    \"comparison\" : {\n        \"compute_read_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\"    : \"_scaled10k.k21_31_51.sig\", \n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compare_read_signatures\" : {\n            \"samples\" : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"csv_out\" : \"SRR606249allsamples_trim2and30_read_comparison.k{kvalue}.csv\"\n        },\n        \"compute_assembly_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\" : \"_scaled10k.k21_31_51.sig\",\n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compare_assembly_signatures\" : {\n            \"samples\"   : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"assembler\" : [\"megahit\",\"metaspades\"],\n            \"csv_out\"   : \"SRR606249_trim2and30_assembly_comparison.k{kvalue}.csv\"\n        },\n        \"compare_read_assembly_signatures\" : {\n            \"samples\"   : [\"SRR606249_subset10\"],\n            \"assembler\" : [\"megahit\",\"metaspades\"],\n            \"kvalues\"   : [21, 31, 51],\n            \"csv_out\"   : \"SRR606249_trim2and30_ra_comparison.k{kvalue}.csv\"\n        }\n    }\n}\n\n\n\n\n\n(Note that there are multiple \"k values\" being specified here, but these\ncorrespond to different steps in the workflow. The \nkvalues\n key in\nthe \ncompute_read_signatures\n and \ncompute_assembly_signatures\n section\nprovides the k values used in the intermediate steps of the workflow,\nand should be a superset of the the k values provided to the build rule.\n\n\nTo use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g.,\n\nconfig/custom_workflowparams.json\n in the \nworkflows\n directory) and pass the\nname of the config file to Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Snakemake"
        },
        {
            "location": "/comparison_snakemake/#comparison-snakemake",
            "text": "As mentioned on the  Running Workflows  page,\nthe Snakemake workflows define  build rules  that trigger all of\nthe rules composing a given workflow.  As a reminder, the  Running Workflows  page \nshowed how to call Snakemake and ask for a particular target:  $ snakemake [FLAGS] <target>",
            "title": "Comparison: Snakemake"
        },
        {
            "location": "/comparison_snakemake/#build-rules",
            "text": "The build rules are the rules that the end user should be calling.\nA list of available build rules in the taxonomic classification \nworkflow is given below.  List of available build rules in the comparison workflow:  comparison_workflow_reads\n\n    Build rule: run sourmash compare on all reads\n\ncomparison_workflow_assembly\n\n    Build rule: run sourmash compare on all assemblies\n\ncomparison_workflow_reads_assembly\n\n    Build rule: run sourmash compare on all reads and assemblies together  Pass the name of the build rule directly to Snakemake\non the command line:  $ snakemake [FLAGS] comparison_workflow_reads \\\n        comparison_workflow_assembly \\\n        comparison_workflow_reads_assembly  See below for how to configure these workflows.  See the  Quick\nStart  for details on the process of running this workflow.",
            "title": "Build Rules"
        },
        {
            "location": "/comparison_snakemake/#snakemake-configuration-dictionary",
            "text": "There are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the  Workflow Configuration  page).",
            "title": "Snakemake Configuration Dictionary"
        },
        {
            "location": "/comparison_snakemake/#data-files-configuration",
            "text": "Set the  files  key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}  Put these in a JSON file (e.g.,  config/custom_datafiles.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>",
            "title": "Data Files Configuration"
        },
        {
            "location": "/comparison_snakemake/#workflow-configuration",
            "text": "Set the  workflows  key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.  Here is the structure of the configuration dictionary\nfor taxonomic classification workflows:  {\n    \"workflows\" : {\n        \"comparison_workflow_reads\": {\n            #\n            # these parameters determine which reads\n            # the comparison workflow will be run on\n            # \n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        },\n\n        \"comparison_workflow_assembly\" : {\n            #\n            # these parameters determine which assembled reads\n            # the comparison workflow will be run on\n            # \n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        },\n\n        \"comparison_workflow_reads_assembly\" : {\n            #\n            # these parameters determine which reads and assembled \n            # reads the comparison workflow will be run on\n            # \n            \"kvalue\"    : [\"21\",\"31\",\"51\"],\n        }\n    }\n}  Put these in a JSON file (e.g.,  config/custom_workflowconfig.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>",
            "title": "Workflow Configuration"
        },
        {
            "location": "/comparison_snakemake/#workflow-parameters",
            "text": "Set the  taxonomic_classification  key of the Snakemake configuration dictionary\nas shown below:  {\n    \"comparison\" : {\n        \"compute_read_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\"    : \"_scaled10k.k21_31_51.sig\", \n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compare_read_signatures\" : {\n            \"samples\" : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"csv_out\" : \"SRR606249allsamples_trim2and30_read_comparison.k{kvalue}.csv\"\n        },\n        \"compute_assembly_signatures\" : {\n            \"scale\"         : 10000,\n            \"kvalues\"       : [21,31,51],\n            \"qual\"          : [\"2\",\"30\"],\n            \"sig_suffix\" : \"_scaled10k.k21_31_51.sig\",\n            \"merge_suffix\"  : \"_scaled10k.k21_31_51.fq.gz\"\n        },\n        \"compare_assembly_signatures\" : {\n            \"samples\"   : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"assembler\" : [\"megahit\",\"metaspades\"],\n            \"csv_out\"   : \"SRR606249_trim2and30_assembly_comparison.k{kvalue}.csv\"\n        },\n        \"compare_read_assembly_signatures\" : {\n            \"samples\"   : [\"SRR606249_subset10\"],\n            \"assembler\" : [\"megahit\",\"metaspades\"],\n            \"kvalues\"   : [21, 31, 51],\n            \"csv_out\"   : \"SRR606249_trim2and30_ra_comparison.k{kvalue}.csv\"\n        }\n    }\n}  (Note that there are multiple \"k values\" being specified here, but these\ncorrespond to different steps in the workflow. The  kvalues  key in\nthe  compute_read_signatures  and  compute_assembly_signatures  section\nprovides the k values used in the intermediate steps of the workflow,\nand should be a superset of the the k values provided to the build rule.  To use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g., config/custom_workflowparams.json  in the  workflows  directory) and pass the\nname of the config file to Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Workflow Parameters"
        },
        {
            "location": "/taxclass_overview/",
            "text": "Taxonomic Classification Using Custom Database\n\u00b6\n\n\n\n\nThe taxonomic classification workflow characterizes \nbulk metagenome data sets with the sourmash\nand kaiju tools using public and private reference databases.\n\n\nMore information:\n\n\n\n\nTaxonomic Classification Walkthrough\n\n\nTaxonomic Classification Snakemake Rules\n\n\nworkflows/taxclass/\n directory in the repository",
            "title": "Overview"
        },
        {
            "location": "/taxclass_overview/#taxonomic-classification-using-custom-database",
            "text": "The taxonomic classification workflow characterizes \nbulk metagenome data sets with the sourmash\nand kaiju tools using public and private reference databases.  More information:   Taxonomic Classification Walkthrough  Taxonomic Classification Snakemake Rules  workflows/taxclass/  directory in the repository",
            "title": "Taxonomic Classification Using Custom Database"
        },
        {
            "location": "/taxclass_walkthru/",
            "text": "Taxonomic Classification Workflow\n\u00b6\n\n\nTo run the taxonomic classification workflow, use the Snakefile in the\ntop level of the repository.\n\n\nWorkflow rules for taxonomic classification are defined in the file\n\nworkflows/taxonomic_classification/Snakefile\n.\nare defined in \nworkflows/read_filtering/Snakefile\n.\n\n\nList of Rules\n\u00b6\n\n\nThe build rules trigger portions of the workflow to run.\nThese are listed below:\n\n\n\n\n\n\ngenbank\n - download pre-assembled SBTs for sourmash built from genome\n  and protein databases (Genbank).\n\n\n\n\n\n\nsbts\n - unpack pre-assembled SBTs for sourmash built from genome and protein\n  databases (Genbank).\n\n\n\n\n\n\nmerge\n - merge read files and calculate signature file\n\n\n\n\n\n\nusekaij\n - download and unpack the kaiju database\n\n\n\n\n\n\nrunkaiju\n - run the kaiju classifier on specified input files\n\n\n\n\n\n\nrunkrona\n - run the krona tool to visualize kaiju output\n\n\n\n\n\n\nYou can see a list of all available rules and brief descriptions\nof each by using the Snakemake list command:\n\n\nsnakemake -l\n\n\n\n\n\nRunning Rules\n\u00b6\n\n\nTo run a rule, call Snakemake with specified environment variables,\ncommand line flags, and options, and pass it the name of the rule.\nFor example, the following command uses Singularity to run all rules\nthat have a singularity directive:\n\n\nSINGULARITY_BINDPATH=\"data:/data\" snakemake --with-singularity post_trim\n\n\n\n\n\nWalkthrough\n\u00b6\n\n\nThe following walkthrough covers the steps in the taxonomic classification\nworkflow.\n\n\nThis workflow covers the use of Docker to interactively run the workflow on a\nfresh Ubuntu 16.04 (Xenial) image, and requires sudo commands to be run.\n\n\nThe Snakefiles contained in this repository utilize Singularity containers\ninstead of Docker containers, but run analogous commands to those given in this\nwalkthrough.\n\n\nThis walkthrough presumes that the steps covered on the \nInstalling\n\npage have been run, and that a version of Python, Conda, and Snakemake are available.\nSee the \nInstalling\n page for instructions on installing\nrequired software.\n\n\nWalkthrough Steps\n\u00b6\n\n\nAdditional requirements:\n\n\n\n\nAt least 120 GB disk space \n\n\nAt least 64 GB RAM \n\n\n\n\nSoftware required:\n\n\n\n\nDocker (covered in this walkthrough) or Singularity (if using Snakefile)\n\n\nUpdated packages (see the \nInstalling\n page)\n\n\n(Optional) OSF CLI (command-line interface; see \nInstalling\n\n    and \nRead Filtering Snakemake\n pages)\n\n\n\n\nSourmash is a tool for calculating and comparing MinHash signatures. Sourmash gather\nallows us to taxonomically classify the components of a metagenome by comparing hashes\nin our dataset to hashes in a sequence bloom tree (SBT) representing genomes. \n\n\nFirst, let's download two SBTs containing hashes that represent the microbial genomes in\nthe \nNCBI GenBank\n and \nRefSeq\n databases. \n\n\n(Note: these are each several GB in size, and unzipped they take up around 100 GB of\nspace.)\n\n\nmkdir data/\ncd data/\n\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-refseq-sbt-k51-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-refseq-sbt-k31-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-refseq-sbt-k21-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-genbank-sbt-k51-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-genbank-sbt-k31-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-genbank-sbt-k21-2017.05.09.tar.gz\n\ntar xzf microbe-refseq-sbt-k51-2017.05.09.tar.gz\ntar xzf microbe-refseq-sbt-k31-2017.05.09.tar.gz\ntar xzf microbe-refseq-sbt-k21-2017.05.09.tar.gz\ntar xzf microbe-genbank-sbt-k51-2017.05.09.tar.gz\ntar xzf microbe-genbank-sbt-k31-2017.05.09.tar.gz\ntar xzf microbe-genbank-sbt-k21-2017.05.09.tar.gz\n\nrm -r microbe-refseq-sbt-k51-2017.05.09.tar.gz\nrm -r microbe-refseq-sbt-k31-2017.05.09.tar.gz\nrm -r microbe-refseq-sbt-k21-2017.05.09.tar.gz\nrm -r microbe-genbank-sbt-k51-2017.05.09.tar.gz\nrm -r microbe-genbank-sbt-k31-2017.05.09.tar.gz\nrm -r microbe-genbank-sbt-k21-2017.05.09.tar.gz\n\ncd ../\n\n\n\n\n\nIf you have not made the trimmed data, you can download it from OSF.\n\n\nStart with a file that contains two columns, a filename and a location, separated by a space.\n\n\nThe location should be a URL (currently works) or a local path (not implemented yet).\nIf no location is given, the script will look for the given file in the current directory.\n\n\nExample data file \ntrimmed.data\n:\n\n\nSRR606249_1.trim2.fq.gz             https://osf.io/tzkjr/download      \nSRR606249_2.trim2.fq.gz             https://osf.io/sd968/download  \nSRR606249_subset50_1.trim2.fq.gz    https://osf.io/acs5k/download \nSRR606249_subset50_2.trim2.fq.gz    https://osf.io/bem28/download \nSRR606249_subset25_1.trim2.fq.gz    https://osf.io/syf3m/download \nSRR606249_subset25_2.trim2.fq.gz    https://osf.io/zbcrx/download \nSRR606249_subset10_1.trim2.fq.gz    https://osf.io/ksu3e/download \nSRR606249_subset10_2.trim2.fq.gz    https://osf.io/k9tqn/download \n\n\n\n\n\nThis can be turned into download commands using a shell script (cut and xargs) or using a Python script.\n\n\nimport subprocess\n\nwith open('trimmed.data','r') as f:\n    for ln in f.readlines():\n        line = ln.split()\n        cmd = [\"wget\",line[1],\"-O\",line[0]]\n        print(\"Calling command %s\"%(\" \".join(cmd)))\n        subprocess.call(cmd)\n\n\n\n\n\nNext, calculate signatures for our data:\n\n\nsourmashurl=\"quay.io/biocontainers/sourmash:2.0.0a3--py36_0\"\n\nfor filename in *_1.trim2.fq.gz\ndo\n    #Remove _1.trim.fq from file name to create base\n    base=$(basename $filename _1.trim2.fq.gz)\n    sigsuffix=\".trim2.scaled10k.k21_31_51.sig\"\n    echo $base\n\n    if [ -f ${base}${sigsuffix} ]\n    then\n        # skip\n        echo \"Skipping file ${base}${sigsuffix}, file exists.\"\n    else\n        docker run \\\n            -v ${PWD}:/data \\\n            ${sourmashurl} \\\n            sourmash compute \\\n            --merge /data/${base}.trim2.fq.gz \\\n            --track-abundance \\\n            --scaled 10000 \\\n            -k 21,31,51 \\\n            /data/${base}_1.trim2.fq.gz \\\n            /data/${base}_2.trim2.fq.gz \\\n            -o /data/${base}${sigsuffix}\n    fi\ndone\n\nfor filename in *_1.trim30.fq.gz\ndo\n    #Remove _1.trim.fq from file name to create base\n    base=$(basename $filename _1.trim30.fq.gz)\n    sigsuffix=\".trim30.scaled10k.k21_31_51.sig\"\n    echo $base\n\n    if [ -f ${base}${sigsuffix} ]\n    then\n        # skip\n        echo \"Skipping file ${base}${sigsuffix}, file exists.\"\n    else\n        docker run \\\n            -v ${PWD}:/data \\\n            ${sourmashurl} \\\n            sourmash compute \\\n            --merge /data/${base}.trim30.fq.gz \\\n            --track-abundance \\\n            --scaled 10000 \\\n            -k 21,31,51 \\\n            /data/${base}_1.trim30.fq.gz \\\n            /data/${base}_2.trim30.fq.gz \\\n            -o /data/${base}${sigsuffix}\n    fi\ndone\n\n\n\n\n\nAnd compare those signatures to our database to classify the components.\n\n\nsourmashurl=\"quay.io/biocontainers/sourmash:2.0.0a3--py36_0\"\nfor kmer_len in 21 31 51\ndo\n    for sig in *sig\n    do\n        docker run \\\n            -v ${PWD}:/data \\\n            ${sourmashurl} \\\n            sourmash gather \\\n            -k ${kmer_len} \\\n            ${sig} \\\n            genbank-k${kmer_len}.sbt.json \\\n            refseq-k${kmer_len}.sbt.json \\\n            -o ${sig}.k${kmer_len}.gather.output.csv \\\n            --output-unassigned ${sig}.k${kmer_len}gather_unassigned.output.csv \\\n            --save-matches ${sig}.k${kmer_len}.gather_matches\n    done\ndone\n\n\n\n\n\nNow, let's download and unpack the kaiju database (this takes about 15 minutes on my machine):\n\n\nkaijudir=\"${PWD}/kaijudb\"\ntarfile=\"kaiju_index_nr_euk.tgz\"\n\nmkdir ${kaijudir}\ncurl -LO \"http://kaiju.binf.ku.dk/database/${tarfile}\"\ntar xzf ${tarfile}\nrm -f ${tarfile}\n\n\n\n\n\nand then link the data and run kaiju:\n\n\nfor filename in *1.trim2.fq.gz\ndo\n    #Remove _1.trim2.fq from file name to create base\n    base=$(basename $filename _1.trim2.fq.gz)\n    echo $base\n\n    # Command to run container interactively:\n    #docker run -it --rm -v ${PWD}:/data quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0 /bin/bash\n\n    docker run \\\n        -v ${PWD}:/data \\\n        quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0 \\\n        kaiju \\\n        -x \\\n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -f /data/kaijudb/kaiju_db_nr_euk.fmi \\\n        -i /data/${base}_1.trim2.fq.gz \\\n        -j /data/${base}_2.trim2.fq.gz \\\n        -o /data/${base}.kaiju_output.trim2.out \\\n        -z 4\ndone\n\nfor filename in *1.trim30.fq.gz\ndo\n    #Remove _1.trim30.fq from file name to create base\n    base=$(basename $filename _1.trim30.fq.gz)\n    echo $base\n\n    docker run \\\n        -v ${PWD}:/data \\\n        quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0 \\\n        kaiju \\\n        -x \\\n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -f data/kaijudb/kaiju_db_nr_euk.fmi \\\n        -i /data/${base}_1.trim30.fq.gz \\\n        -j /data/${base}_2.trim30.fq.gz \\\n        -o /data/${base}.kaiju_output.trim30.out \\\n        -z 4\ndone\n\n\n\n\n\nNext, convert kaiju output to format readable by krona. Note that the taxonomic rank for classification (e.g. genus) is determined with the -r flag. \n\n\nkaijuurl=\"quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0\"\nkaijudir=\"kaijudb\"\nfor i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaiju2krona \\\n        -v \\\n        -t \\\n        /data/${kaijudir}/nodes.dmp \\\n        -n /data/${kaijudir}/names.dmp \\\n        -i /data/${i} \\\n        -o /data/${i}.kaiju.out.krona\ndone\n\nfor i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaijuReport \\\n        -v \\\n        -t \\\n        /data/${kaijudir}/nodes.dmp \\\n        -n /data/${kaijudir}/names.dmp \\\n        -i /data/${i} \\\n        -r genus \\\n        -o /data/${i}.kaiju_out_krona.summary \ndone\n\n\n\n\n\nNow let's filter out taxa with low abundances by obtaining genera that comprise at least 1 percent of the total reads:\n\n\nkaijuurl=\"quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0\"\nfor i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaijuReport \n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -n /data/kaijudb/names.dmp \\\n        -i /data/${i} \\\n        -r genus \\\n        -m 1 \\\n        -o /data/${i}.kaiju_out_krona.1percenttotal.summary\ndone\n\n\n\n\n\nNow for comparison, let's take the genera that comprise at least 1 percent of all of the classified reads:\n\n\nfor i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaijuReport \\\n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -n /data/kaijudb/names.dmp \\\n        -i /data/${i} \\\n        -r genus \\\n        -m 1 \\\n        -u \\\n        -o /data/${i}.kaiju_out_krona.1percentclassified.summary\ndone\n\n\n\n\n\nDownload the krona image from quay.io so we can visualize the results from kaiju:\n\n\nkaijudir=\"${PWD}/kaijudb\"\nsuffix=\"kaiju_out_krona\"\nkronaurl=\"quay.io/biocontainers/krona:2.7--pl5.22.0_1\"\ndocker pull ${kronaurl}\n\n\n\n\n\nGenerate krona html with output from all of the reads:\n\n\nsuffix=\"kaiju_out_krona\"\nfor i in *${suffix}.summary\ndo\n        docker run \\\n            -v ${kaijudir}:/data \\\n            ${kronaurl} \\\n            ktImportText \\\n            -o /data/${i}.${suffix}.html \\\n            /data/${i}\ndone\n\n\n\n\n\nGenerate krona html with output from genera at least 1 percent of the total reads:\n\n\nsuffix=\"kaiju_out_krona.1percenttotal\"\nfor i in *${suffix}.summary\ndo\n        docker run \\\n            -v ${kaijudir}:/data \\\n            ${kronaurl} \\\n            ktImportText \\\n            -o /data/${i}.${suffix}.html \\\n            /data/${i}\ndone\n\n\n\n\n\nGenerate krona html with output from genera at least 1 percent of all classified reads:\n\n\nsuffix=\"kaiju_out_krona.1percentclassified\"\nfor i in *${suffix}.summary\ndo\n        docker run \\\n            -v ${kaijudir}:/data \\\n            ${kronaurl} \\\n            ktImportText \\\n            -o /data/${i}.${suffix}.html \\\n            /data/${i}\ndone",
            "title": "Walkthrough"
        },
        {
            "location": "/taxclass_walkthru/#taxonomic-classification-workflow",
            "text": "To run the taxonomic classification workflow, use the Snakefile in the\ntop level of the repository.  Workflow rules for taxonomic classification are defined in the file workflows/taxonomic_classification/Snakefile .\nare defined in  workflows/read_filtering/Snakefile .",
            "title": "Taxonomic Classification Workflow"
        },
        {
            "location": "/taxclass_walkthru/#list-of-rules",
            "text": "The build rules trigger portions of the workflow to run.\nThese are listed below:    genbank  - download pre-assembled SBTs for sourmash built from genome\n  and protein databases (Genbank).    sbts  - unpack pre-assembled SBTs for sourmash built from genome and protein\n  databases (Genbank).    merge  - merge read files and calculate signature file    usekaij  - download and unpack the kaiju database    runkaiju  - run the kaiju classifier on specified input files    runkrona  - run the krona tool to visualize kaiju output    You can see a list of all available rules and brief descriptions\nof each by using the Snakemake list command:  snakemake -l",
            "title": "List of Rules"
        },
        {
            "location": "/taxclass_walkthru/#running-rules",
            "text": "To run a rule, call Snakemake with specified environment variables,\ncommand line flags, and options, and pass it the name of the rule.\nFor example, the following command uses Singularity to run all rules\nthat have a singularity directive:  SINGULARITY_BINDPATH=\"data:/data\" snakemake --with-singularity post_trim",
            "title": "Running Rules"
        },
        {
            "location": "/taxclass_walkthru/#walkthrough",
            "text": "The following walkthrough covers the steps in the taxonomic classification\nworkflow.  This workflow covers the use of Docker to interactively run the workflow on a\nfresh Ubuntu 16.04 (Xenial) image, and requires sudo commands to be run.  The Snakefiles contained in this repository utilize Singularity containers\ninstead of Docker containers, but run analogous commands to those given in this\nwalkthrough.  This walkthrough presumes that the steps covered on the  Installing \npage have been run, and that a version of Python, Conda, and Snakemake are available.\nSee the  Installing  page for instructions on installing\nrequired software.",
            "title": "Walkthrough"
        },
        {
            "location": "/taxclass_walkthru/#walkthrough-steps",
            "text": "Additional requirements:   At least 120 GB disk space   At least 64 GB RAM    Software required:   Docker (covered in this walkthrough) or Singularity (if using Snakefile)  Updated packages (see the  Installing  page)  (Optional) OSF CLI (command-line interface; see  Installing \n    and  Read Filtering Snakemake  pages)   Sourmash is a tool for calculating and comparing MinHash signatures. Sourmash gather\nallows us to taxonomically classify the components of a metagenome by comparing hashes\nin our dataset to hashes in a sequence bloom tree (SBT) representing genomes.   First, let's download two SBTs containing hashes that represent the microbial genomes in\nthe  NCBI GenBank  and  RefSeq  databases.   (Note: these are each several GB in size, and unzipped they take up around 100 GB of\nspace.)  mkdir data/\ncd data/\n\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-refseq-sbt-k51-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-refseq-sbt-k31-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-refseq-sbt-k21-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-genbank-sbt-k51-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-genbank-sbt-k31-2017.05.09.tar.gz\ncurl -O https://s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-genbank-sbt-k21-2017.05.09.tar.gz\n\ntar xzf microbe-refseq-sbt-k51-2017.05.09.tar.gz\ntar xzf microbe-refseq-sbt-k31-2017.05.09.tar.gz\ntar xzf microbe-refseq-sbt-k21-2017.05.09.tar.gz\ntar xzf microbe-genbank-sbt-k51-2017.05.09.tar.gz\ntar xzf microbe-genbank-sbt-k31-2017.05.09.tar.gz\ntar xzf microbe-genbank-sbt-k21-2017.05.09.tar.gz\n\nrm -r microbe-refseq-sbt-k51-2017.05.09.tar.gz\nrm -r microbe-refseq-sbt-k31-2017.05.09.tar.gz\nrm -r microbe-refseq-sbt-k21-2017.05.09.tar.gz\nrm -r microbe-genbank-sbt-k51-2017.05.09.tar.gz\nrm -r microbe-genbank-sbt-k31-2017.05.09.tar.gz\nrm -r microbe-genbank-sbt-k21-2017.05.09.tar.gz\n\ncd ../  If you have not made the trimmed data, you can download it from OSF.  Start with a file that contains two columns, a filename and a location, separated by a space.  The location should be a URL (currently works) or a local path (not implemented yet).\nIf no location is given, the script will look for the given file in the current directory.  Example data file  trimmed.data :  SRR606249_1.trim2.fq.gz             https://osf.io/tzkjr/download      \nSRR606249_2.trim2.fq.gz             https://osf.io/sd968/download  \nSRR606249_subset50_1.trim2.fq.gz    https://osf.io/acs5k/download \nSRR606249_subset50_2.trim2.fq.gz    https://osf.io/bem28/download \nSRR606249_subset25_1.trim2.fq.gz    https://osf.io/syf3m/download \nSRR606249_subset25_2.trim2.fq.gz    https://osf.io/zbcrx/download \nSRR606249_subset10_1.trim2.fq.gz    https://osf.io/ksu3e/download \nSRR606249_subset10_2.trim2.fq.gz    https://osf.io/k9tqn/download   This can be turned into download commands using a shell script (cut and xargs) or using a Python script.  import subprocess\n\nwith open('trimmed.data','r') as f:\n    for ln in f.readlines():\n        line = ln.split()\n        cmd = [\"wget\",line[1],\"-O\",line[0]]\n        print(\"Calling command %s\"%(\" \".join(cmd)))\n        subprocess.call(cmd)  Next, calculate signatures for our data:  sourmashurl=\"quay.io/biocontainers/sourmash:2.0.0a3--py36_0\"\n\nfor filename in *_1.trim2.fq.gz\ndo\n    #Remove _1.trim.fq from file name to create base\n    base=$(basename $filename _1.trim2.fq.gz)\n    sigsuffix=\".trim2.scaled10k.k21_31_51.sig\"\n    echo $base\n\n    if [ -f ${base}${sigsuffix} ]\n    then\n        # skip\n        echo \"Skipping file ${base}${sigsuffix}, file exists.\"\n    else\n        docker run \\\n            -v ${PWD}:/data \\\n            ${sourmashurl} \\\n            sourmash compute \\\n            --merge /data/${base}.trim2.fq.gz \\\n            --track-abundance \\\n            --scaled 10000 \\\n            -k 21,31,51 \\\n            /data/${base}_1.trim2.fq.gz \\\n            /data/${base}_2.trim2.fq.gz \\\n            -o /data/${base}${sigsuffix}\n    fi\ndone\n\nfor filename in *_1.trim30.fq.gz\ndo\n    #Remove _1.trim.fq from file name to create base\n    base=$(basename $filename _1.trim30.fq.gz)\n    sigsuffix=\".trim30.scaled10k.k21_31_51.sig\"\n    echo $base\n\n    if [ -f ${base}${sigsuffix} ]\n    then\n        # skip\n        echo \"Skipping file ${base}${sigsuffix}, file exists.\"\n    else\n        docker run \\\n            -v ${PWD}:/data \\\n            ${sourmashurl} \\\n            sourmash compute \\\n            --merge /data/${base}.trim30.fq.gz \\\n            --track-abundance \\\n            --scaled 10000 \\\n            -k 21,31,51 \\\n            /data/${base}_1.trim30.fq.gz \\\n            /data/${base}_2.trim30.fq.gz \\\n            -o /data/${base}${sigsuffix}\n    fi\ndone  And compare those signatures to our database to classify the components.  sourmashurl=\"quay.io/biocontainers/sourmash:2.0.0a3--py36_0\"\nfor kmer_len in 21 31 51\ndo\n    for sig in *sig\n    do\n        docker run \\\n            -v ${PWD}:/data \\\n            ${sourmashurl} \\\n            sourmash gather \\\n            -k ${kmer_len} \\\n            ${sig} \\\n            genbank-k${kmer_len}.sbt.json \\\n            refseq-k${kmer_len}.sbt.json \\\n            -o ${sig}.k${kmer_len}.gather.output.csv \\\n            --output-unassigned ${sig}.k${kmer_len}gather_unassigned.output.csv \\\n            --save-matches ${sig}.k${kmer_len}.gather_matches\n    done\ndone  Now, let's download and unpack the kaiju database (this takes about 15 minutes on my machine):  kaijudir=\"${PWD}/kaijudb\"\ntarfile=\"kaiju_index_nr_euk.tgz\"\n\nmkdir ${kaijudir}\ncurl -LO \"http://kaiju.binf.ku.dk/database/${tarfile}\"\ntar xzf ${tarfile}\nrm -f ${tarfile}  and then link the data and run kaiju:  for filename in *1.trim2.fq.gz\ndo\n    #Remove _1.trim2.fq from file name to create base\n    base=$(basename $filename _1.trim2.fq.gz)\n    echo $base\n\n    # Command to run container interactively:\n    #docker run -it --rm -v ${PWD}:/data quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0 /bin/bash\n\n    docker run \\\n        -v ${PWD}:/data \\\n        quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0 \\\n        kaiju \\\n        -x \\\n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -f /data/kaijudb/kaiju_db_nr_euk.fmi \\\n        -i /data/${base}_1.trim2.fq.gz \\\n        -j /data/${base}_2.trim2.fq.gz \\\n        -o /data/${base}.kaiju_output.trim2.out \\\n        -z 4\ndone\n\nfor filename in *1.trim30.fq.gz\ndo\n    #Remove _1.trim30.fq from file name to create base\n    base=$(basename $filename _1.trim30.fq.gz)\n    echo $base\n\n    docker run \\\n        -v ${PWD}:/data \\\n        quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0 \\\n        kaiju \\\n        -x \\\n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -f data/kaijudb/kaiju_db_nr_euk.fmi \\\n        -i /data/${base}_1.trim30.fq.gz \\\n        -j /data/${base}_2.trim30.fq.gz \\\n        -o /data/${base}.kaiju_output.trim30.out \\\n        -z 4\ndone  Next, convert kaiju output to format readable by krona. Note that the taxonomic rank for classification (e.g. genus) is determined with the -r flag.   kaijuurl=\"quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0\"\nkaijudir=\"kaijudb\"\nfor i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaiju2krona \\\n        -v \\\n        -t \\\n        /data/${kaijudir}/nodes.dmp \\\n        -n /data/${kaijudir}/names.dmp \\\n        -i /data/${i} \\\n        -o /data/${i}.kaiju.out.krona\ndone\n\nfor i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaijuReport \\\n        -v \\\n        -t \\\n        /data/${kaijudir}/nodes.dmp \\\n        -n /data/${kaijudir}/names.dmp \\\n        -i /data/${i} \\\n        -r genus \\\n        -o /data/${i}.kaiju_out_krona.summary \ndone  Now let's filter out taxa with low abundances by obtaining genera that comprise at least 1 percent of the total reads:  kaijuurl=\"quay.io/biocontainers/kaiju:1.6.1--pl5.22.0_0\"\nfor i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaijuReport \n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -n /data/kaijudb/names.dmp \\\n        -i /data/${i} \\\n        -r genus \\\n        -m 1 \\\n        -o /data/${i}.kaiju_out_krona.1percenttotal.summary\ndone  Now for comparison, let's take the genera that comprise at least 1 percent of all of the classified reads:  for i in *trim{\"2\",\"30\"}.out\ndo\n    docker run \\\n        -v ${PWD}:/data \\\n        ${kaijuurl} \\\n        kaijuReport \\\n        -v \\\n        -t /data/kaijudb/nodes.dmp \\\n        -n /data/kaijudb/names.dmp \\\n        -i /data/${i} \\\n        -r genus \\\n        -m 1 \\\n        -u \\\n        -o /data/${i}.kaiju_out_krona.1percentclassified.summary\ndone  Download the krona image from quay.io so we can visualize the results from kaiju:  kaijudir=\"${PWD}/kaijudb\"\nsuffix=\"kaiju_out_krona\"\nkronaurl=\"quay.io/biocontainers/krona:2.7--pl5.22.0_1\"\ndocker pull ${kronaurl}  Generate krona html with output from all of the reads:  suffix=\"kaiju_out_krona\"\nfor i in *${suffix}.summary\ndo\n        docker run \\\n            -v ${kaijudir}:/data \\\n            ${kronaurl} \\\n            ktImportText \\\n            -o /data/${i}.${suffix}.html \\\n            /data/${i}\ndone  Generate krona html with output from genera at least 1 percent of the total reads:  suffix=\"kaiju_out_krona.1percenttotal\"\nfor i in *${suffix}.summary\ndo\n        docker run \\\n            -v ${kaijudir}:/data \\\n            ${kronaurl} \\\n            ktImportText \\\n            -o /data/${i}.${suffix}.html \\\n            /data/${i}\ndone  Generate krona html with output from genera at least 1 percent of all classified reads:  suffix=\"kaiju_out_krona.1percentclassified\"\nfor i in *${suffix}.summary\ndo\n        docker run \\\n            -v ${kaijudir}:/data \\\n            ${kronaurl} \\\n            ktImportText \\\n            -o /data/${i}.${suffix}.html \\\n            /data/${i}\ndone",
            "title": "Walkthrough Steps"
        },
        {
            "location": "/taxclass_snakemake/",
            "text": "Taxonomic Classification: Snakemake\n\u00b6\n\n\nAs mentioned on the \nRunning Workflows\n page,\nthe Snakemake workflows define \nbuild rules\n that trigger all of\nthe rules composing a given workflow.\n\n\nAs a reminder, the \nRunning Workflows\n page \nshowed how to call Snakemake and ask for a particular target:\n\n\n$ snakemake [FLAGS] <target>\n\n\n\n\n\nYou can replace \n<target>\n with any of the build rules below.\n\n\nBuild Rules\n\u00b6\n\n\nThe build rules are the rules that the end user should be calling.\nA list of available build rules in the taxonomic classification \nworkflow is given below.\n\n\ntaxonomic_classification_signatures_workflow\n\n    Build rule: trigger calculation of signatures from reads.\n\ntaxonomic_classification_gather_workflow\n\n    Gather and compare read signatures using sourmash gather\n\ntaxonomic_classification_kaijureport_workflow\n\n    Run kaiju and generate a report from all results.\n\ntaxonomic_classification_kaijureport_filtered_workflow\n\n    Run kaiju and generate a report from filtered\n    results (taxa with <X% abundance).\n\ntaxonomic_classification_kaijureport_filteredclass_workflow\n\n    Run kaiju and generate a report from filtered, classified\n    results (taxa with <X% abundance).\n\n\n\n\n\nPass the name of the build rule directly to Snakemake\non the command line:\n\n\n$ snakemake [FLAGS] taxonomic_classification_kaijureport_workflow \\\n        taxonomic_classification_kaijureport_filtered_workflow \\\n        taxonomic_classification_kaijureport_filteredclass_workflow \n\n\n\n\n\nSee below for how to configure these workflows.  See the \nQuick\nStart\n for details on the process of running this workflow.\n\n\nSnakemake Configuration Dictionary\n\u00b6\n\n\nThere are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the \n\nWorkflow Configuration\n page).\n\n\nData Files Configuration\n\u00b6\n\n\nSet the \nfiles\n key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:\n\n\n{\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}\n\n\n\n\n\nPut these in a JSON file (e.g., \nconfig/custom_datafiles.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Configuration\n\u00b6\n\n\nSet the \nworkflows\n key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.\n\n\nHere is the structure of the configuration dictionary\nfor taxonomic classification workflows:\n\n\n{\n    \"workflows\" : {\n        \"taxonomic_classification_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_signatures_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_gather_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n            \"kvalues\" : [\"21\",\"31\",\"51\"]\n        },\n\n        \"taxonomic_classification_kaijureport_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_kaijureport_filtered_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_kaijureport_filteredclass_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        }\n    }\n}\n\n\n\n\n\nPut these in a JSON file (e.g., \nconfig/custom_workflowconfig.json\n \nin the \nworkflows\n directory) and pass the name of the config file\nto Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>\n\n\n\n\n\nWorkflow Parameters\n\u00b6\n\n\nSet the \ntaxonomic_classification\n key of the Snakemake configuration dictionary\nas shown below:\n\n\n{\n    \"taxonomic_classification\" : {\n\n        \"filter_taxa\" : {\n            \"pct_threshold\" : 1\n        },\n\n        \"kaiju\" : {\n            \"dmp1\" : \"nodes.dmp\",\n            \"dmp2\" : \"names.dmp\",\n            \"fmi\"  : \"kaiju_db_nr.fmi\",\n            \"tar\"  : \"kaiju_index_nr.tgz\",\n            \"url\"  : \"http://kaiju.binf.ku.dk/database\",\n            \"out\"  : \"{sample}.kaiju_output.trim{qual}.out\"\n        },\n\n        \"kaiju_report\" : {\n            \"taxonomic_rank\" : \"genus\",\n            \"pct_threshold\"  : 1\n        },\n\n        \"sourmash\" : { \n            \"sbturl\"  : \"s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu\",\n            \"sbttar\"  : \"microbe-{database}-sbt-k{kvalue}-2017.05.09.tar.gz\",\n            \"sbtunpack\" : \"{database}-k{kvalue}.sbt.json\",\n            \"databases\" : [\"genbank\",\"refseq\"],\n            \"gather_csv_out\"        : \"{sample}-k{kvalue}.trim{qual}.gather_output.csv\",\n            \"gather_unassigned_out\" : \"{sample}-k{kvalue}.trim{qual}.gather_unassigned.csv\",\n            \"gather_matches_out\"    : \"{sample}-k{kvalue}.trim{qual}.gather_matches.csv\"\n        },\n\n        \"visualize_krona\" : {\n            \"input_summary\"  : \"{sample}.kaiju_output.trim{qual}.summary\",\n        }\n    }\n}\n\n\n\n\n\nTo use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g.,\n\nconfig/custom_workflowparams.json\n in the \nworkflows\n directory) and pass the\nname of the config file to Snakemake using the \n--configfile\n flag:\n\n\n$ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Snakemake"
        },
        {
            "location": "/taxclass_snakemake/#taxonomic-classification-snakemake",
            "text": "As mentioned on the  Running Workflows  page,\nthe Snakemake workflows define  build rules  that trigger all of\nthe rules composing a given workflow.  As a reminder, the  Running Workflows  page \nshowed how to call Snakemake and ask for a particular target:  $ snakemake [FLAGS] <target>  You can replace  <target>  with any of the build rules below.",
            "title": "Taxonomic Classification: Snakemake"
        },
        {
            "location": "/taxclass_snakemake/#build-rules",
            "text": "The build rules are the rules that the end user should be calling.\nA list of available build rules in the taxonomic classification \nworkflow is given below.  taxonomic_classification_signatures_workflow\n\n    Build rule: trigger calculation of signatures from reads.\n\ntaxonomic_classification_gather_workflow\n\n    Gather and compare read signatures using sourmash gather\n\ntaxonomic_classification_kaijureport_workflow\n\n    Run kaiju and generate a report from all results.\n\ntaxonomic_classification_kaijureport_filtered_workflow\n\n    Run kaiju and generate a report from filtered\n    results (taxa with <X% abundance).\n\ntaxonomic_classification_kaijureport_filteredclass_workflow\n\n    Run kaiju and generate a report from filtered, classified\n    results (taxa with <X% abundance).  Pass the name of the build rule directly to Snakemake\non the command line:  $ snakemake [FLAGS] taxonomic_classification_kaijureport_workflow \\\n        taxonomic_classification_kaijureport_filtered_workflow \\\n        taxonomic_classification_kaijureport_filteredclass_workflow   See below for how to configure these workflows.  See the  Quick\nStart  for details on the process of running this workflow.",
            "title": "Build Rules"
        },
        {
            "location": "/taxclass_snakemake/#snakemake-configuration-dictionary",
            "text": "There are three types of key-value pairs that can be set in the \nSnakemake configuration dictionary (also see the  Workflow Configuration  page).",
            "title": "Snakemake Configuration Dictionary"
        },
        {
            "location": "/taxclass_snakemake/#data-files-configuration",
            "text": "Set the  files  key to a dictionary containing\na list of key-value pairs, where the keys are \nfilenames and values are URLs:  {\n    \"files\" : {\n        \"SRR606249_1_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n        \"SRR606249_2_reads.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n        \"SRR606249_subset10_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n        \"SRR606249_subset10_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n        \"SRR606249_subset25_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n        \"SRR606249_subset25_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n        \"SRR606249_subset50_1_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n        \"SRR606249_subset50_2_reads.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\",\n    }\n}  Put these in a JSON file (e.g.,  config/custom_datafiles.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_datafiles.json [FLAGS] <target>",
            "title": "Data Files Configuration"
        },
        {
            "location": "/taxclass_snakemake/#workflow-configuration",
            "text": "Set the  workflows  key of the Snakemake configuration dictionary to a\ndictionary containing details about the workflow you want to run.  The workflow\nconfiguration values are values that will end up in the final output file's\nfilename.  Here is the structure of the configuration dictionary\nfor taxonomic classification workflows:  {\n    \"workflows\" : {\n        \"taxonomic_classification_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_signatures_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_gather_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n            \"kvalues\" : [\"21\",\"31\",\"51\"]\n        },\n\n        \"taxonomic_classification_kaijureport_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_kaijureport_filtered_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        },\n\n        \"taxonomic_classification_kaijureport_filteredclass_workflow\" : {\n            \"sample\"  : [\"SRR606249_subset10\",\"SRR606249_subset25\"],\n            \"qual\" : [\"2\",\"30\"],\n        }\n    }\n}  Put these in a JSON file (e.g.,  config/custom_workflowconfig.json  \nin the  workflows  directory) and pass the name of the config file\nto Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowconfig.json [FLAGS] <target>",
            "title": "Workflow Configuration"
        },
        {
            "location": "/taxclass_snakemake/#workflow-parameters",
            "text": "Set the  taxonomic_classification  key of the Snakemake configuration dictionary\nas shown below:  {\n    \"taxonomic_classification\" : {\n\n        \"filter_taxa\" : {\n            \"pct_threshold\" : 1\n        },\n\n        \"kaiju\" : {\n            \"dmp1\" : \"nodes.dmp\",\n            \"dmp2\" : \"names.dmp\",\n            \"fmi\"  : \"kaiju_db_nr.fmi\",\n            \"tar\"  : \"kaiju_index_nr.tgz\",\n            \"url\"  : \"http://kaiju.binf.ku.dk/database\",\n            \"out\"  : \"{sample}.kaiju_output.trim{qual}.out\"\n        },\n\n        \"kaiju_report\" : {\n            \"taxonomic_rank\" : \"genus\",\n            \"pct_threshold\"  : 1\n        },\n\n        \"sourmash\" : { \n            \"sbturl\"  : \"s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu\",\n            \"sbttar\"  : \"microbe-{database}-sbt-k{kvalue}-2017.05.09.tar.gz\",\n            \"sbtunpack\" : \"{database}-k{kvalue}.sbt.json\",\n            \"databases\" : [\"genbank\",\"refseq\"],\n            \"gather_csv_out\"        : \"{sample}-k{kvalue}.trim{qual}.gather_output.csv\",\n            \"gather_unassigned_out\" : \"{sample}-k{kvalue}.trim{qual}.gather_unassigned.csv\",\n            \"gather_matches_out\"    : \"{sample}-k{kvalue}.trim{qual}.gather_matches.csv\"\n        },\n\n        \"visualize_krona\" : {\n            \"input_summary\"  : \"{sample}.kaiju_output.trim{qual}.summary\",\n        }\n    }\n}  To use custom values for these parameters, put the configuration dictionary\nabove (or any subset of it) into a JSON file (e.g., config/custom_workflowparams.json  in the  workflows  directory) and pass the\nname of the config file to Snakemake using the  --configfile  flag:  $ snakemake --configfile=config/custom_workflowparams.json [FLAGS] <target>",
            "title": "Workflow Parameters"
        },
        {
            "location": "/contrib/",
            "text": "How to contribute\n\u00b6\n\n\nThis document was inspired by the \nkhmer project getting started\ndocumentation\n.\n\n\nThis document is for people that want to contribute to the dahak metagenomics\nproject. It walks contributors through getting started with GitHub, creating an\nissue, claiming an issue, making a pull request, and where to store data and\nnewly created Docker images.\n\n\nQuickstart\n\u00b6\n\n\nDahak is open source, open data project and we welcome contributions at all\nlevels. We encourage you to submit issues, suggest documentation changes,\ncontribute code, images, workflows etc. Any software included in the workflows\nmust be open source, findable, and reusable. Check out \nGetting\nStarted\n, analyze\nsome data, and contribute however you see fit.\n\n\nHow to get started\n\u00b6\n\n\n\n\n\n\nCreate a \nGitHub\n account.\n\n\n\n\n\n\nFork \nhttps://github.com/dahak-metagenomics/dahak\n.\n\n\nVisit that page, and then click 'Fork' in the upper right-hand corner. This\ncreates a copy of the dahak source code in your GitHub account. If you're\nnew to GitHub or want a refresher, please check out this awesome\n\ntutorial\n.\n\n\n\n\n\n\nClone a copy of dahak into your local environment (or work in your browser!).\n\n\nYour shell command should look something like (you can click the 'clone or\ndownload' button on the dahak github, copy the link, and insert git clone\nbefore it):\n\n\ngit clone https://github.com/dahak-metagenomics/dahak.git\n\n\nThis makes a copy of the dahak repository on your local machine.\n\n\n\n\n\n\nClaiming an issue and starting to develop\n\u00b6\n\n\n\n\n\n\nFind an open issue and claim it.\n\n\nGo to the list \nopen\nissues\n and claim one\nyou like. Once you've found an issue you like, open it, click 'Assignees',\nand assign yourself. Once you've assigned yourself make a comment below the\nissue saying \"I'm working on this.\" That's it; it's all yours. Well not\nreally, because you can always ask for help. Just make another comment below\nstating what you need some help with and we'll get right back to you.\n\n\n(Staking your claim is super important because we're trying to avoid people\nworking on the same issue.)\n\n\n\n\n\n\nIn your local copy of the source code, update your master branch from the main dahak branch.\n\n\ngit checkout master\ngit pull dahak master\n\n\n(This pulls in the latest changes from the master repository)\n\n\nIf git complains about 'merge conflicts' when you execute \ngit pull\n,\nplease refer to the \nResolving merge conflicts\n\n\nsection\n\nof the khmer documentation.\n\n\nIf you are asked to make changes before your pull request can be accepted,\nyou can continue to commit additional changes to the branch associated with\nyour original pull request. The pull request will automatically be updated\neach time you commit to that branch. Github provides a medium for\ncommunicating and providing feedback. Once the pull request is approved, it\nwill be merged into the main branch by the dahak development team.\n\n\n\n\n\n\nCreate a new branch and link it to your fork on GitHub:\n\n\ngit checkout -b name-of-branch\ngit push -u origin name-of-branch\n\n\nwhere you replace \"name-of-branch\" with 2-3 words separated by\ndashes or underscores describing what issue it fixes.\n\n\n\n\n\n\nMake some changes and commit them to your branch.\n\n\nAfter you've made a set of cohesive changes, run the command \ngit status\n.\nThis will display a list of all the files git has noticed you changed. Files\nin the 'untracked' section are files that weren't in the repository before\nbut git has seen.\n\n\nTo commit these changes you have to 'stage' them using the following command.\n\n\ngit add path/to/file\n\n\nOnce you've staged your changes, it's time to make a commit (Don't forget to\nchange path/to/file to the actual path to file):\n\n\ngit commit -m 'Provide a brief description of the changes you made here'\n\n\nPlease make your commit message informative but concise \u2014 these messages\nbecome a part of the history of the repo and an informative message will\nhelp track down changes later. Don't stress over this too much, but before\nyou press the button, please consider whether you will find this commit\nmessage useful when a bug pops up 6 months from now and you need to sort\nthrough issues to find the right one. Once your changes have been commited,\npush them to the remote branch:\n\n\ngit push origin\n\n\n\n\n\n\nAs you develop, please periodically update your branch with changes that have\n   been made to the master branch, and resolve any conflicts that come up.\n\n\ngit pull master\n\n\n\n\n\n\nRepeat until your ready to commit to master\n\n\n\n\n\n\nSet up a 'Pull Request' asking to merge your changes into the master dahak repository\n\n\nIn a web browser, go to your GitHub fork of dahak, e.g.:\n\n\nhttps://github.com/your-github-username/dahak\n\nand you will see a list of 'recently pushed branches' just above the source\ncode listing. On the right side, there should be a green 'Compare and pull\nrequest' button. This will add a pull request submission checklist in the\nfollowing form:\n\n\nMerge Checklist\n - Typos are a sign of poorly maintained code. Has this request been checked with a spell checker?\n - Tutorials should be universally reproducible. If this request modifies a tutorial, does it assume we're starting from a blank Ubuntu 16.04 (Xenial Xerus) image?\n - Large diffs to binary or data files can artificially inflate the size of the repository. Are there large diffs to binary or data files, and are these changes necessary?\n\n\nNext, click \"Create Pull Request\". This creates a new issue where others can review and make suggestions before your code is added the master dahak repository.\n\n\n\n\n\n\nReview the pull request checklist and make changes, if necessary.\n\n\nCheck off as many boxes as possible and make a comment if you need help. If\nyou have an \nORCID ID\n, please add that as a comment. Dahak\nis an open-source, community-driven project and we'd like to acknowledge\nyour contribution when we publish. Including your ORCID ID helps that\nprocess move smoothly.\n\n\nAs you add new changes, you can keep pushing to your pull request using\n\ngit push origin\n.\n\n\n\n\n\n\nWhen you're ready to have the pull request reviewed, please mention\n    @brooksph, @charlesreid1, @kternus, @stephenturner, @ctb or anyone else on\n    the list of\n    \ncollaborators\n\n    plus the comment \nready for review\n. Often pull requests will require\n    changes, need more work before they can be merged, or simply need to be\n    addressed later. Adding tags can help with organizing. Check out this list\n    for some examples of\n    \ntags\n.\n\n\n\n\n\n\nOnce your issue has been reviewed an merged, stand-up, throw your hands in\n    the air, and do a little dance; you're officially a GitHub master and a\n    contributor to the dahak project \u2013 we hold you in the highest of regards.\n\n\n\n\n\n\nMy pull request was merged. What now?\n\u00b6\n\n\nBefore continuing on your journey towards your next pull request, there are a\ncouple of steps that you need take to clean up your local copy of dahak.\n\n\ngit checkout master\ngit pull master\ngit branch -d my-branch-name     # delete the branch locally\ngit push origin :my-branch-name  # delete the branch on GitHub (your fork)\n\n\n\n\n\nI have a dataset to contribute to the project\n\u00b6\n\n\nGreat! A big part of this project is benchmarking tools to determine when and\nhow we should use them. Datasets with interesting composition help us uncover\nnew and interesting things about metagenomics tools. If you have a dataset that\nyou would like to benchmark and/or submit for benchmarking please create an\nissue and mention @brooksph, @kternus, or @ctb. In general, we'll advise you to\nmake the data publicly available and go crazy characterizing it. We can help you\nthink about the best way to do it but in general we're using the workflows in the\n\nworkflows/\n\nrepository and analyzing the data in Jupyter notebooks. Poke us and we'd be\nhappy to discuss the process. If your dataset is less than 5 GB in size, the\n\nopen science framework\n is a great, free place to put it.\n\n\nI have a tool to contribute to the project\n\u00b6\n\n\nGreater! The more tools the better. A major goal of this project to make more\ntools easy to use. We opted to do this by using or creating containerized tools.\nThe \nbiocontainers project\n is leading the way in\nthis effort and we've contributed a few things there. You're not required to\ncontribute to the biocontainers project to add a tool to this project but the\ntool must be open source and the image must be stored in a public repository\nlike \nDocker hub\n or \nquay.io\n.\n\nHere's\n\nan excellent guide to getting started building containers. We're using\n\nSingularity\n to run the containers in\nour workflows. Docker does not need to be installed.\n\n\nI have a workflow to contribute to the project\n\u00b6\n\n\nGreatest! New metagenomics analysis tools are created all the time. We're using\na small subset that we think encompasses most the methods that are most commonly\nused to probe metagenomic communities. If you want to include a new tool or\nworkflow, create an issue and we can point you in the right direction. The\ncritical bits are we're stringing together open-source, containerized tools to\nmake workflows using \nSnakemake\n.\nTake a look here for a \nbasic\nexample\n\nand here for a bit \nmore\nflavor\n.\nThis is a work in progress but the second example is where we're headed.\n\n\nContributor Code of Conduct\n\u00b6\n\n\nAs contributors and maintainers of this project, we pledge to respect all people\nwho contribute through reporting issues, posting feature requests, updating\ndocumentation, submitting pull requests or patches, and other activities.\n\n\nWe are committed to making participation in this project a harassment-free\nexperience for everyone, regardless of level of experience, gender, gender\nidentity and expression, sexual orientation, disability, personal appearance,\nbody size, race, ethnicity, age, or religion.\n\n\nExamples of unacceptable behavior by participants include the use of sexual\nlanguage or imagery, derogatory comments or personal attacks, trolling, public\nor private harassment, insults, or other unprofessional conduct.\n\n\nProject maintainers have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct. Project maintainers who do not follow the\nCode of Conduct may be removed from the project team.\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by emailing \nPhil Brooks\n or \nC. Titus\nBrown\n. To report an issue involving either of them, please\nemail \nJudi Brown Clarke\n, Ph.D. the Diversity Director at the\nBEACON Center for the Study of Evolution in Action, an NSF Center for Science\nand Technology.\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.0.0,\navailable from http://contributor-covenant.org/version/1/0/0/",
            "title": "Contributing"
        },
        {
            "location": "/contrib/#how-to-contribute",
            "text": "This document was inspired by the  khmer project getting started\ndocumentation .  This document is for people that want to contribute to the dahak metagenomics\nproject. It walks contributors through getting started with GitHub, creating an\nissue, claiming an issue, making a pull request, and where to store data and\nnewly created Docker images.",
            "title": "How to contribute"
        },
        {
            "location": "/contrib/#quickstart",
            "text": "Dahak is open source, open data project and we welcome contributions at all\nlevels. We encourage you to submit issues, suggest documentation changes,\ncontribute code, images, workflows etc. Any software included in the workflows\nmust be open source, findable, and reusable. Check out  Getting\nStarted , analyze\nsome data, and contribute however you see fit.",
            "title": "Quickstart"
        },
        {
            "location": "/contrib/#how-to-get-started",
            "text": "Create a  GitHub  account.    Fork  https://github.com/dahak-metagenomics/dahak .  Visit that page, and then click 'Fork' in the upper right-hand corner. This\ncreates a copy of the dahak source code in your GitHub account. If you're\nnew to GitHub or want a refresher, please check out this awesome tutorial .    Clone a copy of dahak into your local environment (or work in your browser!).  Your shell command should look something like (you can click the 'clone or\ndownload' button on the dahak github, copy the link, and insert git clone\nbefore it):  git clone https://github.com/dahak-metagenomics/dahak.git  This makes a copy of the dahak repository on your local machine.",
            "title": "How to get started"
        },
        {
            "location": "/contrib/#claiming-an-issue-and-starting-to-develop",
            "text": "Find an open issue and claim it.  Go to the list  open\nissues  and claim one\nyou like. Once you've found an issue you like, open it, click 'Assignees',\nand assign yourself. Once you've assigned yourself make a comment below the\nissue saying \"I'm working on this.\" That's it; it's all yours. Well not\nreally, because you can always ask for help. Just make another comment below\nstating what you need some help with and we'll get right back to you.  (Staking your claim is super important because we're trying to avoid people\nworking on the same issue.)    In your local copy of the source code, update your master branch from the main dahak branch.  git checkout master\ngit pull dahak master  (This pulls in the latest changes from the master repository)  If git complains about 'merge conflicts' when you execute  git pull ,\nplease refer to the  Resolving merge conflicts  section \nof the khmer documentation.  If you are asked to make changes before your pull request can be accepted,\nyou can continue to commit additional changes to the branch associated with\nyour original pull request. The pull request will automatically be updated\neach time you commit to that branch. Github provides a medium for\ncommunicating and providing feedback. Once the pull request is approved, it\nwill be merged into the main branch by the dahak development team.    Create a new branch and link it to your fork on GitHub:  git checkout -b name-of-branch\ngit push -u origin name-of-branch  where you replace \"name-of-branch\" with 2-3 words separated by\ndashes or underscores describing what issue it fixes.    Make some changes and commit them to your branch.  After you've made a set of cohesive changes, run the command  git status .\nThis will display a list of all the files git has noticed you changed. Files\nin the 'untracked' section are files that weren't in the repository before\nbut git has seen.  To commit these changes you have to 'stage' them using the following command.  git add path/to/file  Once you've staged your changes, it's time to make a commit (Don't forget to\nchange path/to/file to the actual path to file):  git commit -m 'Provide a brief description of the changes you made here'  Please make your commit message informative but concise \u2014 these messages\nbecome a part of the history of the repo and an informative message will\nhelp track down changes later. Don't stress over this too much, but before\nyou press the button, please consider whether you will find this commit\nmessage useful when a bug pops up 6 months from now and you need to sort\nthrough issues to find the right one. Once your changes have been commited,\npush them to the remote branch:  git push origin    As you develop, please periodically update your branch with changes that have\n   been made to the master branch, and resolve any conflicts that come up.  git pull master    Repeat until your ready to commit to master    Set up a 'Pull Request' asking to merge your changes into the master dahak repository  In a web browser, go to your GitHub fork of dahak, e.g.:  https://github.com/your-github-username/dahak \nand you will see a list of 'recently pushed branches' just above the source\ncode listing. On the right side, there should be a green 'Compare and pull\nrequest' button. This will add a pull request submission checklist in the\nfollowing form:  Merge Checklist\n - Typos are a sign of poorly maintained code. Has this request been checked with a spell checker?\n - Tutorials should be universally reproducible. If this request modifies a tutorial, does it assume we're starting from a blank Ubuntu 16.04 (Xenial Xerus) image?\n - Large diffs to binary or data files can artificially inflate the size of the repository. Are there large diffs to binary or data files, and are these changes necessary?  Next, click \"Create Pull Request\". This creates a new issue where others can review and make suggestions before your code is added the master dahak repository.    Review the pull request checklist and make changes, if necessary.  Check off as many boxes as possible and make a comment if you need help. If\nyou have an  ORCID ID , please add that as a comment. Dahak\nis an open-source, community-driven project and we'd like to acknowledge\nyour contribution when we publish. Including your ORCID ID helps that\nprocess move smoothly.  As you add new changes, you can keep pushing to your pull request using git push origin .    When you're ready to have the pull request reviewed, please mention\n    @brooksph, @charlesreid1, @kternus, @stephenturner, @ctb or anyone else on\n    the list of\n     collaborators \n    plus the comment  ready for review . Often pull requests will require\n    changes, need more work before they can be merged, or simply need to be\n    addressed later. Adding tags can help with organizing. Check out this list\n    for some examples of\n     tags .    Once your issue has been reviewed an merged, stand-up, throw your hands in\n    the air, and do a little dance; you're officially a GitHub master and a\n    contributor to the dahak project \u2013 we hold you in the highest of regards.",
            "title": "Claiming an issue and starting to develop"
        },
        {
            "location": "/contrib/#my-pull-request-was-merged-what-now",
            "text": "Before continuing on your journey towards your next pull request, there are a\ncouple of steps that you need take to clean up your local copy of dahak.  git checkout master\ngit pull master\ngit branch -d my-branch-name     # delete the branch locally\ngit push origin :my-branch-name  # delete the branch on GitHub (your fork)",
            "title": "My pull request was merged. What now?"
        },
        {
            "location": "/contrib/#i-have-a-dataset-to-contribute-to-the-project",
            "text": "Great! A big part of this project is benchmarking tools to determine when and\nhow we should use them. Datasets with interesting composition help us uncover\nnew and interesting things about metagenomics tools. If you have a dataset that\nyou would like to benchmark and/or submit for benchmarking please create an\nissue and mention @brooksph, @kternus, or @ctb. In general, we'll advise you to\nmake the data publicly available and go crazy characterizing it. We can help you\nthink about the best way to do it but in general we're using the workflows in the workflows/ \nrepository and analyzing the data in Jupyter notebooks. Poke us and we'd be\nhappy to discuss the process. If your dataset is less than 5 GB in size, the open science framework  is a great, free place to put it.",
            "title": "I have a dataset to contribute to the project"
        },
        {
            "location": "/contrib/#i-have-a-tool-to-contribute-to-the-project",
            "text": "Greater! The more tools the better. A major goal of this project to make more\ntools easy to use. We opted to do this by using or creating containerized tools.\nThe  biocontainers project  is leading the way in\nthis effort and we've contributed a few things there. You're not required to\ncontribute to the biocontainers project to add a tool to this project but the\ntool must be open source and the image must be stored in a public repository\nlike  Docker hub  or  quay.io . Here's \nan excellent guide to getting started building containers. We're using Singularity  to run the containers in\nour workflows. Docker does not need to be installed.",
            "title": "I have a tool to contribute to the project"
        },
        {
            "location": "/contrib/#i-have-a-workflow-to-contribute-to-the-project",
            "text": "Greatest! New metagenomics analysis tools are created all the time. We're using\na small subset that we think encompasses most the methods that are most commonly\nused to probe metagenomic communities. If you want to include a new tool or\nworkflow, create an issue and we can point you in the right direction. The\ncritical bits are we're stringing together open-source, containerized tools to\nmake workflows using  Snakemake .\nTake a look here for a  basic\nexample \nand here for a bit  more\nflavor .\nThis is a work in progress but the second example is where we're headed.",
            "title": "I have a workflow to contribute to the project"
        },
        {
            "location": "/contrib/#contributor-code-of-conduct",
            "text": "As contributors and maintainers of this project, we pledge to respect all people\nwho contribute through reporting issues, posting feature requests, updating\ndocumentation, submitting pull requests or patches, and other activities.  We are committed to making participation in this project a harassment-free\nexperience for everyone, regardless of level of experience, gender, gender\nidentity and expression, sexual orientation, disability, personal appearance,\nbody size, race, ethnicity, age, or religion.  Examples of unacceptable behavior by participants include the use of sexual\nlanguage or imagery, derogatory comments or personal attacks, trolling, public\nor private harassment, insults, or other unprofessional conduct.  Project maintainers have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct. Project maintainers who do not follow the\nCode of Conduct may be removed from the project team.  Instances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by emailing  Phil Brooks  or  C. Titus\nBrown . To report an issue involving either of them, please\nemail  Judi Brown Clarke , Ph.D. the Diversity Director at the\nBEACON Center for the Study of Evolution in Action, an NSF Center for Science\nand Technology.  This Code of Conduct is adapted from the Contributor Covenant, version 1.0.0,\navailable from http://contributor-covenant.org/version/1/0/0/",
            "title": "Contributor Code of Conduct"
        }
    ]
}